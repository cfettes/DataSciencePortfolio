{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognize named entities on Twitter with LSTMs\n",
    "\n",
    "Recurrent neural networks, in particular Bi-Directional Long Short Memory Networks (Bi-LSTMs), will be used to solve a Named Entity Recognition (NER) problem, extracting named entities from Twitter data. \n",
    "\n",
    "The data is a corpus containing tweets with NE tags. Every line of a file contains a token (word/punctuation symbol-tag pair, separated by a whitespace. Different tweets are separated by an empty line.\n",
    "\n",
    "There are three separate dataset files: (1) train data for training the model; (2) validation data for evaluation and hyperparameters tuning; (3) test data for final evaluation of the model.\n",
    "\n",
    "\n",
    "## Load Twitter Named Entity Recognition corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to read corpus and return 2 lists - tokens and tags\n",
    "def read_data(file_path):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    \n",
    "    tweet_tokens = []\n",
    "    tweet_tags = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if tweet_tokens:\n",
    "                tokens.append(tweet_tokens)\n",
    "                tags.append(tweet_tags)\n",
    "            tweet_tokens = []\n",
    "            tweet_tags = []\n",
    "        else:\n",
    "            token, tag = line.split()\n",
    "            # Replace all urls with <URL> token\n",
    "            # Replace all users with <USR> token\n",
    "            if token.startswith('@'):\n",
    "                token = '<USR>'\n",
    "            elif token.startswith('http://') or token.startswith('https://'):\n",
    "                token = '<URL>'\n",
    "            \n",
    "            tweet_tokens.append(token)\n",
    "            tweet_tags.append(tag)\n",
    "            \n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, train_tags = read_data('train.txt')\n",
    "validation_tokens, validation_tags = read_data('validation.txt')\n",
    "test_tokens, test_tags = read_data('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT\tO\n",
      "<USR>\tO\n",
      ":\tO\n",
      "Online\tO\n",
      "ticket\tO\n",
      "sales\tO\n",
      "for\tO\n",
      "Ghostland\tB-musicartist\n",
      "Observatory\tI-musicartist\n",
      "extended\tO\n",
      "until\tO\n",
      "6\tO\n",
      "PM\tO\n",
      "EST\tO\n",
      "due\tO\n",
      "to\tO\n",
      "high\tO\n",
      "demand\tO\n",
      ".\tO\n",
      "Get\tO\n",
      "them\tO\n",
      "before\tO\n",
      "they\tO\n",
      "sell\tO\n",
      "out\tO\n",
      "...\tO\n",
      "\n",
      "Apple\tB-product\n",
      "MacBook\tI-product\n",
      "Pro\tI-product\n",
      "A1278\tI-product\n",
      "13.3\tI-product\n",
      "\"\tI-product\n",
      "Laptop\tI-product\n",
      "-\tI-product\n",
      "MD101LL/A\tI-product\n",
      "(\tO\n",
      "June\tO\n",
      ",\tO\n",
      "2012\tO\n",
      ")\tO\n",
      "-\tO\n",
      "Full\tO\n",
      "read\tO\n",
      "by\tO\n",
      "eBay\tB-company\n",
      "<URL>\tO\n",
      "<URL>\tO\n",
      "\n",
      "Happy\tO\n",
      "Birthday\tO\n",
      "<USR>\tO\n",
      "!\tO\n",
      "May\tO\n",
      "Allah\tB-person\n",
      "s.w.t\tO\n",
      "bless\tO\n",
      "you\tO\n",
      "with\tO\n",
      "goodness\tO\n",
      "and\tO\n",
      "happiness\tO\n",
      ".\tO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    for token, tag in zip(train_tokens[i], train_tags[i]):\n",
    "        print('%s\\t%s' % (token, tag))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that takes in tokens or tags list and special tokens\n",
    "def build_dict(tokens_or_tags, special_tokens):\n",
    "\n",
    "    # Create dictionary with default value 0\n",
    "    tok2idx = defaultdict(lambda: 0)\n",
    "    idx2tok = []\n",
    "    \n",
    "    # Create mappings from tokens (or tags) to indices and vice versa\n",
    "    # At first, add special tokens (or tags) to the dictionaries - the first special token must have index 0\n",
    "    \n",
    "    # Mapping tok2idx should contain each token or tag only once; to do so, must:\n",
    "    # 1. extract unique tokens/tags from tokens_or_tags variable not in special_tokens\n",
    "    # 2. index them \n",
    "    # 3. for each token/tag save index into tok2idx\n",
    "\n",
    "    for i, token in enumerate(special_tokens):\n",
    "        tok2idx[token] = i\n",
    "        idx2tok.append(token)\n",
    "        \n",
    "    nextIndex = len(special_tokens)\n",
    "    for tokens in tokens_or_tags:\n",
    "        for token in tokens:\n",
    "            if token not in tok2idx:\n",
    "                tok2idx[token] = nextIndex\n",
    "                idx2tok.append(token)\n",
    "                nextIndex += 1\n",
    "    \n",
    "    return tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#special token <PAD> for padding sentence to same length when create batches of sentences\n",
    "#special token <UNK> is for out of vocab tokens\n",
    "special_tokens = ['<UNK>', '<PAD>']\n",
    "special_tags = ['O']\n",
    "\n",
    "# Create dictionaries \n",
    "token2idx, idx2token = build_dict(train_tokens + validation_tokens, special_tokens)\n",
    "tag2idx, idx2tag = build_dict(train_tags, special_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check same length\n",
    "len(token2idx) == len(idx2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.build_dict.<locals>.<lambda>()>,\n",
       "            {'O': 0,\n",
       "             'B-musicartist': 1,\n",
       "             'I-musicartist': 2,\n",
       "             'B-product': 3,\n",
       "             'I-product': 4,\n",
       "             'B-company': 5,\n",
       "             'B-person': 6,\n",
       "             'B-other': 7,\n",
       "             'I-other': 8,\n",
       "             'B-facility': 9,\n",
       "             'I-facility': 10,\n",
       "             'B-sportsteam': 11,\n",
       "             'B-geo-loc': 12,\n",
       "             'I-geo-loc': 13,\n",
       "             'I-company': 14,\n",
       "             'I-person': 15,\n",
       "             'B-movie': 16,\n",
       "             'I-movie': 17,\n",
       "             'B-tvshow': 18,\n",
       "             'I-tvshow': 19,\n",
       "             'I-sportsteam': 20})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-musicartist',\n",
       " 'I-musicartist',\n",
       " 'B-product',\n",
       " 'I-product',\n",
       " 'B-company',\n",
       " 'B-person',\n",
       " 'B-other',\n",
       " 'I-other',\n",
       " 'B-facility',\n",
       " 'I-facility',\n",
       " 'B-sportsteam',\n",
       " 'B-geo-loc',\n",
       " 'I-geo-loc',\n",
       " 'I-company',\n",
       " 'I-person',\n",
       " 'B-movie',\n",
       " 'I-movie',\n",
       " 'B-tvshow',\n",
       " 'I-tvshow',\n",
       " 'I-sportsteam']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to help create mapping between tokens and ids for a sentence\n",
    "def words2idxs(tokens_list):\n",
    "    return [token2idx[word] for word in tokens_list]\n",
    "\n",
    "def tags2idxs(tags_list):\n",
    "    return [tag2idx[tag] for tag in tags_list]\n",
    "\n",
    "def idxs2words(idxs):\n",
    "    return [idx2token[idx] for idx in idxs]\n",
    "\n",
    "def idxs2tags(idxs):\n",
    "    return [idx2tag[idx] for idx in idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate batches\n",
    "\n",
    "As Neural Networks are often trained with batches, weight updates of the network are based on several sequences at a single time. All sequences within a batch need to have the same length. To ensure this, they will be padded with a special <PAD> token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batching function, which generates padded batches of tokens and tags\n",
    "def batches_generator(batch_size, tokens, tags,\n",
    "                      shuffle=True, allow_smaller_last_batch=True):\n",
    "    \n",
    "    n_samples = len(tokens)\n",
    "    if shuffle:\n",
    "        order = np.random.permutation(n_samples)\n",
    "    else:\n",
    "        order = np.arange(n_samples)\n",
    "\n",
    "    n_batches = n_samples // batch_size\n",
    "    if allow_smaller_last_batch and n_samples % batch_size:\n",
    "        n_batches += 1\n",
    "\n",
    "    for k in range(n_batches):\n",
    "        batch_start = k * batch_size\n",
    "        batch_end = min((k + 1) * batch_size, n_samples)\n",
    "        current_batch_size = batch_end - batch_start\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        max_len_token = 0\n",
    "        for idx in order[batch_start: batch_end]:\n",
    "            x_list.append(words2idxs(tokens[idx]))\n",
    "            y_list.append(tags2idxs(tags[idx]))\n",
    "            max_len_token = max(max_len_token, len(tags[idx]))\n",
    "            \n",
    "        #data into numpy nd-arrays filled with padding indices.\n",
    "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token2idx['<PAD>']\n",
    "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag2idx['O']\n",
    "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
    "        for n in range(current_batch_size):\n",
    "            utt_len = len(x_list[n])\n",
    "            x[n, :utt_len] = x_list[n]\n",
    "            lengths[n] = utt_len\n",
    "            y[n, :utt_len] = y_list[n]\n",
    "        yield x, y, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RNN\n",
    "\n",
    "The network architecture will now be specified, based on TensorFlow building blocks. A LSTM network will be created, which will produce a probability distribution over tags for each token in a sentence. A Bi-LSTM will be used to take into account both right and left contexts of the token. A Dense layer will be used on top to perform tag classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() #this is to ensure v1 modules run that have been deprecated in v2\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMModel():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create placeholders to specify what data is going to be fed into the network during execution time. For this task, the following placeholders are needed:\n",
    "+ input_batch — sequences of words (shape equals [batch_size, sequence_len])\n",
    "+ ground_truth_tags — sequences of tags (shape equals [batch_size, sequence_len])\n",
    "+ lengths — lengths of un-padded sequences (shape equals [batch_size])\n",
    "+ dropout_ph — dropout keep probability (predefined value 1)\n",
    "+ learning_rate_ph — learning rate (needed as want to change value during training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to specify model placeholders\n",
    "def declare_placeholders(self):\n",
    "\n",
    "    # Placeholders for input and ground truth output.\n",
    "    self.input_batch = tf.placeholder(dtype=tf.int32, shape=[None, None], name='input_batch') \n",
    "    self.ground_truth_tags = tf.placeholder(dtype=tf.int32, shape=[None, None], name='ground_truth_tags') \n",
    "  \n",
    "    # Placeholder for lengths of the sequences.\n",
    "    self.lengths = tf.placeholder(dtype=tf.int32, shape=[None], name='lengths') \n",
    "    \n",
    "    # Placeholder for a dropout keep probability. If we don't feed\n",
    "    # a value for this placeholder, it will be equal to 1.0.\n",
    "    self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
    "    \n",
    "    # Placeholder for a learning rate (tf.float32).\n",
    "    self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__declare_placeholders = classmethod(declare_placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, specify neural network layers. Preparatory steps:\n",
    "+ Create embeddings matrix with tf.Variable. Specify its name (embeddings_matrix), type (tf.float32), and initialise with random values.\n",
    "+ Create forward and backward LSTM cells (TensorFlow provides a number of RNN cells)\n",
    "+ Wrap cells with DropoutWrapper (regularisation technique for neural networks)\n",
    "\n",
    "Then, build computation graph that transforms an input_batch:\n",
    "+ Look up embeddings for an input_batch in the prepared embedding_matrix\n",
    "+ Pass embeddings through Bidirectional Dynamic RNN with specified forward and backward cells.\n",
    "+ Create a dense layer on top. Its output will be used directly in loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to specify Bi_LSTM architecture and compute logits for inputs\n",
    "def build_layers(self, vocabulary_size, embedding_dim, n_hidden_rnn, n_tags):\n",
    "    \n",
    "    # Create embedding variable with dtype tf.float32\n",
    "    initial_embedding_matrix = np.random.randn(vocabulary_size, embedding_dim) / np.sqrt(embedding_dim)\n",
    "    embedding_matrix_variable = tf.Variable(initial_embedding_matrix, dtype=tf.float32)\n",
    "    \n",
    "    # Create RNN cells with n_hidden_rnn number of units \n",
    "    # and dropout, initializing all *_keep_prob with dropout placeholder.\n",
    "    forward_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(n_hidden_rnn),\n",
    "                                                 input_keep_prob=self.dropout_ph,\n",
    "                                                 output_keep_prob=self.dropout_ph,\n",
    "                                                 state_keep_prob=self.dropout_ph)\n",
    "    backward_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(n_hidden_rnn),\n",
    "                                                  input_keep_prob=self.dropout_ph,\n",
    "                                                  output_keep_prob=self.dropout_ph,\n",
    "                                                  state_keep_prob=self.dropout_ph)\n",
    "\n",
    "    # Look up embeddings for self.input_batch (tf.nn.embedding_lookup).\n",
    "    # Shape: [batch_size, sequence_len, embedding_dim].\n",
    "    embeddings =  tf.nn.embedding_lookup(embedding_matrix_variable, self.input_batch)\n",
    "    \n",
    "    # Pass them through Bidirectional Dynamic RNN \n",
    "    # Shape: [batch_size, sequence_len, 2 * n_hidden_rnn]. \n",
    "    # And initialise sequence_length as self.lengths and dtype as tf.float32.\n",
    "    (rnn_output_fw, rnn_output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=forward_cell,\n",
    "                                                                        cell_bw=backward_cell,\n",
    "                                                                        inputs=embeddings,\n",
    "                                                                        sequence_length=self.lengths,\n",
    "                                                                        dtype=tf.float32)\n",
    "    rnn_output = tf.concat([rnn_output_fw, rnn_output_bw], axis=2)\n",
    "\n",
    "    # Dense layer on top.\n",
    "    # Shape: [batch_size, sequence_len, n_tags].   \n",
    "    self.logits = tf.layers.dense(rnn_output, n_tags, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__build_layers = classmethod(build_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute actual predictions of the neural network, need to apply softmax to last layer and find most probable tags with argmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that transforms logits to probabilities and finds the most probable tags\n",
    "def compute_predictions(self):    \n",
    "    # Create softmax function\n",
    "    softmax_output = tf.nn.softmax(self.logits)\n",
    "    \n",
    "    # Use argmax to get the most probable tags and set axis=-1 or argmax will be calculated incorrectly\n",
    "    self.predictions = tf.argmax(softmax_output, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__compute_predictions = classmethod(compute_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, need a loss function - here will use cross-entropy loss, efficiently implemented in TF as cross entropy with logits. It should be applied to logits of the model (not to softmax probabilities!). Do not want to take into account loss terms coming from <PAD> tokens, so these need to be masked out before computing mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that computes masked cross-entropy loss with logits\n",
    "def compute_loss(self, n_tags, PAD_index):\n",
    "    \n",
    "    # Create cross entropy function function \n",
    "    ground_truth_tags_one_hot = tf.one_hot(self.ground_truth_tags, n_tags)\n",
    "    loss_tensor =  tf.nn.softmax_cross_entropy_with_logits_v2(labels=ground_truth_tags_one_hot, logits=self.logits)\n",
    "    \n",
    "    mask = tf.cast(tf.not_equal(self.input_batch, PAD_index), tf.float32)\n",
    "    # Create loss function that ignores <PAD> tokens \n",
    "    self.loss =  tf.reduce_mean(mask*loss_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__compute_loss = classmethod(compute_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, will use Adam optimiser with a learning rate from the corresponding placeholder to optimise loss. Clipping will also be applied to eliminate exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that specifies the optimiser and train_op for the model\n",
    "def perform_optimization(self):\n",
    "    \n",
    "    # Create optimiser\n",
    "    self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph)\n",
    "    self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "    \n",
    "    # Gradient clipping - list comprehension used to apply only to gradients\n",
    "    clip_norm = tf.cast(1.0, tf.float32)\n",
    "    self.grads_and_vars = [(tf.clip_by_norm(grad, clip_norm), var) for grad, var in self.grads_and_vars]\n",
    "    \n",
    "    self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__perform_optimization = classmethod(perform_optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the parts of your network have now been specified. This will now be put to the constructor of our Bi-LSTM class to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(self, vocabulary_size, n_tags, embedding_dim, n_hidden_rnn, PAD_index):\n",
    "    self.__declare_placeholders()\n",
    "    self.__build_layers(vocabulary_size, embedding_dim, n_hidden_rnn, n_tags)\n",
    "    self.__compute_predictions()\n",
    "    self.__compute_loss(n_tags, PAD_index)\n",
    "    self.__perform_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__init__ = classmethod(init_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network and predict tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session.run initiates computations in the graph previously defined\n",
    "#self.train_op, declared in perform_optimization, computed to train the network\n",
    "def train_on_batch(self, session, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability):\n",
    "    feed_dict = {self.input_batch: x_batch,\n",
    "                 self.ground_truth_tags: y_batch,\n",
    "                 self.learning_rate_ph: learning_rate,\n",
    "                 self.dropout_ph: dropout_keep_probability,\n",
    "                 self.lengths: lengths}\n",
    "    \n",
    "    session.run(self.train_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.train_on_batch = classmethod(train_on_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To predict tags, compute self.predictions\n",
    "def predict_for_batch(self, session, x_batch, lengths):\n",
    "    predictions = session.run(self.predictions,\n",
    "                              feed_dict={self.input_batch:x_batch, self.lengths:lengths})\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.predict_for_batch = classmethod(predict_for_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for evaluation\n",
    "from collections import OrderedDict\n",
    "\n",
    "def _update_chunk(candidate, prev, current_tag, current_chunk, current_pos, prediction=False):\n",
    "    if candidate == 'B-' + current_tag:\n",
    "        if len(current_chunk) > 0 and len(current_chunk[-1]) == 1:\n",
    "                current_chunk[-1].append(current_pos - 1)\n",
    "        current_chunk.append([current_pos])\n",
    "    elif candidate == 'I-' + current_tag:\n",
    "        if prediction and (current_pos == 0 or current_pos > 0 and prev.split('-', 1)[-1] != current_tag):\n",
    "            current_chunk.append([current_pos])\n",
    "        if not prediction and (current_pos == 0 or current_pos > 0 and prev == 'O'):\n",
    "            current_chunk.append([current_pos])\n",
    "    elif current_pos > 0 and prev.split('-', 1)[-1] == current_tag:\n",
    "        if len(current_chunk) > 0:\n",
    "            current_chunk[-1].append(current_pos - 1)\n",
    "\n",
    "def _update_last_chunk(current_chunk, current_pos):\n",
    "    if len(current_chunk) > 0 and len(current_chunk[-1]) == 1:\n",
    "        current_chunk[-1].append(current_pos - 1)\n",
    "\n",
    "def _tag_precision_recall_f1(tp, fp, fn):\n",
    "    precision, recall, f1 = 0, 0, 0\n",
    "    if tp + fp > 0:\n",
    "        precision = tp / (tp + fp) * 100\n",
    "    if tp + fn > 0:\n",
    "        recall = tp / (tp + fn) * 100\n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f1\n",
    "\n",
    "def _aggregate_metrics(results, total_correct):\n",
    "    total_true_entities = 0\n",
    "    total_predicted_entities = 0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    for tag, tag_metrics in results.items():\n",
    "        n_pred = tag_metrics['n_predicted_entities']\n",
    "        n_true = tag_metrics['n_true_entities']\n",
    "        total_true_entities += n_true\n",
    "        total_predicted_entities += n_pred\n",
    "        total_precision += tag_metrics['precision'] * n_pred\n",
    "        total_recall += tag_metrics['recall'] * n_true\n",
    "    \n",
    "    accuracy = 0\n",
    "    if total_true_entities > 0:\n",
    "        accuracy = total_correct / total_true_entities * 100\n",
    "    else:\n",
    "        print('CAUTION! Accuracy equals zero because there are no '\\\n",
    "              'correct entities. Check the correctness of your data.')\n",
    "    if total_predicted_entities > 0:\n",
    "        total_precision = total_precision / total_predicted_entities\n",
    "    total_recall = total_recall / total_true_entities\n",
    "    if total_precision + total_recall > 0:\n",
    "        total_f1 = 2 * total_precision * total_recall / (total_precision + total_recall)\n",
    "    return total_true_entities, total_predicted_entities, \\\n",
    "           total_precision, total_recall, total_f1, accuracy\n",
    "\n",
    "def _print_info(n_tokens, total_true_entities, total_predicted_entities, total_correct):\n",
    "    print('processed {len} tokens ' \\\n",
    "          'with {tot_true} phrases; ' \\\n",
    "          'found: {tot_pred} phrases; ' \\\n",
    "          'correct: {tot_cor}.\\n'.format(len=n_tokens,\n",
    "                                         tot_true=total_true_entities,\n",
    "                                         tot_pred=total_predicted_entities,\n",
    "                                         tot_cor=total_correct))\n",
    "\n",
    "def _print_metrics(accuracy, total_precision, total_recall, total_f1):\n",
    "    print('precision:  {tot_prec:.2f}%; ' \\\n",
    "          'recall:  {tot_recall:.2f}%; ' \\\n",
    "          'F1:  {tot_f1:.2f}\\n'.format(acc=accuracy,\n",
    "                                           tot_prec=total_precision,\n",
    "                                           tot_recall=total_recall,\n",
    "                                           tot_f1=total_f1))\n",
    "\n",
    "def _print_tag_metrics(tag, tag_results):\n",
    "    print(('\\t%12s' % tag) + ': precision:  {tot_prec:6.2f}%; ' \\\n",
    "                               'recall:  {tot_recall:6.2f}%; ' \\\n",
    "                               'F1:  {tot_f1:6.2f}; ' \\\n",
    "                               'predicted:  {tot_predicted:4d}\\n'.format(tot_prec=tag_results['precision'],\n",
    "                                                                         tot_recall=tag_results['recall'],\n",
    "                                                                         tot_f1=tag_results['f1'],\n",
    "                                                                         tot_predicted=tag_results['n_predicted_entities']))\n",
    "    \n",
    "\n",
    "def precision_recall_f1(y_true, y_pred, print_results=True, short_report=False):\n",
    "    # Find all tags\n",
    "    tags = sorted(set(tag[2:] for tag in y_true + y_pred if tag != 'O'))\n",
    "\n",
    "    results = OrderedDict((tag, OrderedDict()) for tag in tags)\n",
    "    n_tokens = len(y_true)\n",
    "    total_correct = 0\n",
    "\n",
    "    # For eval_conll_try we find all chunks in the ground truth and prediction\n",
    "    # For each chunk we store starting and ending indices\n",
    "    for tag in tags:\n",
    "        true_chunk = list()\n",
    "        predicted_chunk = list()\n",
    "        for position in range(n_tokens):\n",
    "            _update_chunk(y_true[position], y_true[position - 1], tag, true_chunk, position)\n",
    "            _update_chunk(y_pred[position], y_pred[position - 1], tag, predicted_chunk, position, True)\n",
    "\n",
    "        _update_last_chunk(true_chunk, position)\n",
    "        _update_last_chunk(predicted_chunk, position)\n",
    "\n",
    "        # Then we find all correctly classified intervals\n",
    "        # True positive results\n",
    "        tp = sum(chunk in predicted_chunk for chunk in true_chunk)\n",
    "        total_correct += tp\n",
    "\n",
    "        # And then just calculate errors of the first and second kind\n",
    "        # False negative\n",
    "        fn = len(true_chunk) - tp\n",
    "        # False positive\n",
    "        fp = len(predicted_chunk) - tp\n",
    "        precision, recall, f1 = _tag_precision_recall_f1(tp, fp, fn)\n",
    "\n",
    "        results[tag]['precision'] = precision\n",
    "        results[tag]['recall'] = recall\n",
    "        results[tag]['f1'] = f1\n",
    "        results[tag]['n_predicted_entities'] = len(predicted_chunk)\n",
    "        results[tag]['n_true_entities'] = len(true_chunk)\n",
    "\n",
    "    total_true_entities, total_predicted_entities, \\\n",
    "           total_precision, total_recall, total_f1, accuracy = _aggregate_metrics(results, total_correct)\n",
    "\n",
    "    if print_results:\n",
    "        _print_info(n_tokens, total_true_entities, total_predicted_entities, total_correct)\n",
    "        _print_metrics(accuracy, total_precision, total_recall, total_f1)\n",
    "\n",
    "        if not short_report:\n",
    "            for tag, tag_results in results.items():\n",
    "                _print_tag_metrics(tag, tag_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to perform predictions and transform indices to tokens and tags\n",
    "def predict_tags(model, session, token_idxs_batch, lengths):    \n",
    "    tag_idxs_batch = model.predict_for_batch(session, token_idxs_batch, lengths)\n",
    "    \n",
    "    tags_batch, tokens_batch = [], []\n",
    "    for tag_idxs, token_idxs in zip(tag_idxs_batch, token_idxs_batch):\n",
    "        tags, tokens = [], []\n",
    "        for tag_idx, token_idx in zip(tag_idxs, token_idxs):\n",
    "            tags.append(idx2tag[tag_idx])\n",
    "            tokens.append(idx2token[token_idx])\n",
    "        tags_batch.append(tags)\n",
    "        tokens_batch.append(tokens)\n",
    "    return tags_batch, tokens_batch\n",
    "    \n",
    "\n",
    "#function that computses NER quality measures\n",
    "def eval_conll(model, session, tokens, tags, short_report=True):\n",
    "    y_true, y_pred = [], []\n",
    "    for x_batch, y_batch, lengths in batches_generator(1, tokens, tags):\n",
    "        tags_batch, tokens_batch = predict_tags(model, session, x_batch, lengths)\n",
    "        if len(x_batch[0]) != len(tags_batch[0]):\n",
    "            raise Exception(\"Incorrect length of prediction for the input, \"\n",
    "                            \"expected length: %i, got: %i\" % (len(x_batch[0]), len(tags_batch[0])))\n",
    "        predicted_tags = []\n",
    "        ground_truth_tags = []\n",
    "        for gt_tag_idx, pred_tag, token in zip(y_batch[0], tags_batch[0], tokens_batch[0]): \n",
    "            if token != '<PAD>':\n",
    "                ground_truth_tags.append(idx2tag[gt_tag_idx])\n",
    "                predicted_tags.append(pred_tag)\n",
    "\n",
    "        # extend every prediction and ground truth sequence with 'O' tag to indicate a possible end of entity.\n",
    "        y_true.extend(ground_truth_tags + ['O'])\n",
    "        y_pred.extend(predicted_tags + ['O'])\n",
    "        \n",
    "    results = precision_recall_f1(y_true, y_pred, print_results=True, short_report=short_report)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiment\n",
    "\n",
    "Create BiLSTMModel model with the following parameters:\n",
    "+ vocabulary_size — number of tokens\n",
    "+ n_tags — number of tags\n",
    "+ embedding_dim — dimension of embeddings\n",
    "+ n_hidden_rnn — size of hidden layers for RNN\n",
    "+ PAD_index — an index of the padding token (<PAD>).\n",
    "\n",
    "Set hyperparameters:\n",
    "+ batch_size\n",
    "+ epochs\n",
    "+ starting value of learning_rate\n",
    "+ learning_rate_decay\n",
    "+ dropout_keep_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-f154aca0eafe>:10: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-17-f154aca0eafe>:30: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/charlottefettes/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/charlottefettes/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /Users/charlottefettes/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-17-f154aca0eafe>:35: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /Users/charlottefettes/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "model = BiLSTMModel(vocabulary_size=len(token2idx), n_tags=len(tag2idx), embedding_dim=200, n_hidden_rnn=200, PAD_index=token2idx['<PAD>'])\n",
    "\n",
    "batch_size = 32\n",
    "n_epochs = 4\n",
    "learning_rate = 0.005\n",
    "learning_rate_decay = np.sqrt(2)\n",
    "dropout_keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training... \n",
      "\n",
      "-------------------- Epoch 1 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 78413 phrases; correct: 228.\n",
      "\n",
      "precision:  0.29%; recall:  5.08%; F1:  0.55\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 9552 phrases; correct: 29.\n",
      "\n",
      "precision:  0.30%; recall:  5.40%; F1:  0.57\n",
      "\n",
      "-------------------- Epoch 2 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 3014 phrases; correct: 622.\n",
      "\n",
      "precision:  20.64%; recall:  13.86%; F1:  16.58\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 222 phrases; correct: 55.\n",
      "\n",
      "precision:  24.77%; recall:  10.24%; F1:  14.49\n",
      "\n",
      "-------------------- Epoch 3 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4762 phrases; correct: 1838.\n",
      "\n",
      "precision:  38.60%; recall:  40.94%; F1:  39.74\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 370 phrases; correct: 133.\n",
      "\n",
      "precision:  35.95%; recall:  24.77%; F1:  29.33\n",
      "\n",
      "-------------------- Epoch 4 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4567 phrases; correct: 2883.\n",
      "\n",
      "precision:  63.13%; recall:  64.22%; F1:  63.67\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 367 phrases; correct: 167.\n",
      "\n",
      "precision:  45.50%; recall:  31.10%; F1:  36.95\n",
      "\n",
      "...training finished.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('Start training... \\n')\n",
    "for epoch in range(n_epochs):\n",
    "    # For each epoch evaluate the model on train and validation data\n",
    "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(n_epochs) + '-' * 20)\n",
    "    print('Train data evaluation:')\n",
    "    eval_conll(model, sess, train_tokens, train_tags, short_report=True)\n",
    "    print('Validation data evaluation:')\n",
    "    eval_conll(model, sess, validation_tokens, validation_tags, short_report=True)\n",
    "    \n",
    "    # Train the model\n",
    "    for x_batch, y_batch, lengths in batches_generator(batch_size, train_tokens, train_tags):\n",
    "        model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability)\n",
    "        \n",
    "    # Decaying the learning rate\n",
    "    learning_rate = learning_rate / learning_rate_decay\n",
    "    \n",
    "print('...training finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the eval_conll function, full quality reports will be printed for this model on train, validation, and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Train set quality: --------------------\n",
      "processed 105778 tokens with 4489 phrases; found: 4792 phrases; correct: 3465.\n",
      "\n",
      "precision:  72.31%; recall:  77.19%; F1:  74.67\n",
      "\n",
      "\t     company: precision:   81.78%; recall:   88.65%; F1:   85.07; predicted:   697\n",
      "\n",
      "\t    facility: precision:   67.65%; recall:   80.57%; F1:   73.55; predicted:   374\n",
      "\n",
      "\t     geo-loc: precision:   76.89%; recall:   94.18%; F1:   84.66; predicted:  1220\n",
      "\n",
      "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:    14\n",
      "\n",
      "\t musicartist: precision:   29.85%; recall:    8.62%; F1:   13.38; predicted:    67\n",
      "\n",
      "\t       other: precision:   68.19%; recall:   78.73%; F1:   73.08; predicted:   874\n",
      "\n",
      "\t      person: precision:   77.04%; recall:   94.70%; F1:   84.96; predicted:  1089\n",
      "\n",
      "\t     product: precision:   53.79%; recall:   75.79%; F1:   62.92; predicted:   448\n",
      "\n",
      "\t  sportsteam: precision:   88.89%; recall:    3.69%; F1:    7.08; predicted:     9\n",
      "\n",
      "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n",
      "-------------------- Validation set quality: --------------------\n",
      "processed 12836 tokens with 537 phrases; found: 436 phrases; correct: 186.\n",
      "\n",
      "precision:  42.66%; recall:  34.64%; F1:  38.23\n",
      "\n",
      "\t     company: precision:   61.63%; recall:   50.96%; F1:   55.79; predicted:    86\n",
      "\n",
      "\t    facility: precision:   30.30%; recall:   29.41%; F1:   29.85; predicted:    33\n",
      "\n",
      "\t     geo-loc: precision:   65.59%; recall:   53.98%; F1:   59.22; predicted:    93\n",
      "\n",
      "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n",
      "\t musicartist: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n",
      "\t       other: precision:   28.57%; recall:   34.57%; F1:   31.28; predicted:    98\n",
      "\n",
      "\t      person: precision:   42.86%; recall:   26.79%; F1:   32.97; predicted:    70\n",
      "\n",
      "\t     product: precision:    7.14%; recall:   11.76%; F1:    8.89; predicted:    56\n",
      "\n",
      "\t  sportsteam: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n",
      "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n",
      "-------------------- Test set quality: --------------------\n",
      "processed 13258 tokens with 604 phrases; found: 482 phrases; correct: 221.\n",
      "\n",
      "precision:  45.85%; recall:  36.59%; F1:  40.70\n",
      "\n",
      "\t     company: precision:   63.79%; recall:   44.05%; F1:   52.11; predicted:    58\n",
      "\n",
      "\t    facility: precision:   44.44%; recall:   34.04%; F1:   38.55; predicted:    36\n",
      "\n",
      "\t     geo-loc: precision:   67.91%; recall:   55.15%; F1:   60.87; predicted:   134\n",
      "\n",
      "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n",
      "\t musicartist: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     2\n",
      "\n",
      "\t       other: precision:   25.00%; recall:   32.04%; F1:   28.09; predicted:   132\n",
      "\n",
      "\t      person: precision:   51.25%; recall:   39.42%; F1:   44.57; predicted:    80\n",
      "\n",
      "\t     product: precision:    7.50%; recall:   10.71%; F1:    8.82; predicted:    40\n",
      "\n",
      "\t  sportsteam: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n",
      "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
    "train_results = eval_conll(model, sess, train_tokens, train_tags, short_report=False)\n",
    "\n",
    "print('-' * 20 + ' Validation set quality: ' + '-' * 20)\n",
    "validation_results = eval_conll(model, sess, validation_tokens, validation_tags, short_report=False)\n",
    "\n",
    "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
    "test_results = eval_conll(model, sess, test_tokens, test_tags, short_report=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training... \n",
      "\n",
      "-------------------- Epoch 1 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 79511 phrases; correct: 206.\n",
      "\n",
      "precision:  0.26%; recall:  4.59%; F1:  0.49\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 9576 phrases; correct: 19.\n",
      "\n",
      "precision:  0.20%; recall:  3.54%; F1:  0.38\n",
      "\n",
      "-------------------- Epoch 2 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 2706 phrases; correct: 1498.\n",
      "\n",
      "precision:  55.36%; recall:  33.37%; F1:  41.64\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 211 phrases; correct: 115.\n",
      "\n",
      "precision:  54.50%; recall:  21.42%; F1:  30.75\n",
      "\n",
      "-------------------- Epoch 3 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4723 phrases; correct: 3217.\n",
      "\n",
      "precision:  68.11%; recall:  71.66%; F1:  69.84\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 371 phrases; correct: 182.\n",
      "\n",
      "precision:  49.06%; recall:  33.89%; F1:  40.09\n",
      "\n",
      "-------------------- Epoch 4 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 105778 tokens with 4489 phrases; found: 4778 phrases; correct: 4028.\n",
      "\n",
      "precision:  84.30%; recall:  89.73%; F1:  86.93\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 12836 tokens with 537 phrases; found: 439 phrases; correct: 200.\n",
      "\n",
      "precision:  45.56%; recall:  37.24%; F1:  40.98\n",
      "\n",
      "...training finished.\n"
     ]
    }
   ],
   "source": [
    "#the model will now be retrained using different parameters\n",
    "tf.reset_default_graph()\n",
    "\n",
    "model = BiLSTMModel(vocabulary_size=len(token2idx), n_tags=len(tag2idx), embedding_dim=200, n_hidden_rnn=200, PAD_index=token2idx['<PAD>'])\n",
    "\n",
    "batch_size = 32\n",
    "n_epochs = 4\n",
    "learning_rate = 0.01\n",
    "learning_rate_decay = np.sqrt(2)\n",
    "dropout_keep_probability = 0.6\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('Start training... \\n')\n",
    "for epoch in range(n_epochs):\n",
    "    # For each epoch evaluate the model on train and validation data\n",
    "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(n_epochs) + '-' * 20)\n",
    "    print('Train data evaluation:')\n",
    "    eval_conll(model, sess, train_tokens, train_tags, short_report=True)\n",
    "    print('Validation data evaluation:')\n",
    "    eval_conll(model, sess, validation_tokens, validation_tags, short_report=True)\n",
    "    \n",
    "    # Train the model\n",
    "    for x_batch, y_batch, lengths in batches_generator(batch_size, train_tokens, train_tags):\n",
    "        model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability)\n",
    "        \n",
    "    # Decaying the learning rate\n",
    "    learning_rate = learning_rate / learning_rate_decay\n",
    "    \n",
    "print('...training finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the eval_conll function, full quality reports will be printed for the final model on train, validation, and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Train set quality: --------------------\n",
      "processed 105778 tokens with 4489 phrases; found: 4596 phrases; correct: 4257.\n",
      "\n",
      "precision:  92.62%; recall:  94.83%; F1:  93.71\n",
      "\n",
      "\t     company: precision:   93.13%; recall:   97.05%; F1:   95.05; predicted:   670\n",
      "\n",
      "\t    facility: precision:   91.25%; recall:   92.99%; F1:   92.11; predicted:   320\n",
      "\n",
      "\t     geo-loc: precision:   96.05%; recall:   97.59%; F1:   96.81; predicted:  1012\n",
      "\n",
      "\t       movie: precision:   70.27%; recall:   76.47%; F1:   73.24; predicted:    74\n",
      "\n",
      "\t musicartist: precision:   85.48%; recall:   91.38%; F1:   88.33; predicted:   248\n",
      "\n",
      "\t       other: precision:   91.81%; recall:   94.72%; F1:   93.24; predicted:   781\n",
      "\n",
      "\t      person: precision:   96.83%; recall:   96.39%; F1:   96.61; predicted:   882\n",
      "\n",
      "\t     product: precision:   87.57%; recall:   93.08%; F1:   90.24; predicted:   338\n",
      "\n",
      "\t  sportsteam: precision:   92.89%; recall:   90.32%; F1:   91.59; predicted:   211\n",
      "\n",
      "\t      tvshow: precision:   70.00%; recall:   72.41%; F1:   71.19; predicted:    60\n",
      "\n",
      "-------------------- Validation set quality: --------------------\n",
      "processed 12836 tokens with 537 phrases; found: 405 phrases; correct: 192.\n",
      "\n",
      "precision:  47.41%; recall:  35.75%; F1:  40.76\n",
      "\n",
      "\t     company: precision:   67.86%; recall:   54.81%; F1:   60.64; predicted:    84\n",
      "\n",
      "\t    facility: precision:   40.00%; recall:   35.29%; F1:   37.50; predicted:    30\n",
      "\n",
      "\t     geo-loc: precision:   65.82%; recall:   46.02%; F1:   54.17; predicted:    79\n",
      "\n",
      "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     4\n",
      "\n",
      "\t musicartist: precision:   22.73%; recall:   17.86%; F1:   20.00; predicted:    22\n",
      "\n",
      "\t       other: precision:   40.00%; recall:   34.57%; F1:   37.09; predicted:    70\n",
      "\n",
      "\t      person: precision:   53.57%; recall:   26.79%; F1:   35.71; predicted:    56\n",
      "\n",
      "\t     product: precision:    9.30%; recall:   11.76%; F1:   10.39; predicted:    43\n",
      "\n",
      "\t  sportsteam: precision:   36.36%; recall:   20.00%; F1:   25.81; predicted:    11\n",
      "\n",
      "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     6\n",
      "\n",
      "-------------------- Test set quality: --------------------\n",
      "processed 13258 tokens with 604 phrases; found: 528 phrases; correct: 231.\n",
      "\n",
      "precision:  43.75%; recall:  38.25%; F1:  40.81\n",
      "\n",
      "\t     company: precision:   59.02%; recall:   42.86%; F1:   49.66; predicted:    61\n",
      "\n",
      "\t    facility: precision:   43.18%; recall:   40.43%; F1:   41.76; predicted:    44\n",
      "\n",
      "\t     geo-loc: precision:   72.41%; recall:   50.91%; F1:   59.79; predicted:   116\n",
      "\n",
      "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     2\n",
      "\n",
      "\t musicartist: precision:    8.33%; recall:    7.41%; F1:    7.84; predicted:    24\n",
      "\n",
      "\t       other: precision:   37.50%; recall:   37.86%; F1:   37.68; predicted:   104\n",
      "\n",
      "\t      person: precision:   65.00%; recall:   37.50%; F1:   47.56; predicted:    60\n",
      "\n",
      "\t     product: precision:    4.55%; recall:   14.29%; F1:    6.90; predicted:    88\n",
      "\n",
      "\t  sportsteam: precision:   34.78%; recall:   25.81%; F1:   29.63; predicted:    23\n",
      "\n",
      "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
    "train_results = eval_conll(model, sess, train_tokens, train_tags, short_report=False)\n",
    "\n",
    "print('-' * 20 + ' Validation set quality: ' + '-' * 20)\n",
    "validation_results = eval_conll(model, sess, validation_tokens, validation_tags, short_report=False)\n",
    "\n",
    "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
    "test_results = eval_conll(model, sess, test_tokens, test_tags, short_report=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
