{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear models to predict StackOverflow tags\n",
    "\n",
    "Data used is found on Kaggle as 'StackSample: 10% of Stack Overflow Q&A' through the following link: https://www.kaggle.com/stackoverflow/stacksample/data\n",
    "\n",
    "This is a dataset with the text of 10% of questions and answers from the Stack Overflow programming Q&A website.\n",
    "\n",
    "This is organized as three tables - Questions, Answers and Tags. Here, only two will be used:\n",
    "+ Questions contains the title, body, creation date, closed date (if applicable), score, and owner ID for all non-deleted Stack Overflow questions whose Id is a multiple of 10.\n",
    "+ Tags contains the tags on each of these questions\n",
    "\n",
    "The task is to use questions to predict tags. This is a multilabel classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/charlottefettes/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pd.read_csv('Questions.csv', encoding='latin-1')\n",
    "\n",
    "tags = pd.read_csv('Tags.csv')\n",
    "tags[\"Tag\"]= tags[\"Tag\"].astype(str) \n",
    "tags = tags.groupby('Id').agg(lambda x: x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(tags, questions, how='inner', on='Id')\n",
    "data = data[['Title','Tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "train, test = train_test_split(data, train_size = 0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train['Title'].values, train['Tag'].values\n",
    "X_test, y_test = test['Title'].values, test['Tag'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "\n",
    "### Prepare the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;<>]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def text_prepare(text):\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join([x for x in text.split() if x and x not in STOPWORDS]) # delete stopwords from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "def test_text_prepare():\n",
    "    examples = [\"SQL Server - any equivalent of Excel's CHOOSE function?\",\n",
    "                \"How to free c++ memory vector<int> * arr?\"]\n",
    "    answers = [\"sql server equivalent excels choose function\", \n",
    "               \"free c++ memory vector int arr\"]\n",
    "    for ex, ans in zip(examples, answers):\n",
    "        if text_prepare(ex) != ans:\n",
    "            return \"Wrong answer for the case: '%s'\" % ex\n",
    "    return 'Basic tests are passed.'\n",
    "\n",
    "print(test_text_prepare())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [text_prepare(x) for x in X_train]\n",
    "X_test = [text_prepare(x) for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['get 0 180 degree onsensorchanged',\n",
       " 'ria service generated code support partial class',\n",
       " 'error building packages r 320 latex error creating pdf version']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words Tags Count\n",
    "\n",
    "Find most common words and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['android', 'android-layout']),\n",
       "       list(['wcf-ria-services', 'ado.net-entity-data-model']),\n",
       "       list(['r', 'package'])], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Dictionary of all tags from train corpus with their counts.\n",
    "tags_counts = Counter()\n",
    "\n",
    "# Dictionary of all words from train corpus with their counts.\n",
    "words_counts = Counter()\n",
    "\n",
    "for tags in y_train:\n",
    "    for tag in tags:\n",
    "        tags_counts[tag] += 1\n",
    "        \n",
    "for words in X_train:\n",
    "    for word in words.split():\n",
    "        words_counts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common tags:  [('javascript', 99082), ('java', 92219), ('c#', 80973)]\n",
      "Most common words:  [('using', 74079), ('file', 42948), ('error', 39977)]\n"
     ]
    }
   ],
   "source": [
    "most_common_tags = sorted(tags_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "most_common_words = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "print('Most common tags: ', most_common_tags)\n",
    "print('Most common words: ', most_common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming text to a vector\n",
    "\n",
    "Bag-of-words representation will be used to convert the text data to numeric vectors. To do this, the following steps will be taken:\n",
    "1. Find N most popular words in training corpus and numerate them, creating a dictionary containing the most popular words.\n",
    "2. For each title in the corpora, create a zero vector with dimension equal to N.\n",
    "3. For each text in the corpora, iterate over words that are in the dictionary and increase by 1 the corresponding coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_SIZE = 5000\n",
    "INDEX_TO_WORDS = sorted(words_counts.keys(), key=lambda x: words_counts[x], reverse=True)[:DICT_SIZE]\n",
    "WORDS_TO_INDEX = {word:i for i, word in enumerate(INDEX_TO_WORDS)}\n",
    "ALL_WORDS = WORDS_TO_INDEX.keys()\n",
    "\n",
    "def my_bag_of_words(text, words_to_index, dict_size):\n",
    "\n",
    "    result_vector = np.zeros(dict_size)\n",
    "\n",
    "    for word in text.split():\n",
    "        if word in words_to_index:\n",
    "            result_vector[words_to_index[word]] += 1\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "def test_my_bag_of_words():\n",
    "    words_to_index = {'hi': 0, 'you': 1, 'me': 2, 'are': 3}\n",
    "    examples = ['hi how are you']\n",
    "    answers = [[1, 1, 0, 1]]\n",
    "    for ex, ans in zip(examples, answers):\n",
    "        if (my_bag_of_words(ex, words_to_index, 4) != ans).any():\n",
    "            return \"Wrong answer for the case: '%s'\" % ex\n",
    "    return 'Basic tests are passed.'\n",
    "\n",
    "print(test_my_bag_of_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse as sp_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_train])\n",
    "X_test_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_test])\n",
    "\n",
    "print('X_train shape ', X_train_mybag.shape)\n",
    "print('X_test shape ', X_test_mybag.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been transformed into a sparse representation for efficient storage. As sklearn algorithms can only work with cst matrix, this is used here.\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "As an alternative approach, the bag-of-words framework can be extended by taking into account total frequencies of words in the corpora, which helps to penalise too frequent words, and provides better features space.\n",
    "\n",
    "The vectoriser is trained using the train corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_features(X_train, X_test):\n",
    "    # create TF-IDF vectorizer, fit on train set, transform train and test sets and return the result\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=5, max_df=0.9, ngram_range=(1, 2), token_pattern='(\\S+)')\n",
    "    \n",
    "    X_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_test = tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test, tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check results - here will look at c++ and c#\n",
    "X_train_tfidf, X_test_tfidf, tfidf_vocab = tfidf_features(X_train, X_test)\n",
    "tfidf_reversed_vocab = {i:word for word,i in tfidf_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_vocab['c++'])\n",
    "print(tfidf_vocab['c#'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiLabel classifier\n",
    "\n",
    "To deal with situations where there are mutiple tags - thus multilabel predictions - the labels need to transformed to binary form, and the prediction will be a mask of 0's and 1's.\n",
    "\n",
    "MultiLabelBinarizer can be used for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer(classes=sorted(tags_counts.keys()), sparse_output=True)\n",
    "\n",
    "#fit on all data to avoid error if tags missing from train but present in test\n",
    "y_all = pd.concat([y_train, y_test], axis=0)\n",
    "mlb.fit(y_all)\n",
    "\n",
    "#call fit_transform on train and test separately to obtain for each\n",
    "y_train = mlb.fit_transform(y_train)\n",
    "y_test = mlb.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the classifier will be trained. A one-vs-rest approach is used, where k classifiers (# tags) are trained.\n",
    "\n",
    "Logistic Regression will be used. This is a simple classifier - possibly not the best performed, but should perform fairly well on text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(X_train, y_train, penalty='l2', C=1.0):\n",
    "    # Create and fit LogisticRegression wrapped into OneVsRestClassifier.\n",
    "    lr = LogisticRegression(penalty=penalty, C=C, solver='liblinear')\n",
    "    ovr = OneVsRestClassifier(lr)\n",
    "    ovr.fit(X_train, y_train)\n",
    "    return ovr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train classifiers for different data transformations: bag-of-words and tf-idf\n",
    "classifier_mybag = train_classifier(X_train_mybag, y_train)\n",
    "classifier_tfidf = train_classifier(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create predictions for the data - 2 types: labels and scores\n",
    "y_test_predicted_labels_mybag = classifier_mybag.predict(X_test_mybag)\n",
    "y_test_predicted_scores_mybag = classifier_mybag.decision_function(X_test_mybag)\n",
    "\n",
    "y_test_predicted_labels_tfidf = classifier_tfidf.predict(X_test_tfidf)\n",
    "y_test_predicted_scores_tfidf = classifier_tfidf.decision_function(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try examples\n",
    "y_test_pred_inversed = mlb.inverse_transform(y_test_predicted_labels_tfidf)\n",
    "y_test_inversed = mlb.inverse_transform(y_test)\n",
    "for i in range(3):\n",
    "    print('Title:\\t{}\\nTrue labels:\\t{}\\nPredicted labels:\\t{}\\n\\n'.format(\n",
    "        X_test[i],\n",
    "        ','.join(y_test_inversed[i]),\n",
    "        ','.join(y_test_pred_inversed[i])\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Multiple classification metrics will be used to check performance: accuracy, F1-score, areas under ROC-curve, area under precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_scores(y_test, predicted):\n",
    "    print('Accuracy:', accuracy_score(y_test, predicted))\n",
    "    print('F1-score macro:', f1_score(y_test, predicted, average='macro'))\n",
    "    print('F1-score micro:', f1_score(y_test, predicted, average='micro'))\n",
    "    print('F1-score weighted:', f1_score(y_test, predicted, average='weighted'))\n",
    "    print('Precision macro:', average_precision_score(y_test, predicted, average='macro'))\n",
    "    print('Precision micro:', average_precision_score(y_test, predicted, average='micro'))\n",
    "    print('Precision weighted:', average_precision_score(y_test, predicted, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Bag-of-words')\n",
    "print_evaluation_scores(y_test, y_test_predicted_labels_mybag)\n",
    "print('Tfidf')\n",
    "print_evaluation_scores(y_test, y_test_predicted_labels_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some generalisations of the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import roc_auc\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(tags_counts)\n",
    "roc_auc(y_test, y_test_predicted_scores_mybag, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(tags_counts)\n",
    "roc_auc(y_test, y_test_predicted_scores_tfidf, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation \n",
    "\n",
    "Compare bag-of-words and TF-IDF approaches on F1-score, experimenting with L1 and L2 regularisation, and regression with different coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for penalty in ('l1', 'l2'):\n",
    "    for C in (0.1, 0.6, 1, 3):\n",
    "        print('Penalty:', penalty, 'C=', C)\n",
    "        classifier_mybag = train_classifier(X_train_mybag, y_train, penalty, C)\n",
    "        classifier_tfidf = train_classifier(X_train_tfidf, y_train, penalty, C)\n",
    "        y_test_predicted_labels_mybag = classifier_mybag.predict(X_test_mybag)\n",
    "\n",
    "        y_test_predicted_labels_tfidf = classifier_tfidf.predict(X_test_tfidf)\n",
    "        print('Bag-of-words')\n",
    "        print('F1-score weighted:', f1_score(y_test, y_test_predicted_labels_mybag, average='weighted'))\n",
    "        print('Tfidf')\n",
    "        print('F1-score weighted:', f1_score(y_test, y_test_predicted_labels_tfidf, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_best = train_classifier(X_train_, \n",
    "                                   y_train, penalty='', C=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions\n",
    "y_test_predicted_labels_final = classifier_best.predict(   \n",
    "    X_test_)\n",
    "\n",
    "y_test_predicted_scores_final = classifier_best.decision_function(\n",
    "    X_test_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Evaluation Scores for optimal model')\n",
    "print_evaluation_scores(y_test, y_test_predicted_labels_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of most important features (words)\n",
    "\n",
    "Find the words with the largest weights in Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_words_for_tag(classifier, tag, tags_classes, index_to_words, all_words):\n",
    "    print('Tag:\\t{}'.format(tag))\n",
    "    \n",
    "    # Extract an estimator from the classifier for the given tag.\n",
    "    # Extract feature coefficients from the estimator. \n",
    "    coef = classifier.coef_[tags_classes.index(tag)]\n",
    "    \n",
    "    top_positive_words = [index_to_words[idx] for idx in coef.argsort()[-1:-6:-1]]# top-5 words sorted by the coefficiens.\n",
    "    top_negative_words = [index_to_words[idx] for idx in coef.argsort()[:5]]# bottom-5 words  sorted by the coefficients.\n",
    "    print('Top positive words:\\t{}'.format(', '.join(top_positive_words)))\n",
    "    print('Top negative words:\\t{}\\n'.format(', '.join(top_negative_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_words_for_tag(classifier_tfidf, 'c', mlb.classes, tfidf_reversed_vocab, ALL_WORDS)\n",
    "print_words_for_tag(classifier_tfidf, 'c++', mlb.classes, tfidf_reversed_vocab, ALL_WORDS)\n",
    "print_words_for_tag(classifier_tfidf, 'linux', mlb.classes, tfidf_reversed_vocab, ALL_WORDS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
