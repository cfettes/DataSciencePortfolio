{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval chatbot for initial twitter query\n",
    "\n",
    "This project is a retrieval-based dialog system. It is a model that classifies whether response is the correct response to a given question/comment or not. Ideally, it retrieves the best response to a conversational input from a pool of responses.\n",
    "\n",
    "This uses PyTorch LSTM with encoder and dual encoder.\n",
    "\n",
    "**Data:**    \n",
    "\n",
    "The data utilised for this project is the Customer Support on Twitter dataset, found through the following link: https://www.kaggle.com/thoughtvector/customer-support-on-twitter\n",
    "\n",
    "It contains the following attributes:\n",
    "+ tweet_id - unique, anonymised ID for the Tweet. Referenced by response_tweet_id and in_response_to_tweet_id.\n",
    "+ author_id - unique, anonymised user ID. @s in the dataset have been replaced with their associated anonymized user ID.\n",
    "+ inbound - whether the tweet is \"inbound\" to a company doing customer support on Twitter. \n",
    "+ created_at = date and time when the tweet was sent.\n",
    "+ text = tweet content. Sensitive information like phone numbers and email addresses are replaced with mask values like __email__.\n",
    "+ response_tweet_id - IDs of tweets that are responses to this tweet, comma-separated.\n",
    "+ in_response_to_tweet_id - ID of the tweet this tweet is in response to, if any.\n",
    "\n",
    "This project will involve only the initial comment/question and response (approximately 66% of the full dataset). \n",
    "\n",
    "Firstly, the initial comment/question will be matched to the response from the company it is addressing. The questions texts will be processed, including the following:\n",
    "+ remove handles\n",
    "+ convert all to lowercase \n",
    "+ remove punctuation\n",
    "+ remove stopwords\n",
    "+ lemmatise\n",
    "+ remove emojis and emoticons\n",
    "+ remove urls\n",
    "+ remove html\n",
    "+ convert chat word language\n",
    "\n",
    "For the response texts, as it is a retrieval model, the purpose is to reproduce the response that is evaluated as most appropriate given the initial comment. Therefore, as much of the structure will be retained. Handles, urls and html will be removed to make it a more general response.\n",
    "\n",
    "Spellchecker was initially included - and would be for any model to be used in the real world - but took too long for purposes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>inbound</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>response_tweet_id</th>\n",
       "      <th>in_response_to_tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>sprintcare</td>\n",
       "      <td>False</td>\n",
       "      <td>Tue Oct 31 22:10:47 +0000 2017</td>\n",
       "      <td>@115712 I understand. I would like to assist y...</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>115712</td>\n",
       "      <td>True</td>\n",
       "      <td>Tue Oct 31 22:11:45 +0000 2017</td>\n",
       "      <td>@sprintcare and how do you propose we do that</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>115712</td>\n",
       "      <td>True</td>\n",
       "      <td>Tue Oct 31 22:08:27 +0000 2017</td>\n",
       "      <td>@sprintcare I have sent several private messag...</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>sprintcare</td>\n",
       "      <td>False</td>\n",
       "      <td>Tue Oct 31 21:54:49 +0000 2017</td>\n",
       "      <td>@115712 Please send us a Private Message so th...</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>115712</td>\n",
       "      <td>True</td>\n",
       "      <td>Tue Oct 31 21:49:35 +0000 2017</td>\n",
       "      <td>@sprintcare I did.</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id   author_id  inbound                      created_at  \\\n",
       "0         1  sprintcare    False  Tue Oct 31 22:10:47 +0000 2017   \n",
       "1         2      115712     True  Tue Oct 31 22:11:45 +0000 2017   \n",
       "2         3      115712     True  Tue Oct 31 22:08:27 +0000 2017   \n",
       "3         4  sprintcare    False  Tue Oct 31 21:54:49 +0000 2017   \n",
       "4         5      115712     True  Tue Oct 31 21:49:35 +0000 2017   \n",
       "\n",
       "                                                text response_tweet_id  \\\n",
       "0  @115712 I understand. I would like to assist y...                 2   \n",
       "1      @sprintcare and how do you propose we do that               NaN   \n",
       "2  @sprintcare I have sent several private messag...                 1   \n",
       "3  @115712 Please send us a Private Message so th...                 3   \n",
       "4                                 @sprintcare I did.                 4   \n",
       "\n",
       "   in_response_to_tweet_id  \n",
       "0                      3.0  \n",
       "1                      1.0  \n",
       "2                      4.0  \n",
       "3                      5.0  \n",
       "4                      6.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/twcs.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 787346 initial inbound messages.\n",
      "There are 875292 responses.\n",
      "There are 794299 responses from companies.\n",
      "Tweets Preview:\n",
      "Comment: @sprintcare is the worst customer service\n",
      "Response: @115712 Can you please send us a private message, so that I can gain further details about your account?\n",
      "--------------------\n",
      "Comment: @sprintcare is the worst customer service\n",
      "Response: @115712 I would love the chance to review the account and provide assistance.\n",
      "--------------------\n",
      "Comment: @sprintcare is the worst customer service\n",
      "Response: @115712 Hello! We never like our customers to feel like they are not valued.\n",
      "--------------------\n",
      "Comment: @115714 y’all lie about your “great” connection. 5 bars LTE, still won’t load something. Smh.\n",
      "Response: @115713 H there! We'd definitely like to work with you on this, how long have you been experiencing this issue? -AA\n",
      "--------------------\n",
      "Comment: @115714 whenever I contact customer support, they tell me I have shortcode enabled on my account, but I have never in the 4 years I've tried https://t.co/0G98RtNxPK\n",
      "Response: @115715 Please send me a private message so that I can send you the link to access your account. -FR\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "#create inbound df that have responses\n",
    "inbound = df[pd.isnull(df.in_response_to_tweet_id) & df.inbound]\n",
    "print('There are {} initial inbound messages.'.format(len(inbound)))\n",
    "\n",
    "# Merge in all tweets in response\n",
    "inbounds_and_outbounds = pd.merge(inbound, df, left_on='tweet_id', \n",
    "                                  right_on='in_response_to_tweet_id')\n",
    "print(\"There are {} responses.\".format(len(inbounds_and_outbounds)))\n",
    "\n",
    "#filter out replies not from company\n",
    "inbounds_and_outbounds = inbounds_and_outbounds[inbounds_and_outbounds.inbound_y ^ True]\n",
    "\n",
    "print(\"There are {} responses from companies.\".format(len(inbounds_and_outbounds)))\n",
    "print(\"Tweets Preview:\")\n",
    "for i in range(5):\n",
    "    print('Comment:', inbounds_and_outbounds.text_x[i])\n",
    "    print('Response:', inbounds_and_outbounds.text_y[i])\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inbounds_and_outbounds[['author_id_x','text_x','text_y','author_id_y']]\n",
    "data = data.rename(columns = {\"text_x\": \"question\",\n",
    "                              \"text_y\":\"response\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id_x</th>\n",
       "      <th>question</th>\n",
       "      <th>response</th>\n",
       "      <th>author_id_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115712</td>\n",
       "      <td>@sprintcare is the worst customer service</td>\n",
       "      <td>@115712 Can you please send us a private messa...</td>\n",
       "      <td>sprintcare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>115712</td>\n",
       "      <td>@sprintcare is the worst customer service</td>\n",
       "      <td>@115712 I would love the chance to review the ...</td>\n",
       "      <td>sprintcare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115712</td>\n",
       "      <td>@sprintcare is the worst customer service</td>\n",
       "      <td>@115712 Hello! We never like our customers to ...</td>\n",
       "      <td>sprintcare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115713</td>\n",
       "      <td>@115714 y’all lie about your “great” connectio...</td>\n",
       "      <td>@115713 H there! We'd definitely like to work ...</td>\n",
       "      <td>sprintcare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>115715</td>\n",
       "      <td>@115714 whenever I contact customer support, t...</td>\n",
       "      <td>@115715 Please send me a private message so th...</td>\n",
       "      <td>sprintcare</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  author_id_x                                           question  \\\n",
       "0      115712          @sprintcare is the worst customer service   \n",
       "1      115712          @sprintcare is the worst customer service   \n",
       "2      115712          @sprintcare is the worst customer service   \n",
       "3      115713  @115714 y’all lie about your “great” connectio...   \n",
       "4      115715  @115714 whenever I contact customer support, t...   \n",
       "\n",
       "                                            response author_id_y  \n",
       "0  @115712 Can you please send us a private messa...  sprintcare  \n",
       "1  @115712 I would love the chance to review the ...  sprintcare  \n",
       "2  @115712 Hello! We never like our customers to ...  sprintcare  \n",
       "3  @115713 H there! We'd definitely like to work ...  sprintcare  \n",
       "4  @115715 Please send me a private message so th...  sprintcare  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"function to remove punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\", \".join(stopwords.words('english'))\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"function to remove stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTICONS = {\n",
    "    u\":‑\\)\":\"Happy face or smiley\",\n",
    "    u\":\\)\":\"Happy face or smiley\",\n",
    "    u\":-\\]\":\"Happy face or smiley\",\n",
    "    u\":\\]\":\"Happy face or smiley\",\n",
    "    u\":-3\":\"Happy face smiley\",\n",
    "    u\":3\":\"Happy face smiley\",\n",
    "    u\":->\":\"Happy face smiley\",\n",
    "    u\":>\":\"Happy face smiley\",\n",
    "    u\"8-\\)\":\"Happy face smiley\",\n",
    "    u\":o\\)\":\"Happy face smiley\",\n",
    "    u\":-\\}\":\"Happy face smiley\",\n",
    "    u\":\\}\":\"Happy face smiley\",\n",
    "    u\":-\\)\":\"Happy face smiley\",\n",
    "    u\":c\\)\":\"Happy face smiley\",\n",
    "    u\":\\^\\)\":\"Happy face smiley\",\n",
    "    u\"=\\]\":\"Happy face smiley\",\n",
    "    u\"=\\)\":\"Happy face smiley\",\n",
    "    u\":‑D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\":D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"8‑D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"X‑D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\":-\\)\\)\":\"Very happy\",\n",
    "    u\":‑\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":-\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":‑c\":\"Frown, sad, andry or pouting\",\n",
    "    u\":c\":\"Frown, sad, andry or pouting\",\n",
    "    u\":‑<\":\"Frown, sad, andry or pouting\",\n",
    "    u\":<\":\"Frown, sad, andry or pouting\",\n",
    "    u\":‑\\[\":\"Frown, sad, andry or pouting\",\n",
    "    u\":\\[\":\"Frown, sad, andry or pouting\",\n",
    "    u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\n",
    "    u\">:\\[\":\"Frown, sad, andry or pouting\",\n",
    "    u\":\\{\":\"Frown, sad, andry or pouting\",\n",
    "    u\":@\":\"Frown, sad, andry or pouting\",\n",
    "    u\">:\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":'‑\\(\":\"Crying\",\n",
    "    u\":'\\(\":\"Crying\",\n",
    "    u\":'‑\\)\":\"Tears of happiness\",\n",
    "    u\":'\\)\":\"Tears of happiness\",\n",
    "    u\"D‑':\":\"Horror\",\n",
    "    u\"D:<\":\"Disgust\",\n",
    "    u\"D:\":\"Sadness\",\n",
    "    u\"D8\":\"Great dismay\",\n",
    "    u\"D;\":\"Great dismay\",\n",
    "    u\"D=\":\"Great dismay\",\n",
    "    u\"DX\":\"Great dismay\",\n",
    "    u\":‑O\":\"Surprise\",\n",
    "    u\":O\":\"Surprise\",\n",
    "    u\":‑o\":\"Surprise\",\n",
    "    u\":o\":\"Surprise\",\n",
    "    u\":-0\":\"Shock\",\n",
    "    u\"8‑0\":\"Yawn\",\n",
    "    u\">:O\":\"Yawn\",\n",
    "    u\":-\\*\":\"Kiss\",\n",
    "    u\":\\*\":\"Kiss\",\n",
    "    u\":X\":\"Kiss\",\n",
    "    u\";‑\\)\":\"Wink or smirk\",\n",
    "    u\";\\)\":\"Wink or smirk\",\n",
    "    u\"\\*-\\)\":\"Wink or smirk\",\n",
    "    u\"\\*\\)\":\"Wink or smirk\",\n",
    "    u\";‑\\]\":\"Wink or smirk\",\n",
    "    u\";\\]\":\"Wink or smirk\",\n",
    "    u\";\\^\\)\":\"Wink or smirk\",\n",
    "    u\":‑,\":\"Wink or smirk\",\n",
    "    u\";D\":\"Wink or smirk\",\n",
    "    u\":‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"X‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":‑Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":‑/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\">:/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":‑\\|\":\"Straight face\",\n",
    "    u\":\\|\":\"Straight face\",\n",
    "    u\":$\":\"Embarrassed or blushing\",\n",
    "    u\":‑x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":‑#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":‑&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\"O:‑\\)\":\"Angel, saint or innocent\",\n",
    "    u\"O:\\)\":\"Angel, saint or innocent\",\n",
    "    u\"0:‑3\":\"Angel, saint or innocent\",\n",
    "    u\"0:3\":\"Angel, saint or innocent\",\n",
    "    u\"0:‑\\)\":\"Angel, saint or innocent\",\n",
    "    u\"0:\\)\":\"Angel, saint or innocent\",\n",
    "    u\":‑b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"0;\\^\\)\":\"Angel, saint or innocent\",\n",
    "    u\">:‑\\)\":\"Evil or devilish\",\n",
    "    u\">:\\)\":\"Evil or devilish\",\n",
    "    u\"\\}:‑\\)\":\"Evil or devilish\",\n",
    "    u\"\\}:\\)\":\"Evil or devilish\",\n",
    "    u\"3:‑\\)\":\"Evil or devilish\",\n",
    "    u\"3:\\)\":\"Evil or devilish\",\n",
    "    u\">;\\)\":\"Evil or devilish\",\n",
    "    u\"\\|;‑\\)\":\"Cool\",\n",
    "    u\"\\|‑O\":\"Bored\",\n",
    "    u\":‑J\":\"Tongue-in-cheek\",\n",
    "    u\"#‑\\)\":\"Party all night\",\n",
    "    u\"%‑\\)\":\"Drunk or confused\",\n",
    "    u\"%\\)\":\"Drunk or confused\",\n",
    "    u\":-###..\":\"Being sick\",\n",
    "    u\":###..\":\"Being sick\",\n",
    "    u\"<:‑\\|\":\"Dump\",\n",
    "    u\"\\(>_<\\)\":\"Troubled\",\n",
    "    u\"\\(>_<\\)>\":\"Troubled\",\n",
    "    u\"\\(';'\\)\":\"Baby\",\n",
    "    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(~_~;\\) \\(・\\.・;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(-_-\\)zzz\":\"Sleeping\",\n",
    "    u\"\\(\\^_-\\)\":\"Wink\",\n",
    "    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n",
    "    u\"\\(\\+o\\+\\)\":\"Confused\",\n",
    "    u\"\\(o\\|o\\)\":\"Ultraman\",\n",
    "    u\"\\^_\\^\":\"Joyful\",\n",
    "    u\"\\(\\^_\\^\\)/\":\"Joyful\",\n",
    "    u\"\\(\\^O\\^\\)／\":\"Joyful\",\n",
    "    u\"\\(\\^o\\^\\)／\":\"Joyful\",\n",
    "    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"\\('_'\\)\":\"Sad or Crying\",\n",
    "    u\"\\(/_;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(;_;\":\"Sad of Crying\",\n",
    "    u\"\\(;_:\\)\":\"Sad or Crying\",\n",
    "    u\"\\(;O;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(:_;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(ToT\\)\":\"Sad or Crying\",\n",
    "    u\";_;\":\"Sad or Crying\",\n",
    "    u\";-;\":\"Sad or Crying\",\n",
    "    u\";n;\":\"Sad or Crying\",\n",
    "    u\";;\":\"Sad or Crying\",\n",
    "    u\"Q\\.Q\":\"Sad or Crying\",\n",
    "    u\"T\\.T\":\"Sad or Crying\",\n",
    "    u\"QQ\":\"Sad or Crying\",\n",
    "    u\"Q_Q\":\"Sad or Crying\",\n",
    "    u\"\\(-\\.-\\)\":\"Shame\",\n",
    "    u\"\\(-_-\\)\":\"Shame\",\n",
    "    u\"\\(一一\\)\":\"Shame\",\n",
    "    u\"\\(；一_一\\)\":\"Shame\",\n",
    "    u\"\\(=_=\\)\":\"Tired\",\n",
    "    u\"\\(=\\^\\·\\^=\\)\":\"cat\",\n",
    "    u\"\\(=\\^\\·\\·\\^=\\)\":\"cat\",\n",
    "    u\"=_\\^=\t\":\"cat\",\n",
    "    u\"\\(\\.\\.\\)\":\"Looking down\",\n",
    "    u\"\\(\\._\\.\\)\":\"Looking down\",\n",
    "    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n",
    "    u\"\\(\\・\\・?\":\"Confusion\",\n",
    "    u\"\\(?_?\\)\":\"Confusion\",\n",
    "    u\">\\^_\\^<\":\"Normal Laugh\",\n",
    "    u\"<\\^!\\^>\":\"Normal Laugh\",\n",
    "    u\"\\^/\\^\":\"Normal Laugh\",\n",
    "    u\"\\（\\*\\^_\\^\\*）\" :\"Normal Laugh\",\n",
    "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(^\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^—\\^\\）\":\"Normal Laugh\",\n",
    "    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n",
    "    u\"\\（\\^—\\^\\）\":\"Waving\",\n",
    "    u\"\\(;_;\\)/~~~\":\"Waving\",\n",
    "    u\"\\(\\^\\.\\^\\)/~~~\":\"Waving\",\n",
    "    u\"\\(-_-\\)/~~~ \\($\\·\\·\\)/~~~\":\"Waving\",\n",
    "    u\"\\(T_T\\)/~~~\":\"Waving\",\n",
    "    u\"\\(ToT\\)/~~~\":\"Waving\",\n",
    "    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n",
    "    u\"\\(\\*_\\*\\)\":\"Amazed\",\n",
    "    u\"\\(\\*_\\*;\":\"Amazed\",\n",
    "    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n",
    "    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n",
    "    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n",
    "    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n",
    "    u'\\(-\"-\\)':\"Worried\",\n",
    "    u\"\\(ーー;\\)\":\"Worried\",\n",
    "    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n",
    "    u\"\\(\\＾ｖ\\＾\\)\":\"Happy\",\n",
    "    u\"\\(\\＾ｕ\\＾\\)\":\"Happy\",\n",
    "    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n",
    "    u\"\\(\\^O\\^\\)\":\"Happy\",\n",
    "    u\"\\(\\^o\\^\\)\":\"Happy\",\n",
    "    u\"\\)\\^o\\^\\(\":\"Happy\",\n",
    "    u\":O o_O\":\"Surprised\",\n",
    "    u\"o_0\":\"Surprised\",\n",
    "    u\"o\\.O\":\"Surpised\",\n",
    "    u\"\\(o\\.o\\)\":\"Surprised\",\n",
    "    u\"oO\":\"Surprised\",\n",
    "    u\"\\(\\*￣m￣\\)\":\"Dissatisfied\",\n",
    "    u\"\\(‘A`\\)\":\"Snubbed or Deflated\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoticons(text):\n",
    "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
    "    return emoticon_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "def remove_html(text):\n",
    "    return BeautifulSoup(text, \"lxml\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words_str=[\"AFAIK=As Far As I Know\",\"AFK=Away From Keyboard\",\"ASAP=As Soon As Possible\",\"ATK=At The Keyboard\",\n",
    "                \"ATM=At The Moment\",\"A3=Anytime, Anywhere, Anyplace\",\"BAK=Back At Keyboard\",\"BBL=Be Back Later\",\"BBS=Be Back Soon\",\"BFN=Bye For Now\",\n",
    "                \"B4N=Bye For Now\",\"BRB=Be Right Back\",\"BRT=Be Right There\",\"BTW=By The Way,B4=Before\",\"B4N=Bye For Now\",\n",
    "                \"CU=See You\",\"CUL8R=See You Later\",\"CYA=See You\",\"FAQ=Frequently Asked Questions\",\"FC=Fingers Crossed\",\"FWIW=For What It's Worth\",\n",
    "                \"FYI=For Your Information\",\"GAL=Get A Life\",\"GG=Good Game\",\"GN=Good Night\",\"GMTA=Great Minds Think Alike\",\n",
    "                \"GR8=Great!\",\"G9=Genius\",\"IC=I See\",\"ICQ=I Seek you\",\"ILU=I Love You\",\"IMHO=In My Honest/Humble Opinion\",\n",
    "                \"IMO=In My Opinion\",\"IOW=In Other Words\",\"IRL=In Real Life\",\"KISS=Keep It Simple\", \"Stupid,LDR=Long Distance Relationship\",\n",
    "                \"LMAO=Laugh My A.. Off\",\"LOL=Laughing Out Loud\",\"LTNS=Long Time No See\",\"L8R=Later\",\"MTE=My Thoughts Exactly\",\"M8=Mate\",\"NRN=No Reply Necessary\",\n",
    "                \"OIC=Oh I See\",\"PITA=Pain In The A..\",\"PRT=Party\",\"PRW=Parents Are Watching\",\"QPSA?=Que Pasa?\",\"ROFL=Rolling On The Floor Laughing\",\n",
    "                \"ROFLOL=Rolling On The Floor Laughing Out Loud\",\"ROTFLMAO=Rolling On The Floor Laughing My A.. Off\",\"SK8=Skate\",\n",
    "                \"STATS=Your sex and age\",\"ASL=Age, Sex, Location\",\"THX=Thank You\",\"TTFN=Ta-Ta For Now!\",\"TTYL=Talk To You Later\",\"U=You\",\n",
    "                \"U2=You Too\",\"U4E=Yours For Ever\",\"WB=Welcome Back\",\"WTF=What The F...\",\"WTG=Way To Go!\",\"WUF=Where Are You From?\",\"W8=Wait...\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words_map_dict = {}\n",
    "chat_words_list = []\n",
    "for i in chat_words_str:\n",
    "    cw = i.split(\"=\")[0]\n",
    "    cw_expanded = i.split(\"=\")[1]\n",
    "    chat_words_list.append(cw)\n",
    "    chat_words_map_dict[cw] = cw_expanded\n",
    "chat_words_list = set(chat_words_list)\n",
    "\n",
    "def chat_words_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words_list:\n",
    "            new_text.append(chat_words_map_dict[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this would be applied to all new user input before applying the model to predict a response\n",
    "def process_text(data, col):\n",
    "    data[col] = data[col].astype(str)\n",
    "    \n",
    "    #remove handles\n",
    "    data[col] = data[col].apply(lambda x : ' '.join([w.lower() for w in x.split() if not w.startswith(('@','.@','\".@','\"@','http')) ]))\n",
    "    print('Removal of handles completed')\n",
    "    #lowercase\n",
    "    data[col] = data[col].str.lower()\n",
    "    \n",
    "    data[col] = data[col].apply(lambda text: remove_punctuation(text))\n",
    "    print('Removal of punctuation completed')\n",
    "    data[col] = data[col].apply(lambda text: remove_stopwords(text))\n",
    "    print('Removal of stopwords completed')\n",
    "\n",
    "    data[col] = data[col].apply(lambda text: lemmatize_words(text))\n",
    "    print('Lemmatization completed')\n",
    "\n",
    "    data[col] = data[col].apply(lambda text: remove_emoji(text))\n",
    "    data[col] = data[col].apply(lambda text: remove_emoticons(text))\n",
    "    print('Removal of emoticons and emojis completed')\n",
    "    \n",
    "    data[col] = data[col].apply(lambda text: remove_urls(text))\n",
    "    data[col] = data[col].apply(lambda text: remove_html(text))\n",
    "    print('Removal of urls and html completed')\n",
    "    \n",
    "    data[col] = data[col].apply(lambda text: chat_words_conversion(text))\n",
    "    print('Chat word conversion completed')\n",
    "\n",
    "    #spell checking commented out as ran too long \n",
    "    #data[col] = data[col].apply(lambda text: correct_spellings(text))\n",
    "    #print('Spelling corrections completed')\n",
    "    print('Text processing for {} text completed'.format(col))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removal of handles completed\n",
      "Removal of punctuation completed\n",
      "Removal of stopwords completed\n",
      "Lemmatization completed\n",
      "Removal of emoticons and emojis completed\n",
      "Removal of urls and html completed\n",
      "Chat word conversion completed\n",
      "Text processing for question text completed\n"
     ]
    }
   ],
   "source": [
    "data = process_text(data, 'question')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For responses, these will be kept relatively raw as the purpose of the chatbot will be to retrieve the most relevant response and output it to the user, therefore we want it to remain as a sentence.\n",
    "\n",
    "Handles will be removed so responses are more general (not targetd at a specific user. If we are seeking to create a chatbot for the purpose of responding to tweets, the handle of the user that created the question would be selected from the dataframe and a rule put in place to generate the handle at the start of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlottefettes/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:314: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "#remove handles, urls and htmls from responses\n",
    "data['response'] = data['response'].apply(lambda x : ' '.join([w.lower() for w in x.split() if not w.startswith(('@','.@','\".@','\"@','http')) ]))\n",
    "data['response'] = data['response'].apply(lambda text: remove_urls(text))\n",
    "data['response'] = data['response'].apply(lambda text: remove_html(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id_x</th>\n",
       "      <th>question</th>\n",
       "      <th>response</th>\n",
       "      <th>author_id_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115712</td>\n",
       "      <td>worst customer service</td>\n",
       "      <td>can you please send us a private message, so t...</td>\n",
       "      <td>sprintcare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>115712</td>\n",
       "      <td>worst customer service</td>\n",
       "      <td>i would love the chance to review the account ...</td>\n",
       "      <td>sprintcare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115712</td>\n",
       "      <td>worst customer service</td>\n",
       "      <td>hello! we never like our customers to feel lik...</td>\n",
       "      <td>sprintcare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115713</td>\n",
       "      <td>y’all lie “great” connection 5 bar lte still w...</td>\n",
       "      <td>h there! we'd definitely like to work with you...</td>\n",
       "      <td>sprintcare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>115715</td>\n",
       "      <td>whenever contact customer support tell shortco...</td>\n",
       "      <td>please send me a private message so that i can...</td>\n",
       "      <td>sprintcare</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  author_id_x                                           question  \\\n",
       "0      115712                             worst customer service   \n",
       "1      115712                             worst customer service   \n",
       "2      115712                             worst customer service   \n",
       "3      115713  y’all lie “great” connection 5 bar lte still w...   \n",
       "4      115715  whenever contact customer support tell shortco...   \n",
       "\n",
       "                                            response author_id_y  \n",
       "0  can you please send us a private message, so t...  sprintcare  \n",
       "1  i would love the chance to review the account ...  sprintcare  \n",
       "2  hello! we never like our customers to feel lik...  sprintcare  \n",
       "3  h there! we'd definitely like to work with you...  sprintcare  \n",
       "4  please send me a private message so that i can...  sprintcare  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save processed data\n",
    "data.to_pickle(\"processed_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for model\n",
    "\n",
    "### Creating false responses for questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.nn import init\n",
    "import torch.nn.utils.rnn \n",
    "import datetime\n",
    "import operator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"processed_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create false examples\n",
    "def shuffle_dataframe(dataframe):\n",
    "    return dataframe.reindex(np.random.permutation(dataframe.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take sample of data \n",
    "data = shuffle_dataframe(data)\n",
    "data_sample = data[:25000]\n",
    "data_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlottefettes/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "data_false = data_sample.copy(deep=True)\n",
    "\n",
    "data_false_question = data_false[['author_id_x','question']]\n",
    "data_false_response = data_false[['author_id_y','response']]\n",
    "\n",
    "data_false_response = shuffle_dataframe(data_false_response)\n",
    "\n",
    "data_false_question.reset_index(drop=True, inplace=True)\n",
    "data_false_response.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_false = pd.concat([data_false_question, data_false_response], axis=1)\n",
    "\n",
    "df_false['label'] = 0\n",
    "\n",
    "data_sample['label'] = 1\n",
    "\n",
    "df = data_sample.append(df_false, ignore_index=True)\n",
    "\n",
    "df = shuffle_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([25000.,     0., 25000.]),\n",
       " array([0.        , 0.33333333, 0.66666667, 1.        ]),\n",
       " <a list of 3 Patch objects>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD4CAYAAADsKpHdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAYUUlEQVR4nO3dfWyV9f3/8ec5p2xQCrXnlNpAcAu2bJOwtFLGTSYgdmQR5/gtxsRsGhuYGCZJ6Wamshs35uyiUIW2cSFE58Ifmiwwf/lmc6kVutkxyxA3NMpq1YwBlvYcoAxYb7i+f3g8mV+oLeUc2tLnIyHpua7rc53P2+vmdV2fc85lKAiCAEnSmBce7g5IkkYGA0GSBBgIkqQkA0GSBBgIkqQkA0GSBEDWcHfgUhw+fHhI7fLz8+no6Ehzb0Y2ax4brHlsuJSap06d2u887xAkSYCBIElKMhAkSYCBIElKMhAkSYCBIElKGvBrpx0dHdTV1XH8+HFCoRDl5eXcfPPNPP/887z00ktMnjwZgDvuuIPrr78egB07dtDY2Eg4HKaiooKSkhIA2traqKuro7u7m9LSUioqKgiFQvT09FBbW0tbWxuTJk2isrKSgoKCDJYtSfq/BgyESCTCnXfeyYwZMzhz5gwPPPAAX/ziFwFYvnw5t95668eWP3ToEM3NzWzatIlEIsGGDRt48sknCYfDbN26ldWrV1NcXMyjjz7K/v37KS0tpbGxkYkTJ7JlyxZeeeUVtm/fzrp16zJTsSTpggYcMsrLy2PGjBkATJgwgWnTphGPx/tdvqWlhYULFzJu3DgKCgooLCyktbWVRCLBmTNnmDlzJqFQiEWLFtHS0gLA3r17WbJkCQDz58/nwIED+L9pkKTL66J+qdze3s67775LUVERb731Fi+++CJNTU3MmDGDu+66i5ycHOLxOMXFxak20WiUeDxOJBIhFoulpsdisVSwxOPx1LxIJEJ2djZdXV2p4aiPNDQ00NDQAEB1dTX5+flDKvqD/7dwSO1Gsw+GuwODcPWO5rSuLysra8j7yGiV7ppHw7EyGvbtdMv6/69mZN8edCCcPXuWjRs3cvfdd5Odnc2yZcu47bbbAHjuued49tlnWbNmTb9X9p90xX+heaFQ6Lxp5eXllJeXp16PtZ+rX+nSvT19pIGuVL29vcP36Ire3l42btzIDTfcwLx58wC46qqrCIfDhMNhbrrpJt555x3gwyv/zs7OVNt4PE40Gj1vemdnJ9Fo9Lw2fX19nD59mpycnIssU5J0KQYMhCAIeOqpp5g2bRq33HJLanoikUj9/eqrrzJ9+nQAysrKaG5upqenh/b2do4cOUJRURF5eXlMmDCBgwcPEgQBTU1NlJWVATBnzhx27doFwJ49e5g1a9YF7xAkSZkz4JDR22+/TVNTE9dccw33338/8OFXTF955RXee+89QqEQU6ZM4Z577gFg+vTpLFiwgKqqKsLhMCtXriQc/jB3Vq1aRX19Pd3d3ZSUlFBaWgrA0qVLqa2tZe3ateTk5FBZWZmpeiVJ/QgFo/jrPEN9/HXft28deCFddpGtL6R1fWNxPD3dNXusjExX72j28deSpMwxECRJgIEgSUoyECRJgIEgSUoyECRJgIEgSUoyECRJgIEgSUoyECRJgIEgSUoyECRJgIEgSUoyECRJgIEgSUoyECRJgIEgSUoyECRJgIEgSUoyECRJgIEgSUoyECRJgIEgSUoyECRJgIEgSUoyECRJgIEgSUoyECRJgIEgSUoyECRJgIEgSUoyECRJAGQNtEBHRwd1dXUcP36cUChEeXk5N998M6dOnaKmpoZjx44xZcoU1q1bR05ODgA7duygsbGRcDhMRUUFJSUlALS1tVFXV0d3dzelpaVUVFQQCoXo6emhtraWtrY2Jk2aRGVlJQUFBZmtXJL0MQPeIUQiEe68805qamp45JFHePHFFzl06BA7d+5k9uzZbN68mdmzZ7Nz504ADh06RHNzM5s2bWL9+vVs27aNc+fOAbB161ZWr17N5s2bOXr0KPv37wegsbGRiRMnsmXLFpYvX8727dszWLIk6UIGDIS8vDxmzJgBwIQJE5g2bRrxeJyWlhYWL14MwOLFi2lpaQGgpaWFhQsXMm7cOAoKCigsLKS1tZVEIsGZM2eYOXMmoVCIRYsWpdrs3buXJUuWADB//nwOHDhAEASZqFeS1I8Bh4z+W3t7O++++y5FRUWcOHGCvLw84MPQOHnyJADxeJzi4uJUm2g0SjweJxKJEIvFUtNjsRjxeDzV5qN5kUiE7Oxsurq6mDx58sfev6GhgYaGBgCqq6vJz8+/2HoB+GBIrZRpQ92e/cnKykr7Oke6dNfssTIyZWrfHnQgnD17lo0bN3L33XeTnZ3d73L9Xdl/0hX/heaFQqHzppWXl1NeXp563dHR8Uld1iiT7u2Zn58/5vaRsVjzWNTb2zvk7Tx16tR+5w3qW0a9vb1s3LiRG264gXnz5gGQm5tLIpEAIJFIpK7mY7EYnZ2dqbbxeJxoNHre9M7OTqLR6Hlt+vr6OH36dOoDaknS5TFgIARBwFNPPcW0adO45ZZbUtPLysrYvXs3ALt372bu3Lmp6c3NzfT09NDe3s6RI0coKioiLy+PCRMmcPDgQYIgoKmpibKyMgDmzJnDrl27ANizZw+zZs264B2CJClzBhwyevvtt2lqauKaa67h/vvvB+COO+5gxYoV1NTU0NjYSH5+PlVVVQBMnz6dBQsWUFVVRTgcZuXKlYTDH+bOqlWrqK+vp7u7m5KSEkpLSwFYunQptbW1rF27lpycHCorKzNVrySpH6FgFH+d5/Dhw0Nq1/ftW9PcE6VDZOsLaV3fWBxPT3fNHisj09U7mofvMwRJ0pXPQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAZA10AL19fXs27eP3NxcNm7cCMDzzz/PSy+9xOTJkwG44447uP766wHYsWMHjY2NhMNhKioqKCkpAaCtrY26ujq6u7spLS2loqKCUChET08PtbW1tLW1MWnSJCorKykoKMhUvZKkfgx4h7BkyRIeeuih86YvX76cxx57jMceeywVBocOHaK5uZlNmzaxfv16tm3bxrlz5wDYunUrq1evZvPmzRw9epT9+/cD0NjYyMSJE9myZQvLly9n+/bt6axPkjRIAwbCddddR05OzqBW1tLSwsKFCxk3bhwFBQUUFhbS2tpKIpHgzJkzzJw5k1AoxKJFi2hpaQFg7969LFmyBID58+dz4MABgiAYekWSpCEZcMioPy+++CJNTU3MmDGDu+66i5ycHOLxOMXFxallotEo8XicSCRCLBZLTY/FYsTjcQDi8XhqXiQSITs7m66urtRwlCTp8hhSICxbtozbbrsNgOeee45nn32WNWvW9Htl/0lX/BeaFwqFLrhsQ0MDDQ0NAFRXV5Ofn3+xXQfggyG1UqYNdXv2JysrK+3rHOnSXbPHysiUqX17SIFw1VVXpf6+6aab+MUvfgF8eOXf2dmZmhePx4lGo+dN7+zsJBqNfqxNLBajr6+P06dP9ztEVV5eTnl5eep1R0fHULqvESrd2zM/P3/M7SNjseaxqLe3d8jbeerUqf3OG9LXThOJROrvV199lenTpwNQVlZGc3MzPT09tLe3c+TIEYqKisjLy2PChAkcPHiQIAhoamqirKwMgDlz5rBr1y4A9uzZw6xZs/q9Q5AkZc6AdwhPPPEEb775Jl1dXdx7773cfvvtvPHGG7z33nuEQiGmTJnCPffcA8D06dNZsGABVVVVhMNhVq5cSTj8YeasWrWK+vp6uru7KSkpobS0FIClS5dSW1vL2rVrycnJobKyMoPlSpL6EwpG8Vd6Dh8+PKR2fd++Nc09UTpEtr6Q1vWNxeGTdNfssTIyXb2jeeQMGUmSrjwGgiQJMBAkSUkGgiQJMBAkSUkGgiQJMBAkSUkGgiQJMBAkSUkGgiQJMBAkSUkGgiQJMBAkSUkGgiQJMBAkSUkGgiQJMBAkSUkGgiQJMBAkSUkGgiQJMBAkSUkGgiQJMBAkSUkGgiQJMBAkSUkGgiQJMBAkSUkGgiQJMBAkSUkGgiQJMBAkSUkGgiQJMBAkSUlZAy1QX1/Pvn37yM3NZePGjQCcOnWKmpoajh07xpQpU1i3bh05OTkA7Nixg8bGRsLhMBUVFZSUlADQ1tZGXV0d3d3dlJaWUlFRQSgUoqenh9raWtra2pg0aRKVlZUUFBRksGRJ0oUMeIewZMkSHnrooY9N27lzJ7Nnz2bz5s3Mnj2bnTt3AnDo0CGam5vZtGkT69evZ9u2bZw7dw6ArVu3snr1ajZv3szRo0fZv38/AI2NjUycOJEtW7awfPlytm/fnu4aJUmDMGAgXHfddamr/4+0tLSwePFiABYvXkxLS0tq+sKFCxk3bhwFBQUUFhbS2tpKIpHgzJkzzJw5k1AoxKJFi1Jt9u7dy5IlSwCYP38+Bw4cIAiCdNYoSRqEAYeMLuTEiRPk5eUBkJeXx8mTJwGIx+MUFxenlotGo8TjcSKRCLFYLDU9FosRj8dTbT6aF4lEyM7Opquri8mTJ5/3vg0NDTQ0NABQXV1Nfn7+ULrPB0NqpUwb6vbsT1ZWVtrXOdKlu2aPlZEpU/v2kAKhP/1d2X/SFf+F5oVCoQsuW15eTnl5eep1R0fHRfZQI1m6t2d+fv6Y20fGYs1jUW9v75C389SpU/udN6RvGeXm5pJIJABIJBKpq/lYLEZnZ2dquXg8TjQaPW96Z2cn0Wj0vDZ9fX2cPn36vCEqSVLmDSkQysrK2L17NwC7d+9m7ty5qenNzc309PTQ3t7OkSNHKCoqIi8vjwkTJnDw4EGCIKCpqYmysjIA5syZw65duwDYs2cPs2bN6vcOQZKUOQMOGT3xxBO8+eabdHV1ce+993L77bezYsUKampqaGxsJD8/n6qqKgCmT5/OggULqKqqIhwOs3LlSsLhDzNn1apV1NfX093dTUlJCaWlpQAsXbqU2tpa1q5dS05ODpWVlRksV5LUn1Awir/Sc/jw4SG16/v2rWnuidIhsvWFtK5vLI6np7tmj5WR6eodzSPnMwRJ0pXHQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAQaCJCnJQJAkAZB1KY2/853vMH78eMLhMJFIhOrqak6dOkVNTQ3Hjh1jypQprFu3jpycHAB27NhBY2Mj4XCYiooKSkpKAGhra6Ouro7u7m5KS0upqKggFApdenWSpEG7pEAA+PGPf8zkyZNTr3fu3Mns2bNZsWIFO3fuZOfOnXzrW9/i0KFDNDc3s2nTJhKJBBs2bODJJ58kHA6zdetWVq9eTXFxMY8++ij79++ntLT0UrsmSboIaR8yamlpYfHixQAsXryYlpaW1PSFCxcybtw4CgoKKCwspLW1lUQiwZkzZ5g5cyahUIhFixal2kiSLp9LvkN45JFHAPjKV75CeXk5J06cIC8vD4C8vDxOnjwJQDwep7i4ONUuGo0Sj8eJRCLEYrHU9FgsRjwev+B7NTQ00NDQAEB1dTX5+flD6vMHQ2qlTBvq9uxPVlZW2tc50qW7Zo+VkSlT+/YlBcKGDRuIRqOcOHGCn/3sZ0ydOrXfZYMguKjpF1JeXk55eXnqdUdHx+A7qxEv3dszPz9/zO0jY7Hmsai3t3fI2/mTztOXNGQUjUYByM3NZe7cubS2tpKbm0sikQAgkUikPl+IxWJ0dnam2sbjcaLR6HnTOzs7U+uVJF0+Qw6Es2fPcubMmdTff/vb37jmmmsoKytj9+7dAOzevZu5c+cCUFZWRnNzMz09PbS3t3PkyBGKiorIy8tjwoQJHDx4kCAIaGpqoqysLA2lSZIuxpCHjE6cOMHjjz8OQF9fH1/+8pcpKSnh2muvpaamhsbGRvLz86mqqgJg+vTpLFiwgKqqKsLhMCtXriQc/jCPVq1aRX19Pd3d3ZSUlPgNI0kaBqHgYgbxR5jDhw8PqV3ft29Nc0+UDpGtL6R1fWNxPD3dNXusjExX72geeZ8hSJKuHAaCJAkwECRJSQaCJAkwECRJSQaCJAkwECRJSQaCJAkwECRJSQaCJAkwECRJSQaCJAkwECRJSQaCJAkwECRJSQaCJAkwECRJSQaCJAkwECRJSQaCJAkwECRJSQaCJAkwECRJSQaCJAkwECRJSQaCJAkwECRJSQaCJAkwECRJSQaCJAkwECRJSQaCJAkwECRJSVnD3YGP7N+/n6effppz585x0003sWLFiuHukiSNKSPiDuHcuXNs27aNhx56iJqaGl555RUOHTo03N2SpDFlRARCa2srhYWFXH311WRlZbFw4UJaWlqGu1uSNKaMiCGjeDxOLBZLvY7FYvzjH/84b7mGhgYaGhoAqK6uZurUqUN7w//ZO7R2GnWGvI+MYmmt2WNlxMrEvj0i7hCCIDhvWigUOm9aeXk51dXVVFdXX9L7PfDAA5fUfjSy5rHBmseGTNU8IgIhFovR2dmZet3Z2UleXt4w9kiSxp4REQjXXnstR44cob29nd7eXpqbmykrKxvubknSmBJ5+OGHHx7uToTDYQoLC9myZQu///3vueGGG5g/f35G33PGjBkZXf9IZM1jgzWPDZmoORRcaABfkjTmjIghI0nS8DMQJEnACPkdQqYM9DiMIAh4+umnee211/j0pz/NmjVrRv1Y5EA1//GPf+S3v/0tAOPHj2fVqlV89rOfHYaeps9gH3vS2trK+vXrWbduXcY/o8q0wdT8xhtv8Mwzz9DX18ekSZP4yU9+Mgw9TY+B6j19+jSbN2+ms7OTvr4+vva1r3HjjTcOU2/To76+nn379pGbm8vGjRvPm5+R81dwherr6wvuu+++4OjRo0FPT0/wve99L/jnP//5sWX++te/Bo888khw7ty54O233w4efPDBYeptegym5rfeeivo6uoKgiAI9u3bNyZq/mi5hx9+OPj5z38e/PnPfx6GnqbPYGo+depUUFlZGRw7diwIgiA4fvz4cHQ1LQZT729+85vg17/+dRAEQXDixIng7rvvDnp6eoaju2nzxhtvBO+8805QVVV1wfmZOH9dsUNGg3kcxt69e1m0aBGhUIiZM2fy73//m0QiMUw9vnSDqflzn/scOTk5ABQXF3/s9x+j0WAfe/K73/2OefPmMXny5GHoZXoNpuY//elPzJs3j/z8fAByc3OHo6tpMZh6Q6EQZ8+eJQgCzp49S05ODuHw6D69XXfddalj9UIycf4a3f/FPsGFHocRj8fPW+ajA6a/ZUaTwdT83xobGyktLb0cXcuYwW7nV199lWXLll3u7mXEYGo+cuQIp06d4uGHH+b73/8+u3fvvtzdTJvB1PvVr36Vf/3rX6xevZrvfve7VFRUjPpAGEgmzl9X7GcIwSAehzGYZUaTi6nnwIEDvPzyy/z0pz/NdLcyajA1P/PMM3zzm9+8Yk4Qg6m5r6+Pd999lx/+8Id0d3fzgx/8gOLi4lH5bKfB1Pv666/zmc98hh/96Ed88MEHbNiwgc9//vNkZ2dfrm5edpk4f12xgTCYx2HEYjE6Ojo+cZnRZLCPAHn//ff55S9/yYMPPsikSZMuZxfTbjA1v/POOzz55JMAnDx5ktdee41wOMyXvvSly9rXdBnsvj1p0iTGjx/P+PHj+cIXvsD7778/KgNhMPW+/PLLrFixglAoRGFhIQUFBRw+fJiioqLL3d3LJhPnryvjkukCBvM4jLKyMpqamgiCgIMHD5KdnT2qA2EwNXd0dPD4449z3333jcqTw/81mJrr6upS/+bPn8+qVatGbRjA4Pftt956i76+Pv7zn//Q2trKtGnThqnHl2Yw9ebn5/P3v/8dgOPHj3P48GEKCgqGo7uXTSbOX1f0L5X37dvHr371K86dO8eNN97IN77xDf7whz8AsGzZMoIgYNu2bbz++ut86lOfYs2aNVx77bXD3OtLM1DNTz31FH/5y19SY4+RSOSSnx473Aaq+b/V1dUxZ86cUf+108HU/MILL/Dyyy8TDodZunQpy5cvH84uX5KB6o3H49TX16c+VP3617/OokWLhrPLl+yJJ57gzTffpKuri9zcXG6//XZ6e3uBzJ2/ruhAkCQN3hU7ZCRJujgGgiQJMBAkSUkGgiQJMBAkSUkGgiQJMBAkSUn/C7zHOXkJxXqaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['label'], bins=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    25000\n",
       "0    25000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-validation-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_samp, test_samp = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "training_samp, validation_samp = train_test_split(training_samp, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 8000, 10000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_samp), len(validation_samp), len(test_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels for training set:\n",
      " 1    16008\n",
      "0    15992\n",
      "Name: label, dtype: int64\n",
      "labels for validation set:\n",
      " 1    4005\n",
      "0    3995\n",
      "Name: label, dtype: int64\n",
      "labels for test set:\n",
      " 0    5013\n",
      "1    4987\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('labels for training set:\\n', training_samp['label'].value_counts())\n",
    "print('labels for validation set:\\n', validation_samp['label'].value_counts())\n",
    "print('labels for test set:\\n', test_samp['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_examples = len(training_samp)\n",
    "num_validation_examples = len(validation_samp)\n",
    "\n",
    "training_samp.to_csv(\"training_samp.csv\")\n",
    "validation_samp.to_csv(\"validation_samp.csv\")\n",
    "test_samp.to_csv(\"test_samp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper fuctions to create variables for training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(csvfile):\n",
    "    dataframe = pd.read_csv(csvfile)\n",
    "    return dataframe\n",
    "\n",
    "def create_vocab(dataframe):\n",
    "    vocab = []\n",
    "    word_freq = {}\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        \n",
    "        context_cell = row[\"question\"]\n",
    "        response_cell = row[\"response\"]\n",
    "        \n",
    "        train_words = str(context_cell).split() + str(response_cell).split()\n",
    "        \n",
    "        for word in train_words:\n",
    "          \n",
    "            if word.lower() not in vocab:\n",
    "                vocab.append(word.lower())         \n",
    "                       \n",
    "            if word.lower() not in word_freq:\n",
    "                word_freq[word.lower()] = 1\n",
    "            else:\n",
    "                word_freq[word.lower()] += 1\n",
    "    \n",
    "    word_freq_sorted = sorted(word_freq.items(), key=lambda item: item[1], reverse=True)\n",
    "    vocab = [\"<UNK>\"] + [pair[0] for pair in word_freq_sorted]\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def create_word_to_id(vocab):             \n",
    "    word_to_id = {word: id for id, word in enumerate(vocab)}\n",
    "    \n",
    "    return word_to_id\n",
    "\n",
    "\n",
    "def create_id_to_vec(word_to_id, glovefile): \n",
    "    lines = open(glovefile, 'r').readlines()\n",
    "    id_to_vec = {}\n",
    "    vector = None\n",
    "    \n",
    "    for line in lines:\n",
    "        word = line.split()[0]\n",
    "        vector = np.array(line.split()[1:], dtype='float32') \n",
    "        \n",
    "        if word in word_to_id:\n",
    "            id_to_vec[word_to_id[word]] = torch.FloatTensor(torch.from_numpy(vector))\n",
    "            \n",
    "    for word, id in word_to_id.items(): \n",
    "        if word_to_id[word] not in id_to_vec:\n",
    "            v = np.zeros(*vector.shape, dtype='float32')\n",
    "            v[:] = np.random.randn(*v.shape)*0.01\n",
    "            id_to_vec[word_to_id[word]] = torch.FloatTensor(torch.from_numpy(v))\n",
    "            \n",
    "    embedding_dim = id_to_vec[0].shape[0]\n",
    "    \n",
    "    return id_to_vec, embedding_dim\n",
    "\n",
    "\n",
    "def load_ids_and_labels(row, word_to_id):\n",
    "    context_ids = []\n",
    "    response_ids = []\n",
    "\n",
    "    context_cell = row['question']\n",
    "    response_cell = row['response']\n",
    "    label_cell = row['label']\n",
    "\n",
    "    max_context_len = 160\n",
    "    \n",
    "    context_words = str(context_cell).split()\n",
    "    if len(context_words) > max_context_len:\n",
    "        context_words = context_words[:max_context_len]\n",
    "    for word in context_words:\n",
    "        if word in word_to_id:\n",
    "            context_ids.append(word_to_id[word])\n",
    "        else: \n",
    "            context_ids.append(0) \n",
    "    \n",
    "    response_words = str(response_cell).split()\n",
    "    for word in response_words:\n",
    "        if word in word_to_id:\n",
    "            response_ids.append(word_to_id[word])\n",
    "        else: \n",
    "            response_ids.append(0)\n",
    "    \n",
    "    label = np.array(label_cell).astype(np.float32)\n",
    "\n",
    "    return context_ids, response_ids, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building, training and validating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model classes\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "            emb_size, \n",
    "            hidden_size, \n",
    "            vocab_size, \n",
    "            p_dropout): \n",
    "    \n",
    "            super(Encoder, self).__init__()\n",
    "             \n",
    "            self.emb_size = emb_size\n",
    "            self.hidden_size = hidden_size\n",
    "            self.vocab_size = vocab_size\n",
    "            self.p_dropout = p_dropout\n",
    "       \n",
    "            self.embedding = nn.Embedding(self.vocab_size, self.emb_size)\n",
    "            self.lstm = nn.LSTM(self.emb_size, self.hidden_size)\n",
    "            self.dropout_layer = nn.Dropout(self.p_dropout) \n",
    "\n",
    "            self.init_weights()\n",
    "             \n",
    "    def init_weights(self):\n",
    "        init.uniform_(self.lstm.weight_ih_l0, a = -0.01, b = 0.01)\n",
    "        init.orthogonal_(self.lstm.weight_hh_l0)\n",
    "        self.lstm.weight_ih_l0.requires_grad = True\n",
    "        self.lstm.weight_hh_l0.requires_grad = True\n",
    "        \n",
    "        embedding_weights = torch.FloatTensor(self.vocab_size, self.emb_size)\n",
    "            \n",
    "        for id, vec in id_to_vec.items():\n",
    "            embedding_weights[id] = vec\n",
    "        \n",
    "        self.embedding.weight = nn.Parameter(embedding_weights, requires_grad = True)\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        _, (last_hidden, _) = self.lstm(embeddings) #dimensions (num_layers * num_directions x batch_size x hidden_size)\n",
    "        last_hidden = self.dropout_layer(last_hidden[-1]) #accesses last lstm layer, dimensions (batch_size x hidden_size)\n",
    "\n",
    "        return last_hidden\n",
    "\n",
    "    \n",
    "class DualEncoder(nn.Module):    \n",
    "    def __init__(self, encoder):\n",
    "        super(DualEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.hidden_size = self.encoder.hidden_size\n",
    "        M = torch.FloatTensor(self.hidden_size, self.hidden_size)     \n",
    "        init.xavier_normal_(M)\n",
    "        self.M = nn.Parameter(M, requires_grad = True)\n",
    "\n",
    "    def forward(self, context_tensor, response_tensor):\n",
    "        \n",
    "        context_last_hidden = self.encoder(context_tensor) #dimensions (batch_size x hidden_size)\n",
    "        response_last_hidden = self.encoder(response_tensor) #dimensions (batch_size x hidden_size)\n",
    "        \n",
    "        context = context_last_hidden.mm(self.M) #dimensions (batch_size x hidden_size)\n",
    "        context = context.view(-1, 1, self.hidden_size) #dimensions (batch_size x 1 x hidden_size)\n",
    "        \n",
    "        response = response_last_hidden.view(-1, self.hidden_size, 1) #dimensions (batch_size x hidden_size x 1)\n",
    "        \n",
    "        score = torch.bmm(context, response).view(-1, 1) #dimensions (batch_size x 1 x 1) and (batch_size x 1)\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to call helper functions with number of examples and embedding dimension (GloVe embedding vectors used here)\n",
    "def creating_variables(embedding_dim):\n",
    "\n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Creating variables for training and validation...\")\n",
    "\n",
    "    training_dataframe = create_dataframe('training_samp.csv')\n",
    "    vocab = create_vocab(training_dataframe)\n",
    "    word_to_id = create_word_to_id(vocab)\n",
    "    id_to_vec, emb_dim = create_id_to_vec(word_to_id, 'glove.6B/glove.6B.%dd.txt' %embedding_dim)\n",
    "\n",
    "    validation_dataframe = create_dataframe('validation_samp.csv')\n",
    "\n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Variables created.\\n\")\n",
    "    \n",
    "    return training_dataframe, vocab, word_to_id, id_to_vec, emb_dim, validation_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create model instance with desired hyperparameters (only vary hidden_size and p_dropout)\n",
    "def creating_model(hidden_size, p_dropout):\n",
    "\n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Calling model...\")\n",
    "\n",
    "    encoder = Encoder(\n",
    "            emb_size = emb_dim,\n",
    "            hidden_size = hidden_size,\n",
    "            vocab_size = len(vocab),\n",
    "            p_dropout = p_dropout)\n",
    "\n",
    "    dual_encoder = DualEncoder(encoder)\n",
    "\n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Model created.\\n\")\n",
    "    print(dual_encoder)\n",
    "    \n",
    "    return encoder, dual_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional helper functions\n",
    "def increase_count(correct_count, score, label):\n",
    "    if ((score.data[0][0] >= 0.5) and (label.data[0][0] == 1.0)) or ((score.data[0][0] < 0.5) and (label.data[0][0]  == 0.0)):\n",
    "        correct_count +=1  \n",
    "   \n",
    "    return correct_count\n",
    "\n",
    "def get_accuracy(correct_count, dataframe):\n",
    "    accuracy = correct_count/(len(dataframe))\n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function defining training and validation method\n",
    "def train_model(learning_rate, l2_penalty, epochs): \n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Commencing training and validation...\\n\")\n",
    "    print(\"Data and Hyperparameter Overview:\\n\")\n",
    "    print(\"Number of training examples: {0}, Number of validation examples: {1}\".format(len(training_dataframe), len(validation_dataframe)))\n",
    "    print(\"Learning rate: {0}, Embedding Dimension: {1}, Hidden Size: {2}, Dropout: {3}, L2:{4}\\n\".format(learning_rate, emb_dim, encoder.hidden_size, encoder.p_dropout, l2_penalty))\n",
    "    print(\"Results:\\n\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(dual_encoder.parameters(), lr = learning_rate, weight_decay = l2_penalty)\n",
    "    loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "    best_validation_accuracy = 0.0\n",
    "     \n",
    "    for epoch in range(epochs): \n",
    "                     \n",
    "            shuffle_dataframe(training_dataframe)\n",
    "            sum_loss_training = 0.0\n",
    "            training_correct_count = 0\n",
    "            dual_encoder.train()\n",
    "\n",
    "            for index, row in training_dataframe.iterrows():            \n",
    "            \n",
    "                context_ids, response_ids, label = load_ids_and_labels(row, word_to_id)\n",
    "                context = autograd.Variable(torch.LongTensor(context_ids).view(-1,1), requires_grad = False) \n",
    "                response = autograd.Variable(torch.LongTensor(response_ids).view(-1, 1), requires_grad = False) \n",
    "                label = autograd.Variable(torch.FloatTensor(torch.from_numpy(np.array(label).reshape(1,1))), requires_grad = False) \n",
    "                             \n",
    "                score = dual_encoder(context, response)\n",
    "                loss = loss_func(score, label)\n",
    "                #sum_loss_training += loss.data[0]\n",
    "                sum_loss_training += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                training_correct_count = increase_count(training_correct_count, score, label)\n",
    "                                                    \n",
    "            training_accuracy = get_accuracy(training_correct_count, training_dataframe)\n",
    "                \n",
    "            shuffle_dataframe(validation_dataframe)\n",
    "            \n",
    "            validation_correct_count = 0\n",
    "            sum_loss_validation = 0.0\n",
    "            dual_encoder.eval()\n",
    "\n",
    "            for index, row in validation_dataframe.iterrows():\n",
    "                \n",
    "                context_ids, response_ids, label = load_ids_and_labels(row, word_to_id)\n",
    "                context = autograd.Variable(torch.LongTensor(context_ids).view(-1,1)) \n",
    "                response = autograd.Variable(torch.LongTensor(response_ids).view(-1, 1)) \n",
    "                label = autograd.Variable(torch.FloatTensor(torch.from_numpy(np.array(label).reshape(1,1)))) \n",
    "                \n",
    "                score = dual_encoder(context, response)\n",
    "                loss = loss_func(score, label)\n",
    "                sum_loss_validation += loss.item()\n",
    "                validation_correct_count = increase_count(validation_correct_count, score, label)\n",
    "                    \n",
    "            validation_accuracy = get_accuracy(validation_correct_count, validation_dataframe)\n",
    "                        \n",
    "            print(str(datetime.datetime.now()).split('.')[0], \n",
    "                  \"Epoch: {0}/{1}\".format(epoch,epochs),  \n",
    "                  \"TrainLoss: {0}\".format(sum_loss_training/len(training_dataframe)), \n",
    "                  \"TrainAccuracy: {0}\".format(training_accuracy), \n",
    "                  \"ValLoss: {0}\".format(sum_loss_validation/len(validation_dataframe)), \n",
    "                  \"ValAccuracy: {0}\".format(validation_accuracy))\n",
    "            \n",
    "            if validation_accuracy > best_validation_accuracy:\n",
    "                best_validation_accuracy = validation_accuracy\n",
    "                torch.save(dual_encoder.state_dict(), 'saved_model_samp.pt')\n",
    "                print(\"New best found and saved.\")\n",
    "                \n",
    "    print(str(datetime.datetime.now()).split('.')[0], \"Training and validation epochs finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-05 09:00:33 Creating variables for training and validation...\n",
      "2020-02-05 09:01:39 Variables created.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define embedding dimension (for glove embedding), create variables for training and validation\n",
    "training_dataframe, vocab, word_to_id, id_to_vec, emb_dim, validation_dataframe = creating_variables(embedding_dim = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-05 09:01:39 Calling model...\n",
      "2020-02-05 09:01:39 Model created.\n",
      "\n",
      "DualEncoder(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(43667, 300)\n",
      "    (lstm): LSTM(300, 100)\n",
      "    (dropout_layer): Dropout(p=0.85, inplace=False)\n",
      "  )\n",
      ")\n",
      "M\n",
      "encoder.embedding.weight\n",
      "encoder.lstm.weight_ih_l0\n",
      "encoder.lstm.weight_hh_l0\n",
      "encoder.lstm.bias_ih_l0\n",
      "encoder.lstm.bias_hh_l0\n"
     ]
    }
   ],
   "source": [
    "#define hidden size and dropout probability, creating model\n",
    "encoder, dual_encoder = creating_model(hidden_size = 100, \n",
    "                                       p_dropout = 0.85)\n",
    "\n",
    "for name, param in dual_encoder.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-05 09:01:39 Commencing training and validation...\n",
      "\n",
      "Data and Hyperparameter Overview:\n",
      "\n",
      "Number of training examples: 32000, Number of validation examples: 8000\n",
      "Learning rate: 0.01, Embedding Dimension: 300, Hidden Size: 100, Dropout: 0.85, L2:0.0001\n",
      "\n",
      "Results:\n",
      "\n",
      "2020-02-05 11:52:56 Epoch: 0/10 TrainLoss: 2.1231754573071098 TrainAccuracy: 0.50171875 ValLoss: 0.8747915415583399 ValAccuracy: 0.50025\n",
      "New best found and saved.\n",
      "2020-02-05 14:40:12 Epoch: 1/10 TrainLoss: 1.149710510901465 TrainAccuracy: 0.50021875 ValLoss: 0.7001029913139064 ValAccuracy: 0.50075\n",
      "New best found and saved.\n",
      "2020-02-05 19:32:41 Epoch: 2/10 TrainLoss: 1.0215891242468764 TrainAccuracy: 0.50096875 ValLoss: 0.7056017472544918 ValAccuracy: 0.500625\n",
      "2020-02-05 22:05:04 Epoch: 3/10 TrainLoss: 1.1513734656316552 TrainAccuracy: 0.5030625 ValLoss: 0.7712304085881333 ValAccuracy: 0.50025\n",
      "2020-02-06 00:37:30 Epoch: 4/10 TrainLoss: 1.178643086723236 TrainAccuracy: 0.49896875 ValLoss: 0.7062790361363441 ValAccuracy: 0.500125\n",
      "2020-02-06 03:08:55 Epoch: 5/10 TrainLoss: 1.2437586848940119 TrainAccuracy: 0.50028125 ValLoss: 0.7274023906841031 ValAccuracy: 0.50275\n",
      "New best found and saved.\n",
      "2020-02-06 05:49:32 Epoch: 6/10 TrainLoss: 1.0763970437252302 TrainAccuracy: 0.50228125 ValLoss: 0.7552063844742152 ValAccuracy: 0.498625\n",
      "2020-02-06 09:02:39 Epoch: 7/10 TrainLoss: 1.2476331638845508 TrainAccuracy: 0.50146875 ValLoss: 0.7073376694718608 ValAccuracy: 0.5\n",
      "2020-02-06 11:44:37 Epoch: 8/10 TrainLoss: 1.4850678539036353 TrainAccuracy: 0.50203125 ValLoss: 0.9178907273889125 ValAccuracy: 0.501875\n",
      "2020-02-06 14:26:36 Epoch: 9/10 TrainLoss: 1.4752166070701132 TrainAccuracy: 0.49890625 ValLoss: 0.7326650992349605 ValAccuracy: 0.495125\n",
      "2020-02-06 14:26:36 Training and validation epochs finished.\n"
     ]
    }
   ],
   "source": [
    "#choose learning rate and number of epochs, start training and validation epochs\n",
    "train_model(learning_rate = 0.01, \n",
    "            l2_penalty = 0.0001,\n",
    "            epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DualEncoder(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(43667, 300)\n",
       "    (lstm): LSTM(300, 100)\n",
       "    (dropout_layer): Dropout(p=0.85, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load saved model\n",
    "dual_encoder.load_state_dict(torch.load('saved_model_samp.pt'))\n",
    "\n",
    "dual_encoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model performance on prediction (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataframe = pd.read_csv('test_samp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing():\n",
    "    \n",
    "    test_correct_count = 0\n",
    "\n",
    "    for index, row in test_dataframe.iterrows():\n",
    "\n",
    "        context_ids, response_ids, label = load_ids_and_labels(row, word_to_id)\n",
    "        context = autograd.Variable(torch.LongTensor(context_ids).view(-1,1)) \n",
    "        response = autograd.Variable(torch.LongTensor(response_ids).view(-1, 1)) \n",
    "        label = autograd.Variable(torch.FloatTensor(torch.from_numpy(np.array(label).reshape(1,1)))) \n",
    "\n",
    "        score = dual_encoder(context, response)\n",
    "        test_correct_count = increase_count(test_correct_count, score, label)\n",
    "\n",
    "    test_accuracy = get_accuracy(test_correct_count, test_dataframe)\n",
    "    \n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.4999\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = testing()\n",
    "print(\"Test accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model to generate a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_response_ids(row, word_to_id):\n",
    "    response_ids = []\n",
    "\n",
    "    response_cell = row['response']\n",
    "    \n",
    "    response_words = str(response_cell).split()\n",
    "    for word in response_words:\n",
    "        if word in word_to_id:\n",
    "            response_ids.append(word_to_id[word])\n",
    "        else: \n",
    "            response_ids.append(0)\n",
    "    \n",
    "    return response_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_context_ids(context_cell, word_to_id):\n",
    "    context_ids = []\n",
    "    \n",
    "    #context_cell = df['question'][row]\n",
    "    max_context_len = 160\n",
    "\n",
    "    context_words = str(context_cell).split()\n",
    "    if len(context_words) > max_context_len:\n",
    "        context_words = context_words[:max_context_len]\n",
    "    for word in context_words:\n",
    "        if word in word_to_id:\n",
    "             context_ids.append(word_to_id[word])\n",
    "        else: \n",
    "            context_ids.append(0) \n",
    "    return context_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "since oct 18 run stop sync know anything\n"
     ]
    }
   ],
   "source": [
    "#make copy of test and reindex, select context, then shuffle and reindex\n",
    "test_dataframe_copy = test_dataframe.copy(deep=True)\n",
    "\n",
    "row = 0\n",
    "context_cell = test_dataframe_copy['question'][row]\n",
    "context_ids = load_context_ids(context_cell, word_to_id)\n",
    "\n",
    "#shuffle and reindex to avoid concerns over response selection based on matching index of question-response\n",
    "test_dataframe_copy = shuffle_dataframe(test_dataframe_copy)\n",
    "test_dataframe_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(context_cell)\n",
    "scores = {}\n",
    "for index, row in test_dataframe_copy.iterrows():\n",
    "\n",
    "    response_ids = load_response_ids(row, word_to_id)\n",
    "    context = autograd.Variable(torch.LongTensor(context_ids).view(-1,1)) \n",
    "    response = autograd.Variable(torch.LongTensor(response_ids).view(-1, 1)) \n",
    "\n",
    "    score = dual_encoder(context, response)\n",
    "    scores[index] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@166750 hello there. are you getting any error code or message? please provide us more details about the issue\n"
     ]
    }
   ],
   "source": [
    "#find index with max score from scores dictionary\n",
    "row = 0\n",
    "keymax = max(scores, key=scores.get) \n",
    "best_response = test_dataframe_copy['response'][keymax]\n",
    "print('@'+str(test_dataframe.iloc[row,1]) + ' ' + best_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the low accuracy, the response given to the selected test question appears to make sense.\n",
    "\n",
    "The accruacy achieved for train, validation and test sets are all relatively low (though similar, indicating no overfitting). To achieve a higher accuracy, more data is required. However, on this current machine, when attempts were made to use the entire dataset, a single epoch was taking days so that attempt was abandoned.\n",
    "\n",
    "For real-world purposes, and for a single business, a large set of input questions received from customers along with standard responses that the company would want to provide to its customers would be used to train the model, and these responses would be provided to the chatbot in responding to their customers' queries as above - the query processed, and then the best response retrieved from the set of acceptable responses provided by the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
