{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "import spacy\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from empath import Empath\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset/data.csv')\n",
    "df.loc[df['Label']== 0, 'Label'] = 'REAL'\n",
    "df.loc[df['Label']== 1, 'Label'] = 'FAKE'\n",
    "df.columns\n",
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the column URLs from the table\n",
    "df.drop(['URLs'], axis = 1, inplace = True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting only fake news from all the types of news and then replacing the 'fake' by 0\n",
    "df1 = pd.read_csv('Dataset/fake.csv')\n",
    "df1.columns\n",
    "df1['type'].value_counts()\n",
    "df1 = df1.loc[df1['type']=='fake']\n",
    "df1.loc[df1['type']== 'fake', 'type'] = 'FAKE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting some columns from the table and renaming them\\n\",\n",
    "df1 = df1[['title','text','type']]\n",
    "df1.columns = ['Headline', 'Body', 'Label']\n",
    "df1['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('Dataset/fake_or_real_news.csv')\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting few columns from the table and renaming the columns\n",
    "df2 = df2[['title','text','label']]\n",
    "df2.columns = ['Headline', 'Body', 'Label']\n",
    "df2.columns\n",
    "df2['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df3 = pd.read_csv('Dataset/train.csv')\n",
    "df3.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting few columns from the table and renaming the columns\n",
    "df3 = df3[['title','text','label']]\n",
    "df3.columns = ['Headline', 'Body', 'Label']\n",
    "df3.loc[df3['Label']== 0, 'Label'] = 'REAL'\n",
    "df3.loc[df3['Label']== 1, 'Label'] = 'FAKE'\n",
    "df3.columns\n",
    "df3['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appending df1,df2,df3 to df\n",
    "df = df.append(df1, ignore_index = True)\n",
    "df = df.append(df2, ignore_index = True)\n",
    "df = df.append(df3, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "\n",
    "# df.iloc[3647]\n",
    "# print(df['Headline'][3647])\n",
    "# print(len(df['Body'][3647]))\n",
    "#df = df.dropna(how='any',axis=0)\n",
    "cnt = 0\n",
    "ind = []\n",
    "for art in df['Body']:\n",
    "    #print(type(art))\n",
    "    if len(str(art)) < 10:\n",
    "        ind.append(cnt)\n",
    "    cnt+=1\n",
    "df = df.drop(df.index[ind])\n",
    "        \n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'].value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['headline_length'] = [len(str(a)) for a in df['Headline']]\n",
    "df['headline_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body_length'] = [len(a) for a in df['Body']]\n",
    "df['body_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Text\"] = df[\"Headline\"].map(str) + df[\"Body\"]\n",
    "y = df.Label\n",
    "y = y.astype('str')\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df['Text'],y, test_size=0.33)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-idf Bigrams\n",
    "#Initialize the `tfidf_vectorizer` \n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range = (2,2)) \n",
    "\n",
    "# Fit and transform the training data \n",
    "tfidf1_train = tfidf_vectorizer.fit_transform(X_train.astype('str')) \n",
    "\n",
    "# Transform the test set \n",
    "tfidf1_test = tfidf_vectorizer.transform(X_test.astype('str'))\n",
    "\n",
    "pickle.dump(tfidf1_train, open(\"tfidf1_train.pickle\", \"wb\"))\n",
    "\n",
    "pickle.dump(tfidf1_test, open(\"tfidf1_test.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 tfidf bigrams \n",
    "tfidf_vectorizer.get_feature_names()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(tfidf1_train, Y_train)\n",
    "pickle.dump(clf, open('tfidf_nb', 'wb'))\n",
    "pred = clf.predict(tfidf1_test)\n",
    "score = metrics.accuracy_score(Y_test, pred)\n",
    "print(\"Accuracy with Multinomial Naive Bayes:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(Y_test, pred, labels=['FAKE', 'REAL'])\n",
    "plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(tfidf1_train, Y_train)\n",
    "pickle.dump(clf, open('tfidf_gb', 'wb'))\n",
    "#model = pickle.load(open('tfidf_gb', 'rb'))\n",
    "pred = clf.predict(tfidf1_test)\n",
    "score = metrics.accuracy_score(Y_test, pred)\n",
    "print(\"Accuracy with Gradient Boosting:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(Y_test, pred, labels=['FAKE', 'REAL'])\n",
    "plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(tfidf1_train, Y_train)\n",
    "pickle.dump(clf, open('tfidf_rf', 'wb'))\n",
    "pred = clf.predict(tfidf1_test)\n",
    "score = metrics.accuracy_score(Y_test, pred)\n",
    "print(\"Accuracy with RandomForestClassifier:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(Y_test, pred, labels=['FAKE', 'REAL'])\n",
    "plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating the POS tags for all the articles and adding a new column by replacing text with their POS tags\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "x = []\n",
    "df[\"Text\"] = df[\"Headline\"].map(str) + df[\"Body\"]\n",
    "for text in df['Text']:\n",
    "    text_new = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        text_new.append(token.pos_)\n",
    "    txt = ' '.join(text_new)\n",
    "    x.append(txt)\n",
    "df['Text_pos'] = x\n",
    "df.to_pickle('newdata.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('newdata.pkl')\n",
    "cnt = 0\n",
    "ind = []\n",
    "for art in df['Body']:\n",
    "    #print(type(art))\n",
    "    if len(str(art)) < 10:\n",
    "        ind.append(cnt)\n",
    "    cnt+=1\n",
    "df = df.drop(df.index[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.Label\n",
    "y = y.astype('str')\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['Text_pos'],y, test_size=0.33)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the `tfidf_vectorizer` \n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range = (2,2)) \n",
    "\n",
    "# Fit and transform the training data \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(x_train.astype('str')) \n",
    "\n",
    "# Transform the test set \n",
    "tfidf_test = tfidf_vectorizer.transform(x_test.astype('str'))\n",
    "\n",
    "pickle.dump(tfidf_train, open(\"tfidf_train.pickle\", \"wb\"))\n",
    "\n",
    "pickle.dump(tfidf_test, open(\"tfidf_test.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer.get_feature_names()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(tfidf_train, y_train)\n",
    "pickle.dump(clf, open('pos_nb', 'wb'))\n",
    "pred = clf.predict(tfidf_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"Accuracy with Multinomial Naive Bayes:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(tfidf_train, y_train)\n",
    "pickle.dump(clf, open('pos_rf', 'wb'))\n",
    "pred = clf.predict(tfidf_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"Accuracy with RandomForestClassifier:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(tfidf_train, y_train)\n",
    "pickle.dump(clf, open('pos_gb', 'wb'))\n",
    "pred = clf.predict(tfidf_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"Accuracy with Gradient Boosting:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the score of semantic categories generated by Empath of each article and generating a tfidf vector of the unigrams \n",
    "lexicon = Empath()\n",
    "semantic = []\n",
    "cnt = 0\n",
    "df[\"Text\"] = df[\"Headline\"].map(str) + df[\"Body\"]\n",
    "\n",
    "for article in df['Text']:\n",
    "    if article == '':\n",
    "        continue\n",
    "    cnt+=1\n",
    "    d = lexicon.analyze(article, normalize = False)\n",
    "    x = []\n",
    "    for key, value in d.items():\n",
    "        x.append(value)\n",
    "    x = np.asarray(x)\n",
    "    semantic.append(x)\n",
    "df['Semantic'] = semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "a = lexicon.analyze(\"\")\n",
    "for key, value in a.items():\n",
    "    categories.append(key)\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF vector by taking the score for a semantic class as its frequency.\n",
    "sem = []\n",
    "for i in range(df.shape[0]):\n",
    "    a = []\n",
    "    for j in range(len(semantic[0])):\n",
    "        for k in range(int(semantic[i][j])):\n",
    "            a.append(categories[j])\n",
    "    b = \" \".join(a)\n",
    "    sem.append(b)\n",
    "#print(len(sem))\n",
    "df['Semantics'] = sem\n",
    "df.to_pickle('Semantic.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('Semantic.pkl')\n",
    "print(df.columns)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.Label\n",
    "y = y.astype('str')\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['Semantics'],y, test_size=0.33)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x_train))\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the `tfidf_vectorizer` \n",
    "tfidf2_vectorizer = TfidfVectorizer(stop_words='english', ngram_range = (1,1)) \n",
    "\n",
    "# Fit and transform the training data \n",
    "tfidf2_train = tfidf2_vectorizer.fit_transform(x_train.astype('str')) \n",
    "\n",
    "# Transform the test set \n",
    "tfidf2_test = tfidf2_vectorizer.transform(x_test.astype('str'))\n",
    "\n",
    "pickle.dump(tfidf2_train, open(\"tfidf2_train.pickle\", \"wb\"))\n",
    "\n",
    "pickle.dump(tfidf2_test, open(\"tfidf2_test.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "#type(x_train.tolist())\n",
    "clf.fit(x_train.tolist(), y_train)\n",
    "pickle.dump(clf, open('sem_nb', 'wb'))\n",
    "pred = clf.predict(x_test.tolist())\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"Accuracy with Multinomial Naive Bayes:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(x_train.tolist(), y_train)\n",
    "pickle.dump(clf, open('sem_rf', 'wb'))\n",
    "pred = clf.predict(x_test.tolist())\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"Accuracy with RandomForestClassifier:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(x_train.tolist(), y_train)\n",
    "pickle.dump(clf, open('sem_gb', 'wb'))\n",
    "pred = clf.predict(x_test.tolist())\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"Accuracy with Gradient Boosting:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining the 3 feature vectors\n",
    "import scipy.sparse as sp\n",
    "# ui = sp.vstack(tfidf_train, tfidf1_train)\n",
    "# yu = tfidf_train.data.tolist()\n",
    "# yu.append(tfidf1_train.tolist())\n",
    "# test = tfidf_test.data.tolist() + x_test.tolist()\n",
    "#print(type(tfidf_train), tfidf_train.shape)\n",
    "#print(type(tfidf1_train), tfidf1_train.shape)\n",
    "# print(type(x_train), x_train.shape)\n",
    "diff_n_rows = tfidf_train.shape[0] - tfidf1_train.shape[0]\n",
    "\n",
    "Xb_new = sp.vstack((tfidf1_train, sp.csr_matrix((diff_n_rows, tfidf1_train.shape[1])))) \n",
    "#where diff_n_rows is the difference of the number of rows between Xa and Xb\n",
    "\n",
    "c = sp.hstack((tfidf_train, Xb_new))\n",
    "\n",
    "diff_n_rows = c.shape[0] - tfidf2_train.shape[0]\n",
    "\n",
    "Xb_new = sp.vstack((tfidf2_train, sp.csr_matrix((diff_n_rows, tfidf2_train.shape[1])))) \n",
    "#where diff_n_rows is the difference of the number of rows between Xa and Xb\n",
    "\n",
    "X = sp.hstack((c, Xb_new))\n",
    "X\n",
    "\n",
    "dif_n_rows = tfidf_test.shape[0] - tfidf1_test.shape[0]\n",
    "\n",
    "Xb_ne = sp.vstack((tfidf1_test, sp.csr_matrix((dif_n_rows, tfidf1_test.shape[1])))) \n",
    "#where diff_n_rows is the difference of the number of rows between Xa and Xb\n",
    "\n",
    "d = sp.hstack((tfidf_test, Xb_ne))\n",
    "\n",
    "dif_n_rows = d.shape[0] - tfidf2_test.shape[0]\n",
    "\n",
    "Xb_ne = sp.vstack((tfidf2_test, sp.csr_matrix((dif_n_rows, tfidf2_test.shape[1])))) \n",
    "#where diff_n_rows is the difference of the number of rows between Xa and Xb\n",
    "\n",
    "Y = sp.hstack((d, Xb_ne))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "#print(type(train), type(y_train.tolist()))\n",
    "clf.fit(X, y_train)\n",
    "pickle.dump(clf, open('pos_sem_nb', 'wb'))\n",
    "pred = clf.predict(Y)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"Accuracy with Multinomial Naive Bayes:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(X, y_train)\n",
    "pickle.dump(clf, open('pos_sem_rf', 'wb'))\n",
    "pred = clf.predict(Y)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"Accuracy with RandomForestClassifier:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X, y_train)\n",
    "pickle.dump(clf, open('pos_sem_gb', 'wb'))\n",
    "pred = clf.predict(Y)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"Accuracy with Gradient Boosting:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Directly loading the final dateframe by loading the pickle file from the previously saved pickle file\n",
    "df = pd.read_pickle('Semantic.pkl')\n",
    "print(df.columns)\n",
    "print(df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.Label\n",
    "x_train, x_test, y_train, y_test = train_test_split(df,y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_text = x_train['Text']\n",
    "x_test_text = x_test['Text']\n",
    "x_train_text_pos = x_train['Text_pos']\n",
    "x_test_text_pos = x_test['Text_pos']\n",
    "x_train_semantics = x_train['Semantics']\n",
    "x_test_semantics = x_test['Semantics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-idf Bigrams\n",
    "#Initialize the `tfidf_vectorizer` \n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range = (2,2), max_features = 20000) \n",
    "\n",
    "# Fit and transform the training data \n",
    "tfidf1_train = tfidf_vectorizer.fit_transform(x_train_text.astype('str')) \n",
    "\n",
    "# Transform the test set \n",
    "tfidf1_test = tfidf_vectorizer.transform(x_test_text.astype('str'))\n",
    "\n",
    "pickle.dump(tfidf1_train, open(\"tfidf1_train.pickle\", \"wb\"))\n",
    "\n",
    "pickle.dump(tfidf1_test, open(\"tfidf1_test.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS\n",
    "#Initialize the `tfidf_vectorizer` \n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range = (2,2)) \n",
    "\n",
    "# Fit and transform the training data \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(x_train_text_pos.astype('str')) \n",
    "\n",
    "# Transform the test set \n",
    "tfidf_test = tfidf_vectorizer.transform(x_test_text_pos.astype('str'))\n",
    "\n",
    "pickle.dump(tfidf_train, open(\"tfidf_train.pickle\", \"wb\"))\n",
    "\n",
    "pickle.dump(tfidf_test, open(\"tfidf_test.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the `tfidf_vectorizer` \n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range = (1,1)) \n",
    "\n",
    "# Fit and transform the training data \n",
    "tfidf2_train = tfidf_vectorizer.fit_transform(x_train_semantics.astype('str')) \n",
    "\n",
    "# Transform the test set \n",
    "tfidf2_test = tfidf_vectorizer.transform(x_test_semantics.astype('str'))\n",
    "\n",
    "pickle.dump(tfidf2_train, open(\"tfidf_train.pickle\", \"wb\"))\n",
    "\n",
    "pickle.dump(tfidf2_test, open(\"tfidf_test.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttf1_train = tfidf1_train\n",
    "ttf1_test = tfidf1_test\n",
    "ttf_train = tfidf_train\n",
    "ttf_test = tfidf_test\n",
    "ttf2_train = tfidf2_train\n",
    "ttf2_test = tfidf2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Giving weights to each of the 3 feature vectors generated\n",
    "big_w = 0.35\n",
    "synt_w = 0.5\n",
    "sem_w = 0.15\n",
    "big_w *= 3\n",
    "synt_w *= 3\n",
    "sem_w *= 3\n",
    "tfidf1_train = big_w*ttf1_train\n",
    "tfidf1_test = big_w*ttf1_test\n",
    "tfidf_train = synt_w*ttf_train\n",
    "tfidf_test = synt_w*ttf_test\n",
    "tfidf2_train = sem_w*ttf2_train\n",
    "tfidf2_test = sem_w*ttf2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "# ui = sp.vstack(tfidf_train, tfidf1_train)\n",
    "# yu = tfidf_train.data.tolist()\n",
    "# yu.append(tfidf1_train.tolist())\n",
    "# test = tfidf_test.data.tolist() + x_test.tolist()\n",
    "#print(type(tfidf_train), tfidf_train.shape)\n",
    "#print(type(tfidf1_train), tfidf1_train.shape)\n",
    "# print(type(x_train), x_train.shape)\n",
    "diff_n_rows = tfidf_train.shape[0] - tfidf1_train.shape[0]\n",
    "\n",
    "Xb_new = sp.vstack((tfidf1_train, sp.csr_matrix((diff_n_rows, tfidf1_train.shape[1])))) \n",
    "#where diff_n_rows is the difference of the number of rows between Xa and Xb\n",
    "\n",
    "c = sp.hstack((tfidf_train, Xb_new))\n",
    "\n",
    "diff_n_rows = c.shape[0] - tfidf2_train.shape[0]\n",
    "\n",
    "Xb_new = sp.vstack((tfidf2_train, sp.csr_matrix((diff_n_rows, tfidf2_train.shape[1])))) \n",
    "#where diff_n_rows is the difference of the number of rows between Xa and Xb\n",
    "\n",
    "X = sp.hstack((c, Xb_new))\n",
    "X\n",
    "\n",
    "dif_n_rows = tfidf_test.shape[0] - tfidf1_test.shape[0]\n",
    "\n",
    "Xb_ne = sp.vstack((tfidf1_test, sp.csr_matrix((dif_n_rows, tfidf1_test.shape[1])))) \n",
    "#where diff_n_rows is the difference of the number of rows between Xa and Xb\n",
    "\n",
    "d = sp.hstack((tfidf_test, Xb_ne))\n",
    "\n",
    "dif_n_rows = d.shape[0] - tfidf2_test.shape[0]\n",
    "\n",
    "Xb_ne = sp.vstack((tfidf2_test, sp.csr_matrix((dif_n_rows, tfidf2_test.shape[1])))) \n",
    "#where diff_n_rows is the difference of the number of rows between Xa and Xb\n",
    "\n",
    "Y = sp.hstack((d, Xb_ne))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "#type(x_train.tolist())\n",
    "clf.fit(X, y_train)\n",
    "pickle.dump(clf, open('bi_pos_sem_nb', 'wb'))\n",
    "pred = clf.predict(Y)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"Accuracy with Multinomial Naive Bayes:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(X, y_train)\n",
    "pickle.dump(clf, open('bi_pos_sem_rf', 'wb'))\n",
    "pred = clf.predict(Y)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"Accuracy with RandomForestClassifier:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X, y_train)\n",
    "pickle.dump(clf, open('pos_gb', 'wb'))\n",
    "pred = clf.predict(Y)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"Accuracy with Gradient Boosting:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For testing any new article \n",
    "a = (open('a.txt'))\n",
    "x_test = a.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-idf Bigrams\n",
    "#Initialize the `tfidf_vectorizer` \n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range = (2,2), max_features = 20000) \n",
    "\n",
    "# Fit and transform the training data \n",
    "tfidf1_train = tfidf_vectorizer.fit_transform(x_train_text.astype('str')) \n",
    "\n",
    "# Transform the test set \n",
    "tfidf1_test = tfidf_vectorizer.transform([x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "x = []\n",
    "text_new = []\n",
    "doc = nlp(x_test)\n",
    "for token in doc:\n",
    "    text_new.append(token.pos_)\n",
    "txt = ' '.join(text_new)\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-idf Bigrams\n",
    "#Initialize the `tfidf_vectorizer` \n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range = (2,2)) \n",
    "\n",
    "# Fit and transform the training data \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(x_train_text_pos.astype('str')) \n",
    "\n",
    "# Transform the test set \n",
    "tfidf_test = tfidf_vectorizer.transform([x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "a = lexicon.analyze(\"\")\n",
    "for key, value in a.items():\n",
    "    categories.append(key)\n",
    "categories\n",
    "lexicon = Empath()\n",
    "semantic = []\n",
    "cnt = 0\n",
    "d = lexicon.analyze(x_test)\n",
    "d\n",
    "sem = []\n",
    "for key,value in d.items() :\n",
    "    sem.append(value)\n",
    "a = []\n",
    "for j in range(len(sem)):\n",
    "    for k in range(int(sem[j])):\n",
    "        a.append(categories[j])\n",
    "    b = \" \".join(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the `tfidf_vectorizer` \n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range = (1,1)) \n",
    "\n",
    "# Fit and transform the training data \n",
    "tfidf2_train = tfidf_vectorizer.fit_transform(x_train_semantics.astype('str')) \n",
    "\n",
    "# Transform the test set \n",
    "tfidf2_test = tfidf_vectorizer.transform([b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "# ui = sp.vstack(tfidf_train, tfidf1_train)\n",
    "# yu = tfidf_train.data.tolist()\n",
    "# yu.append(tfidf1_train.tolist())\n",
    "# test = tfidf_test.data.tolist() + x_test.tolist()\n",
    "#print(type(tfidf_train), tfidf_train.shape)\n",
    "#print(type(tfidf1_train), tfidf1_train.shape)\n",
    "# print(type(x_train), x_train.shape)\n",
    "diff_n_rows = tfidf_train.shape[0] - tfidf1_train.shape[0]\n",
    "\n",
    "Xb_new = sp.vstack((tfidf1_train, sp.csr_matrix((diff_n_rows, tfidf1_train.shape[1])))) \n",
    "#where diff_n_rows is the difference of the number of rows between Xa and Xb\n",
    "\n",
    "c = sp.hstack((tfidf_train, Xb_new))\n",
    "\n",
    "diff_n_rows = c.shape[0] - tfidf2_train.shape[0]\n",
    "\n",
    "Xb_new = sp.vstack((tfidf2_train, sp.csr_matrix((diff_n_rows, tfidf2_train.shape[1])))) \n",
    "#where diff_n_rows is the difference of the number of rows between Xa and Xb\n",
    "\n",
    "X = sp.hstack((c, Xb_new))\n",
    "X\n",
    "\n",
    "dif_n_rows = tfidf_test.shape[0] - tfidf1_test.shape[0]\n",
    "\n",
    "Xb_ne = sp.vstack((tfidf1_test, sp.csr_matrix((dif_n_rows, tfidf1_test.shape[1])))) \n",
    "#where diff_n_rows is the difference of the number of rows between Xa and Xb\n",
    "\n",
    "d = sp.hstack((tfidf_test, Xb_ne))\n",
    "\n",
    "dif_n_rows = d.shape[0] - tfidf2_test.shape[0]\n",
    "\n",
    "Xb_ne = sp.vstack((tfidf2_test, sp.csr_matrix((dif_n_rows, tfidf2_test.shape[1])))) \n",
    "#where diff_n_rows is the difference of the number of rows between Xa and Xb\n",
    "\n",
    "Y = sp.hstack((d, Xb_ne))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "#type(x_train.tolist())\n",
    "clf.fit(X, y_train)\n",
    "clf.predict(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
