{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "The parameters of the following will be tuned:\n",
    "+ XGBoost\n",
    "+ LightGBM\n",
    "+ Random Forest\n",
    "+ RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlottefettes/opt/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import gc\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data_6.pickle.gzde', compression='gzip')\n",
    "\n",
    "#train-validation split\n",
    "X_train = df[df.date_block_num < 33].drop(['item_cnt_month','date_block_num'], axis=1)\n",
    "Y_train = df[df.date_block_num < 33]['item_cnt_month']\n",
    "X_valid = df[df.date_block_num == 33].drop(['item_cnt_month','date_block_num'], axis=1)\n",
    "Y_valid = df[df.date_block_num == 33]['item_cnt_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gridsearchcv function\n",
    "my_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "def algorithm_pipeline(X_train_data, X_test_data, y_train_data, y_test_data, \n",
    "                       model, param_grid, cv=10, scoring=my_scorer,\n",
    "                       do_probabilities = False):\n",
    "    gs = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid, \n",
    "        cv=cv, \n",
    "        n_jobs=-1, \n",
    "        scoring=scoring,\n",
    "        #scoring=mean_squared_error,\n",
    "        verbose=2\n",
    "    )\n",
    "    fitted_model = gs.fit(X_train_data, y_train_data)\n",
    "    \n",
    "    pred = fitted_model.predict(X_test_data)\n",
    "    \n",
    "    return fitted_model, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  27 | elapsed: 64.0min remaining: 108.8min\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  27 | elapsed: 105.9min remaining: 13.2min\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed: 108.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:53:38] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[08:53:38] WARNING: src/learner.cc:686: Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "0.8672279120092348\n",
      "{'colsample_bytree': 0.6, 'seed': 42, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBRegressor()\n",
    "param_grid = {\n",
    "    #'max_depth': [6, 7, 8, 9, 10],\n",
    "    #'min_child_weight': [0, 5, 15, 300],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    #'eta': [.3, .2, .1, .05, .01, .005],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'seed': [42],\n",
    "}\n",
    "\n",
    "model, pred = algorithm_pipeline(X_train, X_valid, Y_train, Y_valid, model, param_grid, cv=3, scoring=my_scorer)\n",
    "\n",
    "# Root Mean Squared Error\n",
    "print(np.sqrt(-model.best_score_))\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed: 83.3min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 400.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 400.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:16:10] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:16:10] WARNING: src/learner.cc:686: Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "0.8197703531721671\n",
      "{'colsample_bytree': 0.6, 'max_depth': 10, 'min_child_weight': 300, 'seed': 42, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBRegressor()\n",
    "param_grid = {\n",
    "    'max_depth': [6, 7, 8, 9, 10],\n",
    "    'min_child_weight': [0, 5, 15, 300],\n",
    "    'colsample_bytree': [0.6],\n",
    "    #'eta': [.3, .2, .1, .05, .01, .005],\n",
    "    'subsample': [1.0],\n",
    "    'seed': [42],\n",
    "}\n",
    "\n",
    "model, pred = algorithm_pipeline(X_train, X_valid, Y_train, Y_valid, model, param_grid, cv=3, scoring=my_scorer)\n",
    "\n",
    "# Root Mean Squared Error\n",
    "print(np.sqrt(-model.best_score_))\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 7 candidates, totalling 21 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  21 | elapsed: 132.0min remaining: 99.0min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  21 | elapsed: 189.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:22:35] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:22:35] WARNING: src/learner.cc:686: Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "0.8197703531721671\n",
      "{'colsample_bytree': 0.6, 'eta': 0.3, 'max_depth': 10, 'min_child_weight': 300, 'seed': 42, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBRegressor()\n",
    "param_grid = {\n",
    "    'max_depth': [10],\n",
    "    'min_child_weight': [300],\n",
    "    'colsample_bytree': [0.6],\n",
    "    'eta': [.3, .2, .1, .05, .01, .005, 0.001],\n",
    "    'subsample': [1.0],\n",
    "    'seed': [42],\n",
    "}\n",
    "\n",
    "model, pred = algorithm_pipeline(X_train, X_valid, Y_train, Y_valid, model, param_grid, cv=3, scoring=my_scorer)\n",
    "\n",
    "xgb_params = model.best_params_\n",
    "\n",
    "# Root Mean Squared Error\n",
    "print(np.sqrt(-model.best_score_))\n",
    "print(xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:36:19] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[06:36:19] WARNING: src/learner.cc:686: Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "[0]\tvalidation_0-rmse:1.13828\tvalidation_1-rmse:1.05933\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-rmse:1.08921\tvalidation_1-rmse:1.01872\n",
      "[2]\tvalidation_0-rmse:1.05276\tvalidation_1-rmse:0.983901\n",
      "[3]\tvalidation_0-rmse:1.00803\tvalidation_1-rmse:0.955343\n",
      "[4]\tvalidation_0-rmse:0.974959\tvalidation_1-rmse:0.928747\n",
      "[5]\tvalidation_0-rmse:0.952064\tvalidation_1-rmse:0.909383\n",
      "[6]\tvalidation_0-rmse:0.927388\tvalidation_1-rmse:0.890421\n",
      "[7]\tvalidation_0-rmse:0.904863\tvalidation_1-rmse:0.874841\n",
      "[8]\tvalidation_0-rmse:0.888418\tvalidation_1-rmse:0.862744\n",
      "[9]\tvalidation_0-rmse:0.874235\tvalidation_1-rmse:0.851062\n",
      "[10]\tvalidation_0-rmse:0.861365\tvalidation_1-rmse:0.841532\n",
      "[11]\tvalidation_0-rmse:0.851707\tvalidation_1-rmse:0.835277\n",
      "[12]\tvalidation_0-rmse:0.841647\tvalidation_1-rmse:0.829038\n",
      "[13]\tvalidation_0-rmse:0.83429\tvalidation_1-rmse:0.824873\n",
      "[14]\tvalidation_0-rmse:0.829082\tvalidation_1-rmse:0.821635\n",
      "[15]\tvalidation_0-rmse:0.822915\tvalidation_1-rmse:0.817983\n",
      "[16]\tvalidation_0-rmse:0.81859\tvalidation_1-rmse:0.814261\n",
      "[17]\tvalidation_0-rmse:0.813831\tvalidation_1-rmse:0.810436\n",
      "[18]\tvalidation_0-rmse:0.809799\tvalidation_1-rmse:0.80863\n",
      "[19]\tvalidation_0-rmse:0.805937\tvalidation_1-rmse:0.806326\n",
      "[20]\tvalidation_0-rmse:0.802441\tvalidation_1-rmse:0.803555\n",
      "[21]\tvalidation_0-rmse:0.799964\tvalidation_1-rmse:0.801377\n",
      "[22]\tvalidation_0-rmse:0.797318\tvalidation_1-rmse:0.799937\n",
      "[23]\tvalidation_0-rmse:0.795152\tvalidation_1-rmse:0.798485\n",
      "[24]\tvalidation_0-rmse:0.793651\tvalidation_1-rmse:0.797129\n",
      "[25]\tvalidation_0-rmse:0.790841\tvalidation_1-rmse:0.794201\n",
      "[26]\tvalidation_0-rmse:0.788734\tvalidation_1-rmse:0.794248\n",
      "[27]\tvalidation_0-rmse:0.787581\tvalidation_1-rmse:0.79361\n",
      "[28]\tvalidation_0-rmse:0.786377\tvalidation_1-rmse:0.792673\n",
      "[29]\tvalidation_0-rmse:0.785272\tvalidation_1-rmse:0.792109\n",
      "[30]\tvalidation_0-rmse:0.783794\tvalidation_1-rmse:0.791443\n",
      "[31]\tvalidation_0-rmse:0.781823\tvalidation_1-rmse:0.790576\n",
      "[32]\tvalidation_0-rmse:0.780608\tvalidation_1-rmse:0.789251\n",
      "[33]\tvalidation_0-rmse:0.779831\tvalidation_1-rmse:0.789103\n",
      "[34]\tvalidation_0-rmse:0.778705\tvalidation_1-rmse:0.787664\n",
      "[35]\tvalidation_0-rmse:0.777962\tvalidation_1-rmse:0.787445\n",
      "[36]\tvalidation_0-rmse:0.777368\tvalidation_1-rmse:0.787207\n",
      "[37]\tvalidation_0-rmse:0.776683\tvalidation_1-rmse:0.787139\n",
      "[38]\tvalidation_0-rmse:0.775892\tvalidation_1-rmse:0.786683\n",
      "[39]\tvalidation_0-rmse:0.77515\tvalidation_1-rmse:0.786548\n",
      "[40]\tvalidation_0-rmse:0.774425\tvalidation_1-rmse:0.786052\n",
      "[41]\tvalidation_0-rmse:0.773773\tvalidation_1-rmse:0.785659\n",
      "[42]\tvalidation_0-rmse:0.773111\tvalidation_1-rmse:0.785488\n",
      "[43]\tvalidation_0-rmse:0.772544\tvalidation_1-rmse:0.784588\n",
      "[44]\tvalidation_0-rmse:0.772021\tvalidation_1-rmse:0.784316\n",
      "[45]\tvalidation_0-rmse:0.771446\tvalidation_1-rmse:0.784048\n",
      "[46]\tvalidation_0-rmse:0.770965\tvalidation_1-rmse:0.783665\n",
      "[47]\tvalidation_0-rmse:0.770532\tvalidation_1-rmse:0.783561\n",
      "[48]\tvalidation_0-rmse:0.77014\tvalidation_1-rmse:0.78347\n",
      "[49]\tvalidation_0-rmse:0.769612\tvalidation_1-rmse:0.783163\n",
      "[50]\tvalidation_0-rmse:0.769248\tvalidation_1-rmse:0.78301\n",
      "[51]\tvalidation_0-rmse:0.768605\tvalidation_1-rmse:0.782837\n",
      "[52]\tvalidation_0-rmse:0.767455\tvalidation_1-rmse:0.782426\n",
      "[53]\tvalidation_0-rmse:0.766891\tvalidation_1-rmse:0.782199\n",
      "[54]\tvalidation_0-rmse:0.766404\tvalidation_1-rmse:0.782272\n",
      "[55]\tvalidation_0-rmse:0.765971\tvalidation_1-rmse:0.78197\n",
      "[56]\tvalidation_0-rmse:0.765591\tvalidation_1-rmse:0.781733\n",
      "[57]\tvalidation_0-rmse:0.764697\tvalidation_1-rmse:0.781776\n",
      "[58]\tvalidation_0-rmse:0.764055\tvalidation_1-rmse:0.781835\n",
      "[59]\tvalidation_0-rmse:0.763725\tvalidation_1-rmse:0.781704\n",
      "[60]\tvalidation_0-rmse:0.763456\tvalidation_1-rmse:0.78158\n",
      "[61]\tvalidation_0-rmse:0.762991\tvalidation_1-rmse:0.781329\n",
      "[62]\tvalidation_0-rmse:0.762022\tvalidation_1-rmse:0.782038\n",
      "[63]\tvalidation_0-rmse:0.761015\tvalidation_1-rmse:0.781777\n",
      "[64]\tvalidation_0-rmse:0.760397\tvalidation_1-rmse:0.78123\n",
      "[65]\tvalidation_0-rmse:0.759284\tvalidation_1-rmse:0.782079\n",
      "[66]\tvalidation_0-rmse:0.75894\tvalidation_1-rmse:0.781994\n",
      "[67]\tvalidation_0-rmse:0.758589\tvalidation_1-rmse:0.781989\n",
      "[68]\tvalidation_0-rmse:0.758228\tvalidation_1-rmse:0.78174\n",
      "[69]\tvalidation_0-rmse:0.757891\tvalidation_1-rmse:0.781693\n",
      "[70]\tvalidation_0-rmse:0.75754\tvalidation_1-rmse:0.781494\n",
      "[71]\tvalidation_0-rmse:0.757278\tvalidation_1-rmse:0.781451\n",
      "[72]\tvalidation_0-rmse:0.756965\tvalidation_1-rmse:0.781003\n",
      "[73]\tvalidation_0-rmse:0.756597\tvalidation_1-rmse:0.78123\n",
      "[74]\tvalidation_0-rmse:0.756338\tvalidation_1-rmse:0.781217\n",
      "[75]\tvalidation_0-rmse:0.75599\tvalidation_1-rmse:0.781089\n",
      "[76]\tvalidation_0-rmse:0.755679\tvalidation_1-rmse:0.781135\n",
      "[77]\tvalidation_0-rmse:0.755426\tvalidation_1-rmse:0.781034\n",
      "[78]\tvalidation_0-rmse:0.755069\tvalidation_1-rmse:0.780857\n",
      "[79]\tvalidation_0-rmse:0.754769\tvalidation_1-rmse:0.780756\n",
      "[80]\tvalidation_0-rmse:0.754508\tvalidation_1-rmse:0.780688\n",
      "[81]\tvalidation_0-rmse:0.754222\tvalidation_1-rmse:0.780652\n",
      "[82]\tvalidation_0-rmse:0.754\tvalidation_1-rmse:0.780614\n",
      "[83]\tvalidation_0-rmse:0.753733\tvalidation_1-rmse:0.780472\n",
      "[84]\tvalidation_0-rmse:0.753091\tvalidation_1-rmse:0.780075\n",
      "[85]\tvalidation_0-rmse:0.752193\tvalidation_1-rmse:0.779321\n",
      "[86]\tvalidation_0-rmse:0.751771\tvalidation_1-rmse:0.779112\n",
      "[87]\tvalidation_0-rmse:0.751453\tvalidation_1-rmse:0.779095\n",
      "[88]\tvalidation_0-rmse:0.749842\tvalidation_1-rmse:0.779397\n",
      "[89]\tvalidation_0-rmse:0.749571\tvalidation_1-rmse:0.779261\n",
      "[90]\tvalidation_0-rmse:0.749253\tvalidation_1-rmse:0.779422\n",
      "[91]\tvalidation_0-rmse:0.749021\tvalidation_1-rmse:0.779348\n",
      "[92]\tvalidation_0-rmse:0.748826\tvalidation_1-rmse:0.779267\n",
      "[93]\tvalidation_0-rmse:0.748645\tvalidation_1-rmse:0.779131\n",
      "[94]\tvalidation_0-rmse:0.748452\tvalidation_1-rmse:0.77902\n",
      "[95]\tvalidation_0-rmse:0.748164\tvalidation_1-rmse:0.778934\n",
      "[96]\tvalidation_0-rmse:0.747988\tvalidation_1-rmse:0.778904\n",
      "[97]\tvalidation_0-rmse:0.747628\tvalidation_1-rmse:0.778814\n",
      "[98]\tvalidation_0-rmse:0.747419\tvalidation_1-rmse:0.77883\n",
      "[99]\tvalidation_0-rmse:0.747064\tvalidation_1-rmse:0.779051\n",
      "[100]\tvalidation_0-rmse:0.746688\tvalidation_1-rmse:0.778568\n",
      "[101]\tvalidation_0-rmse:0.746525\tvalidation_1-rmse:0.778498\n",
      "[102]\tvalidation_0-rmse:0.746386\tvalidation_1-rmse:0.778464\n",
      "[103]\tvalidation_0-rmse:0.746016\tvalidation_1-rmse:0.778256\n",
      "[104]\tvalidation_0-rmse:0.745719\tvalidation_1-rmse:0.777967\n",
      "[105]\tvalidation_0-rmse:0.745475\tvalidation_1-rmse:0.777904\n",
      "[106]\tvalidation_0-rmse:0.745183\tvalidation_1-rmse:0.777853\n",
      "[107]\tvalidation_0-rmse:0.744965\tvalidation_1-rmse:0.777963\n",
      "[108]\tvalidation_0-rmse:0.744658\tvalidation_1-rmse:0.777885\n",
      "[109]\tvalidation_0-rmse:0.744193\tvalidation_1-rmse:0.777669\n",
      "[110]\tvalidation_0-rmse:0.743847\tvalidation_1-rmse:0.777718\n",
      "[111]\tvalidation_0-rmse:0.743699\tvalidation_1-rmse:0.777614\n",
      "[112]\tvalidation_0-rmse:0.743514\tvalidation_1-rmse:0.777621\n",
      "[113]\tvalidation_0-rmse:0.743283\tvalidation_1-rmse:0.777529\n",
      "[114]\tvalidation_0-rmse:0.743045\tvalidation_1-rmse:0.777479\n",
      "[115]\tvalidation_0-rmse:0.742829\tvalidation_1-rmse:0.777457\n",
      "[116]\tvalidation_0-rmse:0.742549\tvalidation_1-rmse:0.777475\n",
      "[117]\tvalidation_0-rmse:0.742367\tvalidation_1-rmse:0.777527\n",
      "[118]\tvalidation_0-rmse:0.742053\tvalidation_1-rmse:0.777487\n",
      "[119]\tvalidation_0-rmse:0.741869\tvalidation_1-rmse:0.777407\n",
      "[120]\tvalidation_0-rmse:0.741638\tvalidation_1-rmse:0.777334\n",
      "[121]\tvalidation_0-rmse:0.74121\tvalidation_1-rmse:0.776941\n",
      "[122]\tvalidation_0-rmse:0.74027\tvalidation_1-rmse:0.777994\n",
      "[123]\tvalidation_0-rmse:0.740043\tvalidation_1-rmse:0.777982\n",
      "[124]\tvalidation_0-rmse:0.739893\tvalidation_1-rmse:0.778073\n",
      "[125]\tvalidation_0-rmse:0.739655\tvalidation_1-rmse:0.77809\n",
      "[126]\tvalidation_0-rmse:0.739439\tvalidation_1-rmse:0.777931\n",
      "[127]\tvalidation_0-rmse:0.739252\tvalidation_1-rmse:0.777917\n",
      "[128]\tvalidation_0-rmse:0.739026\tvalidation_1-rmse:0.777926\n",
      "[129]\tvalidation_0-rmse:0.738807\tvalidation_1-rmse:0.777593\n",
      "[130]\tvalidation_0-rmse:0.738382\tvalidation_1-rmse:0.777612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[131]\tvalidation_0-rmse:0.738091\tvalidation_1-rmse:0.777914\n",
      "Stopping. Best iteration:\n",
      "[121]\tvalidation_0-rmse:0.74121\tvalidation_1-rmse:0.776941\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.6, eta=0.3, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=10, min_child_weight=300, missing=None,\n",
       "             n_estimators=1000, n_jobs=1, nthread=None, objective='reg:linear',\n",
       "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "             seed=42, silent=None, subsample=1.0, verbosity=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = xgb.XGBRegressor(**xgb_params, n_estimators=1000)\n",
    "xgb_model.fit(\n",
    "    X_train, \n",
    "    Y_train,\n",
    "    eval_metric=\"rmse\", \n",
    "    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
    "    verbose=True, \n",
    "    early_stopping_rounds = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.6,\n",
       " 'eta': 0.3,\n",
       " 'max_depth': 10,\n",
       " 'min_child_weight': 300,\n",
       " 'seed': 42,\n",
       " 'subsample': 1.0,\n",
       " 'n_estimators': 125}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_params['n_estimators'] = 125\n",
    "xgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save untrained model to file \n",
    "\n",
    "Pkl_Filename = \"XBG_Params.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(xgb_params, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done  42 out of  48 | elapsed: 10.9min remaining:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed: 11.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8239681837266936\n",
      "{'bagging_fraction': 0.2, 'feature_fraction': 0.8, 'seed': 42}\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMRegressor()\n",
    "param_grid = {\n",
    "    #'max_depth': [-1, 5, 6, 7, 8],\n",
    "    #'num_leaves': [30, 80, 100, 128, 200],\n",
    "    'bagging_fraction': [0.2, 0.5, 0.8, 1.0],\n",
    "    'feature_fraction': [0.2, 0.5, 0.8, 1.0],\n",
    "    #'min_data_in_leaf': [0, 5, 15, 300],\n",
    "    #'learning_rate': [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08]\n",
    "    'seed': [42],\n",
    "}\n",
    "\n",
    "model, pred = algorithm_pipeline(X_train, X_valid, Y_train, Y_valid, model, \n",
    "                                 param_grid, cv=3)\n",
    "\n",
    "print(np.sqrt(-model.best_score_))\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed: 36.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8171918670759332\n",
      "{'bagging_fraction': 0.2, 'feature_fraction': 0.8, 'max_depth': -1, 'num_leaves': 128, 'seed': 42}\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMRegressor()\n",
    "param_grid = {\n",
    "    'max_depth': [-1, 6, 7, 8, 9, 10],\n",
    "    'num_leaves': [31, 80, 100, 128, 200, 300],\n",
    "    'bagging_fraction': [0.2],\n",
    "    'feature_fraction': [0.8],\n",
    "    #'min_data_in_leaf': [0, 5, 15, 300],\n",
    "    #'learning_rate': [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08]\n",
    "    'seed': [42],\n",
    "}\n",
    "\n",
    "model, pred = algorithm_pipeline(X_train, X_valid, Y_train, Y_valid, model, \n",
    "                                 param_grid, cv=3)\n",
    "\n",
    "print(np.sqrt(-model.best_score_))\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed:  6.6min remaining:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  6.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8145338077464104\n",
      "{'bagging_fraction': 0.2, 'feature_fraction': 0.8, 'max_depth': -1, 'min_data_in_leaf': 300, 'num_leaves': 128, 'seed': 42}\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMRegressor()\n",
    "param_grid = {\n",
    "    'max_depth': [-1],\n",
    "    'num_leaves': [128],\n",
    "    'bagging_fraction': [0.2],\n",
    "    'feature_fraction': [0.8],\n",
    "    'min_data_in_leaf': [0, 5, 15, 20, 300],\n",
    "    #'learning_rate': [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08]\n",
    "    'seed': [42],\n",
    "}\n",
    "\n",
    "model, pred = algorithm_pipeline(X_train, X_valid, Y_train, Y_valid, model, \n",
    "                                 param_grid, cv=3)\n",
    "\n",
    "print(np.sqrt(-model.best_score_))\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  18 | elapsed:  8.4min remaining: 13.2min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed: 10.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8136538257965431\n",
      "{'bagging_fraction': 0.2, 'feature_fraction': 0.8, 'learning_rate': 0.05, 'max_depth': -1, 'min_data_in_leaf': 300, 'num_leaves': 128, 'seed': 42}\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMRegressor()\n",
    "param_grid = {\n",
    "    'max_depth': [-1],\n",
    "    'num_leaves': [128],\n",
    "    'bagging_fraction': [0.2],\n",
    "    'feature_fraction': [0.8],\n",
    "    'min_data_in_leaf': [300],\n",
    "    'learning_rate': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5],\n",
    "    'seed': [42],\n",
    "}\n",
    "\n",
    "model, pred = algorithm_pipeline(X_train, X_valid, Y_train, Y_valid, model, \n",
    "                                 param_grid, cv=3)\n",
    "\n",
    "print(np.sqrt(-model.best_score_))\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's rmse: 1.17279\ttraining's l2: 1.37542\tvalid_1's rmse: 1.05825\tvalid_1's l2: 1.1199\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\ttraining's rmse: 1.14385\ttraining's l2: 1.3084\tvalid_1's rmse: 1.03669\tvalid_1's l2: 1.07472\n",
      "[3]\ttraining's rmse: 1.11635\ttraining's l2: 1.24624\tvalid_1's rmse: 1.01711\tvalid_1's l2: 1.0345\n",
      "[4]\ttraining's rmse: 1.09265\ttraining's l2: 1.19388\tvalid_1's rmse: 0.999659\tvalid_1's l2: 0.999317\n",
      "[5]\ttraining's rmse: 1.06912\ttraining's l2: 1.14302\tvalid_1's rmse: 0.984868\tvalid_1's l2: 0.969965\n",
      "[6]\ttraining's rmse: 1.04686\ttraining's l2: 1.09592\tvalid_1's rmse: 0.970344\tvalid_1's l2: 0.941568\n",
      "[7]\ttraining's rmse: 1.0262\ttraining's l2: 1.05308\tvalid_1's rmse: 0.956946\tvalid_1's l2: 0.915745\n",
      "[8]\ttraining's rmse: 1.00717\ttraining's l2: 1.01439\tvalid_1's rmse: 0.945431\tvalid_1's l2: 0.893839\n",
      "[9]\ttraining's rmse: 0.989685\ttraining's l2: 0.979476\tvalid_1's rmse: 0.932951\tvalid_1's l2: 0.870398\n",
      "[10]\ttraining's rmse: 0.973499\ttraining's l2: 0.9477\tvalid_1's rmse: 0.923587\tvalid_1's l2: 0.853013\n",
      "[11]\ttraining's rmse: 0.95837\ttraining's l2: 0.918472\tvalid_1's rmse: 0.913958\tvalid_1's l2: 0.835318\n",
      "[12]\ttraining's rmse: 0.944076\ttraining's l2: 0.891279\tvalid_1's rmse: 0.904508\tvalid_1's l2: 0.818135\n",
      "[13]\ttraining's rmse: 0.9312\ttraining's l2: 0.867133\tvalid_1's rmse: 0.896919\tvalid_1's l2: 0.804464\n",
      "[14]\ttraining's rmse: 0.919291\ttraining's l2: 0.845096\tvalid_1's rmse: 0.888747\tvalid_1's l2: 0.789872\n",
      "[15]\ttraining's rmse: 0.908321\ttraining's l2: 0.825047\tvalid_1's rmse: 0.881168\tvalid_1's l2: 0.776458\n",
      "[16]\ttraining's rmse: 0.898032\ttraining's l2: 0.806461\tvalid_1's rmse: 0.874526\tvalid_1's l2: 0.764796\n",
      "[17]\ttraining's rmse: 0.88861\ttraining's l2: 0.789628\tvalid_1's rmse: 0.868117\tvalid_1's l2: 0.753627\n",
      "[18]\ttraining's rmse: 0.879842\ttraining's l2: 0.774121\tvalid_1's rmse: 0.862975\tvalid_1's l2: 0.744726\n",
      "[19]\ttraining's rmse: 0.87224\ttraining's l2: 0.760803\tvalid_1's rmse: 0.858115\tvalid_1's l2: 0.736362\n",
      "[20]\ttraining's rmse: 0.86466\ttraining's l2: 0.747636\tvalid_1's rmse: 0.853124\tvalid_1's l2: 0.727821\n",
      "[21]\ttraining's rmse: 0.858258\ttraining's l2: 0.736608\tvalid_1's rmse: 0.849497\tvalid_1's l2: 0.721646\n",
      "[22]\ttraining's rmse: 0.851566\ttraining's l2: 0.725164\tvalid_1's rmse: 0.84618\tvalid_1's l2: 0.716021\n",
      "[23]\ttraining's rmse: 0.845729\ttraining's l2: 0.715258\tvalid_1's rmse: 0.842398\tvalid_1's l2: 0.709635\n",
      "[24]\ttraining's rmse: 0.840011\ttraining's l2: 0.705618\tvalid_1's rmse: 0.838604\tvalid_1's l2: 0.703257\n",
      "[25]\ttraining's rmse: 0.834328\ttraining's l2: 0.696103\tvalid_1's rmse: 0.835245\tvalid_1's l2: 0.697634\n",
      "[26]\ttraining's rmse: 0.829321\ttraining's l2: 0.687774\tvalid_1's rmse: 0.832944\tvalid_1's l2: 0.693796\n",
      "[27]\ttraining's rmse: 0.824998\ttraining's l2: 0.680622\tvalid_1's rmse: 0.830662\tvalid_1's l2: 0.689999\n",
      "[28]\ttraining's rmse: 0.820823\ttraining's l2: 0.67375\tvalid_1's rmse: 0.82784\tvalid_1's l2: 0.68532\n",
      "[29]\ttraining's rmse: 0.816702\ttraining's l2: 0.667003\tvalid_1's rmse: 0.825908\tvalid_1's l2: 0.682124\n",
      "[30]\ttraining's rmse: 0.813121\ttraining's l2: 0.661166\tvalid_1's rmse: 0.82379\tvalid_1's l2: 0.678629\n",
      "[31]\ttraining's rmse: 0.810245\ttraining's l2: 0.656498\tvalid_1's rmse: 0.821501\tvalid_1's l2: 0.674864\n",
      "[32]\ttraining's rmse: 0.80665\ttraining's l2: 0.650685\tvalid_1's rmse: 0.819222\tvalid_1's l2: 0.671125\n",
      "[33]\ttraining's rmse: 0.80385\ttraining's l2: 0.646175\tvalid_1's rmse: 0.817337\tvalid_1's l2: 0.66804\n",
      "[34]\ttraining's rmse: 0.800908\ttraining's l2: 0.641454\tvalid_1's rmse: 0.815745\tvalid_1's l2: 0.665439\n",
      "[35]\ttraining's rmse: 0.798138\ttraining's l2: 0.637025\tvalid_1's rmse: 0.814438\tvalid_1's l2: 0.663309\n",
      "[36]\ttraining's rmse: 0.795602\ttraining's l2: 0.632983\tvalid_1's rmse: 0.813195\tvalid_1's l2: 0.661286\n",
      "[37]\ttraining's rmse: 0.793151\ttraining's l2: 0.629089\tvalid_1's rmse: 0.811889\tvalid_1's l2: 0.659163\n",
      "[38]\ttraining's rmse: 0.79119\ttraining's l2: 0.625982\tvalid_1's rmse: 0.811357\tvalid_1's l2: 0.658301\n",
      "[39]\ttraining's rmse: 0.788922\ttraining's l2: 0.622398\tvalid_1's rmse: 0.810145\tvalid_1's l2: 0.656335\n",
      "[40]\ttraining's rmse: 0.78691\ttraining's l2: 0.619227\tvalid_1's rmse: 0.809849\tvalid_1's l2: 0.655856\n",
      "[41]\ttraining's rmse: 0.785045\ttraining's l2: 0.616295\tvalid_1's rmse: 0.80852\tvalid_1's l2: 0.653705\n",
      "[42]\ttraining's rmse: 0.783383\ttraining's l2: 0.613689\tvalid_1's rmse: 0.80729\tvalid_1's l2: 0.651718\n",
      "[43]\ttraining's rmse: 0.781927\ttraining's l2: 0.611409\tvalid_1's rmse: 0.806691\tvalid_1's l2: 0.65075\n",
      "[44]\ttraining's rmse: 0.780563\ttraining's l2: 0.609279\tvalid_1's rmse: 0.805892\tvalid_1's l2: 0.649462\n",
      "[45]\ttraining's rmse: 0.779065\ttraining's l2: 0.606942\tvalid_1's rmse: 0.805087\tvalid_1's l2: 0.648166\n",
      "[46]\ttraining's rmse: 0.777498\ttraining's l2: 0.604502\tvalid_1's rmse: 0.804112\tvalid_1's l2: 0.646596\n",
      "[47]\ttraining's rmse: 0.776047\ttraining's l2: 0.602249\tvalid_1's rmse: 0.804474\tvalid_1's l2: 0.647178\n",
      "[48]\ttraining's rmse: 0.774747\ttraining's l2: 0.600233\tvalid_1's rmse: 0.803849\tvalid_1's l2: 0.646173\n",
      "[49]\ttraining's rmse: 0.773767\ttraining's l2: 0.598716\tvalid_1's rmse: 0.803309\tvalid_1's l2: 0.645305\n",
      "[50]\ttraining's rmse: 0.77254\ttraining's l2: 0.596818\tvalid_1's rmse: 0.802328\tvalid_1's l2: 0.64373\n",
      "[51]\ttraining's rmse: 0.771447\ttraining's l2: 0.59513\tvalid_1's rmse: 0.801555\tvalid_1's l2: 0.64249\n",
      "[52]\ttraining's rmse: 0.770393\ttraining's l2: 0.593506\tvalid_1's rmse: 0.800773\tvalid_1's l2: 0.641238\n",
      "[53]\ttraining's rmse: 0.769088\ttraining's l2: 0.591497\tvalid_1's rmse: 0.800849\tvalid_1's l2: 0.641359\n",
      "[54]\ttraining's rmse: 0.768075\ttraining's l2: 0.58994\tvalid_1's rmse: 0.800296\tvalid_1's l2: 0.640474\n",
      "[55]\ttraining's rmse: 0.767206\ttraining's l2: 0.588604\tvalid_1's rmse: 0.800121\tvalid_1's l2: 0.640193\n",
      "[56]\ttraining's rmse: 0.76634\ttraining's l2: 0.587277\tvalid_1's rmse: 0.799613\tvalid_1's l2: 0.639381\n",
      "[57]\ttraining's rmse: 0.765457\ttraining's l2: 0.585924\tvalid_1's rmse: 0.798839\tvalid_1's l2: 0.638144\n",
      "[58]\ttraining's rmse: 0.764677\ttraining's l2: 0.584732\tvalid_1's rmse: 0.798258\tvalid_1's l2: 0.637216\n",
      "[59]\ttraining's rmse: 0.763798\ttraining's l2: 0.583388\tvalid_1's rmse: 0.797365\tvalid_1's l2: 0.635791\n",
      "[60]\ttraining's rmse: 0.76301\ttraining's l2: 0.582185\tvalid_1's rmse: 0.797008\tvalid_1's l2: 0.635222\n",
      "[61]\ttraining's rmse: 0.762274\ttraining's l2: 0.581061\tvalid_1's rmse: 0.796944\tvalid_1's l2: 0.635119\n",
      "[62]\ttraining's rmse: 0.761536\ttraining's l2: 0.579937\tvalid_1's rmse: 0.796562\tvalid_1's l2: 0.63451\n",
      "[63]\ttraining's rmse: 0.760818\ttraining's l2: 0.578844\tvalid_1's rmse: 0.796047\tvalid_1's l2: 0.633691\n",
      "[64]\ttraining's rmse: 0.760029\ttraining's l2: 0.577643\tvalid_1's rmse: 0.795703\tvalid_1's l2: 0.633143\n",
      "[65]\ttraining's rmse: 0.759351\ttraining's l2: 0.576614\tvalid_1's rmse: 0.79518\tvalid_1's l2: 0.632311\n",
      "[66]\ttraining's rmse: 0.758658\ttraining's l2: 0.575562\tvalid_1's rmse: 0.79457\tvalid_1's l2: 0.631341\n",
      "[67]\ttraining's rmse: 0.75805\ttraining's l2: 0.57464\tvalid_1's rmse: 0.794191\tvalid_1's l2: 0.63074\n",
      "[68]\ttraining's rmse: 0.757333\ttraining's l2: 0.573553\tvalid_1's rmse: 0.793528\tvalid_1's l2: 0.629686\n",
      "[69]\ttraining's rmse: 0.756709\ttraining's l2: 0.572608\tvalid_1's rmse: 0.793194\tvalid_1's l2: 0.629157\n",
      "[70]\ttraining's rmse: 0.756128\ttraining's l2: 0.571729\tvalid_1's rmse: 0.792772\tvalid_1's l2: 0.628488\n",
      "[71]\ttraining's rmse: 0.755628\ttraining's l2: 0.570974\tvalid_1's rmse: 0.792358\tvalid_1's l2: 0.627832\n",
      "[72]\ttraining's rmse: 0.75505\ttraining's l2: 0.5701\tvalid_1's rmse: 0.791651\tvalid_1's l2: 0.626712\n",
      "[73]\ttraining's rmse: 0.754407\ttraining's l2: 0.569131\tvalid_1's rmse: 0.791696\tvalid_1's l2: 0.626783\n",
      "[74]\ttraining's rmse: 0.753897\ttraining's l2: 0.56836\tvalid_1's rmse: 0.791304\tvalid_1's l2: 0.626162\n",
      "[75]\ttraining's rmse: 0.753324\ttraining's l2: 0.567497\tvalid_1's rmse: 0.791006\tvalid_1's l2: 0.62569\n",
      "[76]\ttraining's rmse: 0.752826\ttraining's l2: 0.566746\tvalid_1's rmse: 0.790752\tvalid_1's l2: 0.625289\n",
      "[77]\ttraining's rmse: 0.752349\ttraining's l2: 0.566029\tvalid_1's rmse: 0.790462\tvalid_1's l2: 0.62483\n",
      "[78]\ttraining's rmse: 0.751869\ttraining's l2: 0.565308\tvalid_1's rmse: 0.790163\tvalid_1's l2: 0.624358\n",
      "[79]\ttraining's rmse: 0.751376\ttraining's l2: 0.564566\tvalid_1's rmse: 0.790153\tvalid_1's l2: 0.624342\n",
      "[80]\ttraining's rmse: 0.750919\ttraining's l2: 0.563879\tvalid_1's rmse: 0.789983\tvalid_1's l2: 0.624074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[81]\ttraining's rmse: 0.750444\ttraining's l2: 0.563167\tvalid_1's rmse: 0.789608\tvalid_1's l2: 0.623481\n",
      "[82]\ttraining's rmse: 0.750018\ttraining's l2: 0.562528\tvalid_1's rmse: 0.789483\tvalid_1's l2: 0.623284\n",
      "[83]\ttraining's rmse: 0.749532\ttraining's l2: 0.561798\tvalid_1's rmse: 0.788915\tvalid_1's l2: 0.622386\n",
      "[84]\ttraining's rmse: 0.748967\ttraining's l2: 0.560952\tvalid_1's rmse: 0.788504\tvalid_1's l2: 0.621738\n",
      "[85]\ttraining's rmse: 0.748538\ttraining's l2: 0.56031\tvalid_1's rmse: 0.788251\tvalid_1's l2: 0.621339\n",
      "[86]\ttraining's rmse: 0.748117\ttraining's l2: 0.559679\tvalid_1's rmse: 0.788086\tvalid_1's l2: 0.62108\n",
      "[87]\ttraining's rmse: 0.747634\ttraining's l2: 0.558957\tvalid_1's rmse: 0.787836\tvalid_1's l2: 0.620686\n",
      "[88]\ttraining's rmse: 0.747235\ttraining's l2: 0.55836\tvalid_1's rmse: 0.787213\tvalid_1's l2: 0.619705\n",
      "[89]\ttraining's rmse: 0.74683\ttraining's l2: 0.557755\tvalid_1's rmse: 0.787096\tvalid_1's l2: 0.61952\n",
      "[90]\ttraining's rmse: 0.74604\ttraining's l2: 0.556576\tvalid_1's rmse: 0.787816\tvalid_1's l2: 0.620654\n",
      "[91]\ttraining's rmse: 0.745581\ttraining's l2: 0.555891\tvalid_1's rmse: 0.78753\tvalid_1's l2: 0.620203\n",
      "[92]\ttraining's rmse: 0.745217\ttraining's l2: 0.555349\tvalid_1's rmse: 0.787299\tvalid_1's l2: 0.61984\n",
      "[93]\ttraining's rmse: 0.74484\ttraining's l2: 0.554787\tvalid_1's rmse: 0.787131\tvalid_1's l2: 0.619575\n",
      "[94]\ttraining's rmse: 0.744482\ttraining's l2: 0.554253\tvalid_1's rmse: 0.787055\tvalid_1's l2: 0.619455\n",
      "[95]\ttraining's rmse: 0.744111\ttraining's l2: 0.553701\tvalid_1's rmse: 0.787029\tvalid_1's l2: 0.619415\n",
      "[96]\ttraining's rmse: 0.74367\ttraining's l2: 0.553045\tvalid_1's rmse: 0.786731\tvalid_1's l2: 0.618946\n",
      "[97]\ttraining's rmse: 0.743298\ttraining's l2: 0.552492\tvalid_1's rmse: 0.786539\tvalid_1's l2: 0.618644\n",
      "[98]\ttraining's rmse: 0.742918\ttraining's l2: 0.551927\tvalid_1's rmse: 0.786222\tvalid_1's l2: 0.618146\n",
      "[99]\ttraining's rmse: 0.742592\ttraining's l2: 0.551443\tvalid_1's rmse: 0.786011\tvalid_1's l2: 0.617813\n",
      "[100]\ttraining's rmse: 0.742246\ttraining's l2: 0.550929\tvalid_1's rmse: 0.785796\tvalid_1's l2: 0.617475\n",
      "[101]\ttraining's rmse: 0.741044\ttraining's l2: 0.549147\tvalid_1's rmse: 0.786213\tvalid_1's l2: 0.61813\n",
      "[102]\ttraining's rmse: 0.740722\ttraining's l2: 0.548668\tvalid_1's rmse: 0.786095\tvalid_1's l2: 0.617946\n",
      "[103]\ttraining's rmse: 0.740371\ttraining's l2: 0.54815\tvalid_1's rmse: 0.78591\tvalid_1's l2: 0.617654\n",
      "[104]\ttraining's rmse: 0.739938\ttraining's l2: 0.547508\tvalid_1's rmse: 0.78587\tvalid_1's l2: 0.617592\n",
      "[105]\ttraining's rmse: 0.739059\ttraining's l2: 0.546209\tvalid_1's rmse: 0.785806\tvalid_1's l2: 0.61749\n",
      "[106]\ttraining's rmse: 0.738786\ttraining's l2: 0.545805\tvalid_1's rmse: 0.785736\tvalid_1's l2: 0.617381\n",
      "[107]\ttraining's rmse: 0.738475\ttraining's l2: 0.545345\tvalid_1's rmse: 0.78565\tvalid_1's l2: 0.617245\n",
      "[108]\ttraining's rmse: 0.738125\ttraining's l2: 0.544829\tvalid_1's rmse: 0.785596\tvalid_1's l2: 0.617161\n",
      "[109]\ttraining's rmse: 0.737769\ttraining's l2: 0.544303\tvalid_1's rmse: 0.785447\tvalid_1's l2: 0.616927\n",
      "[110]\ttraining's rmse: 0.73746\ttraining's l2: 0.543848\tvalid_1's rmse: 0.78538\tvalid_1's l2: 0.616822\n",
      "[111]\ttraining's rmse: 0.737054\ttraining's l2: 0.543248\tvalid_1's rmse: 0.785276\tvalid_1's l2: 0.616658\n",
      "[112]\ttraining's rmse: 0.736749\ttraining's l2: 0.542798\tvalid_1's rmse: 0.785206\tvalid_1's l2: 0.616548\n",
      "[113]\ttraining's rmse: 0.736428\ttraining's l2: 0.542326\tvalid_1's rmse: 0.785056\tvalid_1's l2: 0.616313\n",
      "[114]\ttraining's rmse: 0.736134\ttraining's l2: 0.541894\tvalid_1's rmse: 0.784966\tvalid_1's l2: 0.616172\n",
      "[115]\ttraining's rmse: 0.735682\ttraining's l2: 0.541228\tvalid_1's rmse: 0.785275\tvalid_1's l2: 0.616656\n",
      "[116]\ttraining's rmse: 0.735378\ttraining's l2: 0.540781\tvalid_1's rmse: 0.785003\tvalid_1's l2: 0.616229\n",
      "[117]\ttraining's rmse: 0.735061\ttraining's l2: 0.540314\tvalid_1's rmse: 0.784824\tvalid_1's l2: 0.615948\n",
      "[118]\ttraining's rmse: 0.734761\ttraining's l2: 0.539874\tvalid_1's rmse: 0.784724\tvalid_1's l2: 0.615792\n",
      "[119]\ttraining's rmse: 0.734474\ttraining's l2: 0.539451\tvalid_1's rmse: 0.784713\tvalid_1's l2: 0.615775\n",
      "[120]\ttraining's rmse: 0.734193\ttraining's l2: 0.539039\tvalid_1's rmse: 0.784566\tvalid_1's l2: 0.615544\n",
      "[121]\ttraining's rmse: 0.733941\ttraining's l2: 0.538669\tvalid_1's rmse: 0.784555\tvalid_1's l2: 0.615527\n",
      "[122]\ttraining's rmse: 0.733598\ttraining's l2: 0.538166\tvalid_1's rmse: 0.784287\tvalid_1's l2: 0.615107\n",
      "[123]\ttraining's rmse: 0.733359\ttraining's l2: 0.537815\tvalid_1's rmse: 0.784244\tvalid_1's l2: 0.615039\n",
      "[124]\ttraining's rmse: 0.733118\ttraining's l2: 0.537462\tvalid_1's rmse: 0.784183\tvalid_1's l2: 0.614942\n",
      "[125]\ttraining's rmse: 0.732853\ttraining's l2: 0.537074\tvalid_1's rmse: 0.784127\tvalid_1's l2: 0.614855\n",
      "[126]\ttraining's rmse: 0.732632\ttraining's l2: 0.53675\tvalid_1's rmse: 0.784037\tvalid_1's l2: 0.614714\n",
      "[127]\ttraining's rmse: 0.732193\ttraining's l2: 0.536107\tvalid_1's rmse: 0.784181\tvalid_1's l2: 0.61494\n",
      "[128]\ttraining's rmse: 0.731957\ttraining's l2: 0.535761\tvalid_1's rmse: 0.78414\tvalid_1's l2: 0.614875\n",
      "[129]\ttraining's rmse: 0.731715\ttraining's l2: 0.535407\tvalid_1's rmse: 0.784143\tvalid_1's l2: 0.614881\n",
      "[130]\ttraining's rmse: 0.731482\ttraining's l2: 0.535066\tvalid_1's rmse: 0.784049\tvalid_1's l2: 0.614734\n",
      "[131]\ttraining's rmse: 0.731184\ttraining's l2: 0.53463\tvalid_1's rmse: 0.783919\tvalid_1's l2: 0.614529\n",
      "[132]\ttraining's rmse: 0.730956\ttraining's l2: 0.534297\tvalid_1's rmse: 0.783862\tvalid_1's l2: 0.61444\n",
      "[133]\ttraining's rmse: 0.730739\ttraining's l2: 0.533979\tvalid_1's rmse: 0.783918\tvalid_1's l2: 0.614528\n",
      "[134]\ttraining's rmse: 0.730479\ttraining's l2: 0.5336\tvalid_1's rmse: 0.783843\tvalid_1's l2: 0.61441\n",
      "[135]\ttraining's rmse: 0.730269\ttraining's l2: 0.533293\tvalid_1's rmse: 0.783881\tvalid_1's l2: 0.61447\n",
      "[136]\ttraining's rmse: 0.729974\ttraining's l2: 0.532862\tvalid_1's rmse: 0.783771\tvalid_1's l2: 0.614297\n",
      "[137]\ttraining's rmse: 0.729712\ttraining's l2: 0.532479\tvalid_1's rmse: 0.783565\tvalid_1's l2: 0.613974\n",
      "[138]\ttraining's rmse: 0.729484\ttraining's l2: 0.532147\tvalid_1's rmse: 0.783497\tvalid_1's l2: 0.613868\n",
      "[139]\ttraining's rmse: 0.729266\ttraining's l2: 0.531829\tvalid_1's rmse: 0.783448\tvalid_1's l2: 0.613792\n",
      "[140]\ttraining's rmse: 0.728969\ttraining's l2: 0.531396\tvalid_1's rmse: 0.783214\tvalid_1's l2: 0.613424\n",
      "[141]\ttraining's rmse: 0.728709\ttraining's l2: 0.531017\tvalid_1's rmse: 0.783049\tvalid_1's l2: 0.613166\n",
      "[142]\ttraining's rmse: 0.728416\ttraining's l2: 0.53059\tvalid_1's rmse: 0.782955\tvalid_1's l2: 0.613019\n",
      "[143]\ttraining's rmse: 0.728147\ttraining's l2: 0.530199\tvalid_1's rmse: 0.782895\tvalid_1's l2: 0.612925\n",
      "[144]\ttraining's rmse: 0.727935\ttraining's l2: 0.529889\tvalid_1's rmse: 0.782862\tvalid_1's l2: 0.612873\n",
      "[145]\ttraining's rmse: 0.727595\ttraining's l2: 0.529394\tvalid_1's rmse: 0.782725\tvalid_1's l2: 0.612659\n",
      "[146]\ttraining's rmse: 0.72734\ttraining's l2: 0.529023\tvalid_1's rmse: 0.782679\tvalid_1's l2: 0.612587\n",
      "[147]\ttraining's rmse: 0.727098\ttraining's l2: 0.528671\tvalid_1's rmse: 0.782619\tvalid_1's l2: 0.612493\n",
      "[148]\ttraining's rmse: 0.726891\ttraining's l2: 0.52837\tvalid_1's rmse: 0.782498\tvalid_1's l2: 0.612302\n",
      "[149]\ttraining's rmse: 0.726655\ttraining's l2: 0.528027\tvalid_1's rmse: 0.782419\tvalid_1's l2: 0.61218\n",
      "[150]\ttraining's rmse: 0.72644\ttraining's l2: 0.527715\tvalid_1's rmse: 0.782357\tvalid_1's l2: 0.612083\n",
      "[151]\ttraining's rmse: 0.726237\ttraining's l2: 0.527419\tvalid_1's rmse: 0.782273\tvalid_1's l2: 0.611951\n",
      "[152]\ttraining's rmse: 0.726038\ttraining's l2: 0.527131\tvalid_1's rmse: 0.782229\tvalid_1's l2: 0.611882\n",
      "[153]\ttraining's rmse: 0.725859\ttraining's l2: 0.526872\tvalid_1's rmse: 0.782289\tvalid_1's l2: 0.611976\n",
      "[154]\ttraining's rmse: 0.725628\ttraining's l2: 0.526536\tvalid_1's rmse: 0.782277\tvalid_1's l2: 0.611958\n",
      "[155]\ttraining's rmse: 0.725423\ttraining's l2: 0.526238\tvalid_1's rmse: 0.782215\tvalid_1's l2: 0.61186\n",
      "[156]\ttraining's rmse: 0.72523\ttraining's l2: 0.525959\tvalid_1's rmse: 0.782089\tvalid_1's l2: 0.611663\n",
      "[157]\ttraining's rmse: 0.725053\ttraining's l2: 0.525702\tvalid_1's rmse: 0.782051\tvalid_1's l2: 0.611604\n",
      "[158]\ttraining's rmse: 0.724794\ttraining's l2: 0.525327\tvalid_1's rmse: 0.782028\tvalid_1's l2: 0.611567\n",
      "[159]\ttraining's rmse: 0.724583\ttraining's l2: 0.52502\tvalid_1's rmse: 0.781878\tvalid_1's l2: 0.611333\n",
      "[160]\ttraining's rmse: 0.724353\ttraining's l2: 0.524688\tvalid_1's rmse: 0.781912\tvalid_1's l2: 0.611386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[161]\ttraining's rmse: 0.724149\ttraining's l2: 0.524392\tvalid_1's rmse: 0.781807\tvalid_1's l2: 0.611222\n",
      "[162]\ttraining's rmse: 0.723938\ttraining's l2: 0.524086\tvalid_1's rmse: 0.781772\tvalid_1's l2: 0.611167\n",
      "[163]\ttraining's rmse: 0.723743\ttraining's l2: 0.523804\tvalid_1's rmse: 0.781564\tvalid_1's l2: 0.610842\n",
      "[164]\ttraining's rmse: 0.723301\ttraining's l2: 0.523164\tvalid_1's rmse: 0.781783\tvalid_1's l2: 0.611185\n",
      "[165]\ttraining's rmse: 0.723122\ttraining's l2: 0.522906\tvalid_1's rmse: 0.781679\tvalid_1's l2: 0.611023\n",
      "[166]\ttraining's rmse: 0.722873\ttraining's l2: 0.522546\tvalid_1's rmse: 0.781666\tvalid_1's l2: 0.611001\n",
      "[167]\ttraining's rmse: 0.72263\ttraining's l2: 0.522195\tvalid_1's rmse: 0.781568\tvalid_1's l2: 0.610848\n",
      "[168]\ttraining's rmse: 0.722471\ttraining's l2: 0.521964\tvalid_1's rmse: 0.78156\tvalid_1's l2: 0.610836\n",
      "[169]\ttraining's rmse: 0.722238\ttraining's l2: 0.521628\tvalid_1's rmse: 0.781543\tvalid_1's l2: 0.61081\n",
      "[170]\ttraining's rmse: 0.722025\ttraining's l2: 0.52132\tvalid_1's rmse: 0.781444\tvalid_1's l2: 0.610655\n",
      "[171]\ttraining's rmse: 0.721845\ttraining's l2: 0.521061\tvalid_1's rmse: 0.781373\tvalid_1's l2: 0.610544\n",
      "[172]\ttraining's rmse: 0.72156\ttraining's l2: 0.520649\tvalid_1's rmse: 0.781178\tvalid_1's l2: 0.61024\n",
      "[173]\ttraining's rmse: 0.721345\ttraining's l2: 0.520339\tvalid_1's rmse: 0.781021\tvalid_1's l2: 0.609993\n",
      "[174]\ttraining's rmse: 0.72108\ttraining's l2: 0.519957\tvalid_1's rmse: 0.780959\tvalid_1's l2: 0.609896\n",
      "[175]\ttraining's rmse: 0.720926\ttraining's l2: 0.519734\tvalid_1's rmse: 0.780923\tvalid_1's l2: 0.609841\n",
      "[176]\ttraining's rmse: 0.720714\ttraining's l2: 0.519429\tvalid_1's rmse: 0.780832\tvalid_1's l2: 0.609699\n",
      "[177]\ttraining's rmse: 0.720533\ttraining's l2: 0.519167\tvalid_1's rmse: 0.780776\tvalid_1's l2: 0.609612\n",
      "[178]\ttraining's rmse: 0.720165\ttraining's l2: 0.518638\tvalid_1's rmse: 0.780782\tvalid_1's l2: 0.60962\n",
      "[179]\ttraining's rmse: 0.72\ttraining's l2: 0.518399\tvalid_1's rmse: 0.780758\tvalid_1's l2: 0.609583\n",
      "[180]\ttraining's rmse: 0.719735\ttraining's l2: 0.518019\tvalid_1's rmse: 0.780712\tvalid_1's l2: 0.609511\n",
      "[181]\ttraining's rmse: 0.719544\ttraining's l2: 0.517743\tvalid_1's rmse: 0.780695\tvalid_1's l2: 0.609485\n",
      "[182]\ttraining's rmse: 0.719355\ttraining's l2: 0.517471\tvalid_1's rmse: 0.780739\tvalid_1's l2: 0.609553\n",
      "[183]\ttraining's rmse: 0.719182\ttraining's l2: 0.517222\tvalid_1's rmse: 0.780684\tvalid_1's l2: 0.609467\n",
      "[184]\ttraining's rmse: 0.71902\ttraining's l2: 0.516989\tvalid_1's rmse: 0.780499\tvalid_1's l2: 0.609179\n",
      "[185]\ttraining's rmse: 0.718862\ttraining's l2: 0.516763\tvalid_1's rmse: 0.780449\tvalid_1's l2: 0.609101\n",
      "[186]\ttraining's rmse: 0.718669\ttraining's l2: 0.516485\tvalid_1's rmse: 0.780477\tvalid_1's l2: 0.609145\n",
      "[187]\ttraining's rmse: 0.718535\ttraining's l2: 0.516292\tvalid_1's rmse: 0.780461\tvalid_1's l2: 0.60912\n",
      "[188]\ttraining's rmse: 0.718319\ttraining's l2: 0.515982\tvalid_1's rmse: 0.780443\tvalid_1's l2: 0.609091\n",
      "[189]\ttraining's rmse: 0.718147\ttraining's l2: 0.515735\tvalid_1's rmse: 0.780499\tvalid_1's l2: 0.609179\n",
      "[190]\ttraining's rmse: 0.717945\ttraining's l2: 0.515445\tvalid_1's rmse: 0.780535\tvalid_1's l2: 0.609234\n",
      "[191]\ttraining's rmse: 0.717766\ttraining's l2: 0.515188\tvalid_1's rmse: 0.780543\tvalid_1's l2: 0.609247\n",
      "[192]\ttraining's rmse: 0.717563\ttraining's l2: 0.514897\tvalid_1's rmse: 0.780407\tvalid_1's l2: 0.609035\n",
      "[193]\ttraining's rmse: 0.717406\ttraining's l2: 0.514671\tvalid_1's rmse: 0.780332\tvalid_1's l2: 0.608918\n",
      "[194]\ttraining's rmse: 0.71725\ttraining's l2: 0.514448\tvalid_1's rmse: 0.780263\tvalid_1's l2: 0.608811\n",
      "[195]\ttraining's rmse: 0.717102\ttraining's l2: 0.514235\tvalid_1's rmse: 0.780205\tvalid_1's l2: 0.608721\n",
      "[196]\ttraining's rmse: 0.716948\ttraining's l2: 0.514014\tvalid_1's rmse: 0.780127\tvalid_1's l2: 0.608598\n",
      "[197]\ttraining's rmse: 0.716794\ttraining's l2: 0.513794\tvalid_1's rmse: 0.78004\tvalid_1's l2: 0.608462\n",
      "[198]\ttraining's rmse: 0.716629\ttraining's l2: 0.513557\tvalid_1's rmse: 0.780132\tvalid_1's l2: 0.608606\n",
      "[199]\ttraining's rmse: 0.716482\ttraining's l2: 0.513347\tvalid_1's rmse: 0.780146\tvalid_1's l2: 0.608628\n",
      "[200]\ttraining's rmse: 0.716345\ttraining's l2: 0.513151\tvalid_1's rmse: 0.780156\tvalid_1's l2: 0.608643\n",
      "[201]\ttraining's rmse: 0.716205\ttraining's l2: 0.51295\tvalid_1's rmse: 0.780088\tvalid_1's l2: 0.608537\n",
      "[202]\ttraining's rmse: 0.716048\ttraining's l2: 0.512724\tvalid_1's rmse: 0.7801\tvalid_1's l2: 0.608556\n",
      "[203]\ttraining's rmse: 0.715572\ttraining's l2: 0.512043\tvalid_1's rmse: 0.780674\tvalid_1's l2: 0.609453\n",
      "[204]\ttraining's rmse: 0.715408\ttraining's l2: 0.511808\tvalid_1's rmse: 0.780661\tvalid_1's l2: 0.609432\n",
      "[205]\ttraining's rmse: 0.715226\ttraining's l2: 0.511548\tvalid_1's rmse: 0.78061\tvalid_1's l2: 0.609352\n",
      "[206]\ttraining's rmse: 0.715059\ttraining's l2: 0.511309\tvalid_1's rmse: 0.780518\tvalid_1's l2: 0.609209\n",
      "[207]\ttraining's rmse: 0.714905\ttraining's l2: 0.511089\tvalid_1's rmse: 0.780506\tvalid_1's l2: 0.60919\n",
      "Early stopping, best iteration is:\n",
      "[197]\ttraining's rmse: 0.716794\ttraining's l2: 0.513794\tvalid_1's rmse: 0.78004\tvalid_1's l2: 0.608462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(bagging_fraction=0.2, boosting_type='gbdt', class_weight=None,\n",
       "              colsample_bytree=1.0, feature_fraction=0.8,\n",
       "              importance_type='split', learning_rate=0.05, max_depth=-1,\n",
       "              min_child_samples=20, min_child_weight=0.001,\n",
       "              min_data_in_leaf=300, min_split_gain=0.0, n_estimators=1000,\n",
       "              n_jobs=-1, num_leaves=128, objective=None, random_state=None,\n",
       "              reg_alpha=0.0, reg_lambda=0.0, seed=42, silent=True,\n",
       "              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_params = model.best_params_\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(**lgb_params, n_estimators=1000)\n",
    "lgb_model.fit(\n",
    "    X_train, \n",
    "    Y_train,\n",
    "    eval_metric=\"rmse\", \n",
    "    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
    "    verbose=True, \n",
    "    early_stopping_rounds = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params['n_estimators'] = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's rmse: 1.17933\ttraining's l2: 1.39083\tvalid_1's rmse: 1.06395\tvalid_1's l2: 1.13199\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\ttraining's rmse: 1.15218\ttraining's l2: 1.32753\tvalid_1's rmse: 1.04644\tvalid_1's l2: 1.09503\n",
      "[3]\ttraining's rmse: 1.12708\ttraining's l2: 1.2703\tvalid_1's rmse: 1.03007\tvalid_1's l2: 1.06104\n",
      "[4]\ttraining's rmse: 1.1038\ttraining's l2: 1.21837\tvalid_1's rmse: 1.01491\tvalid_1's l2: 1.03004\n",
      "[5]\ttraining's rmse: 1.084\ttraining's l2: 1.17506\tvalid_1's rmse: 1.00078\tvalid_1's l2: 1.00156\n",
      "[6]\ttraining's rmse: 1.06404\ttraining's l2: 1.13218\tvalid_1's rmse: 0.988011\tvalid_1's l2: 0.976166\n",
      "[7]\ttraining's rmse: 1.04711\ttraining's l2: 1.09644\tvalid_1's rmse: 0.975566\tvalid_1's l2: 0.951729\n",
      "[8]\ttraining's rmse: 1.02999\ttraining's l2: 1.06088\tvalid_1's rmse: 0.964773\tvalid_1's l2: 0.930787\n",
      "[9]\ttraining's rmse: 1.01417\ttraining's l2: 1.02854\tvalid_1's rmse: 0.954959\tvalid_1's l2: 0.911946\n",
      "[10]\ttraining's rmse: 1.00098\ttraining's l2: 1.00196\tvalid_1's rmse: 0.945593\tvalid_1's l2: 0.894146\n",
      "[11]\ttraining's rmse: 0.987479\ttraining's l2: 0.975116\tvalid_1's rmse: 0.936917\tvalid_1's l2: 0.877813\n",
      "[12]\ttraining's rmse: 0.975016\ttraining's l2: 0.950656\tvalid_1's rmse: 0.929094\tvalid_1's l2: 0.863216\n",
      "[13]\ttraining's rmse: 0.963534\ttraining's l2: 0.928399\tvalid_1's rmse: 0.921702\tvalid_1's l2: 0.849535\n",
      "[14]\ttraining's rmse: 0.953068\ttraining's l2: 0.908338\tvalid_1's rmse: 0.915054\tvalid_1's l2: 0.837325\n",
      "[15]\ttraining's rmse: 0.943494\ttraining's l2: 0.89018\tvalid_1's rmse: 0.908853\tvalid_1's l2: 0.826013\n",
      "[16]\ttraining's rmse: 0.934413\ttraining's l2: 0.873127\tvalid_1's rmse: 0.902851\tvalid_1's l2: 0.815139\n",
      "[17]\ttraining's rmse: 0.926314\ttraining's l2: 0.858059\tvalid_1's rmse: 0.897671\tvalid_1's l2: 0.805813\n",
      "[18]\ttraining's rmse: 0.919654\ttraining's l2: 0.845763\tvalid_1's rmse: 0.892923\tvalid_1's l2: 0.797312\n",
      "[19]\ttraining's rmse: 0.912556\ttraining's l2: 0.832759\tvalid_1's rmse: 0.888227\tvalid_1's l2: 0.788947\n",
      "[20]\ttraining's rmse: 0.906049\ttraining's l2: 0.820924\tvalid_1's rmse: 0.883922\tvalid_1's l2: 0.781319\n",
      "[21]\ttraining's rmse: 0.899944\ttraining's l2: 0.809898\tvalid_1's rmse: 0.879994\tvalid_1's l2: 0.77439\n",
      "[22]\ttraining's rmse: 0.894415\ttraining's l2: 0.799978\tvalid_1's rmse: 0.876315\tvalid_1's l2: 0.767928\n",
      "[23]\ttraining's rmse: 0.889283\ttraining's l2: 0.790824\tvalid_1's rmse: 0.873046\tvalid_1's l2: 0.762209\n",
      "[24]\ttraining's rmse: 0.884607\ttraining's l2: 0.782529\tvalid_1's rmse: 0.870058\tvalid_1's l2: 0.757002\n",
      "[25]\ttraining's rmse: 0.880324\ttraining's l2: 0.77497\tvalid_1's rmse: 0.867077\tvalid_1's l2: 0.751822\n",
      "[26]\ttraining's rmse: 0.876351\ttraining's l2: 0.767992\tvalid_1's rmse: 0.864226\tvalid_1's l2: 0.746886\n",
      "[27]\ttraining's rmse: 0.87274\ttraining's l2: 0.761676\tvalid_1's rmse: 0.861768\tvalid_1's l2: 0.742643\n",
      "[28]\ttraining's rmse: 0.869317\ttraining's l2: 0.755712\tvalid_1's rmse: 0.85937\tvalid_1's l2: 0.738516\n",
      "[29]\ttraining's rmse: 0.866128\ttraining's l2: 0.750177\tvalid_1's rmse: 0.857516\tvalid_1's l2: 0.735333\n",
      "[30]\ttraining's rmse: 0.863577\ttraining's l2: 0.745766\tvalid_1's rmse: 0.85588\tvalid_1's l2: 0.732531\n",
      "[31]\ttraining's rmse: 0.860861\ttraining's l2: 0.741082\tvalid_1's rmse: 0.853961\tvalid_1's l2: 0.72925\n",
      "[32]\ttraining's rmse: 0.858308\ttraining's l2: 0.736693\tvalid_1's rmse: 0.852319\tvalid_1's l2: 0.726448\n",
      "[33]\ttraining's rmse: 0.855942\ttraining's l2: 0.732636\tvalid_1's rmse: 0.850816\tvalid_1's l2: 0.723888\n",
      "[34]\ttraining's rmse: 0.853716\ttraining's l2: 0.72883\tvalid_1's rmse: 0.848985\tvalid_1's l2: 0.720776\n",
      "[35]\ttraining's rmse: 0.851436\ttraining's l2: 0.724943\tvalid_1's rmse: 0.847746\tvalid_1's l2: 0.718674\n",
      "[36]\ttraining's rmse: 0.849759\ttraining's l2: 0.72209\tvalid_1's rmse: 0.846702\tvalid_1's l2: 0.716905\n",
      "[37]\ttraining's rmse: 0.84781\ttraining's l2: 0.718781\tvalid_1's rmse: 0.84561\tvalid_1's l2: 0.715056\n",
      "[38]\ttraining's rmse: 0.846055\ttraining's l2: 0.715809\tvalid_1's rmse: 0.844413\tvalid_1's l2: 0.713033\n",
      "[39]\ttraining's rmse: 0.84441\ttraining's l2: 0.713028\tvalid_1's rmse: 0.84331\tvalid_1's l2: 0.711172\n",
      "[40]\ttraining's rmse: 0.842756\ttraining's l2: 0.710238\tvalid_1's rmse: 0.842237\tvalid_1's l2: 0.709364\n",
      "[41]\ttraining's rmse: 0.841236\ttraining's l2: 0.707679\tvalid_1's rmse: 0.841179\tvalid_1's l2: 0.707582\n",
      "[42]\ttraining's rmse: 0.839914\ttraining's l2: 0.705456\tvalid_1's rmse: 0.840136\tvalid_1's l2: 0.705829\n",
      "[43]\ttraining's rmse: 0.838577\ttraining's l2: 0.703211\tvalid_1's rmse: 0.839161\tvalid_1's l2: 0.704191\n",
      "[44]\ttraining's rmse: 0.837371\ttraining's l2: 0.70119\tvalid_1's rmse: 0.838364\tvalid_1's l2: 0.702854\n",
      "[45]\ttraining's rmse: 0.836247\ttraining's l2: 0.699309\tvalid_1's rmse: 0.837447\tvalid_1's l2: 0.701317\n",
      "[46]\ttraining's rmse: 0.835066\ttraining's l2: 0.697335\tvalid_1's rmse: 0.836589\tvalid_1's l2: 0.699881\n",
      "[47]\ttraining's rmse: 0.833909\ttraining's l2: 0.695404\tvalid_1's rmse: 0.835975\tvalid_1's l2: 0.698855\n",
      "[48]\ttraining's rmse: 0.832847\ttraining's l2: 0.693634\tvalid_1's rmse: 0.835218\tvalid_1's l2: 0.697588\n",
      "[49]\ttraining's rmse: 0.832108\ttraining's l2: 0.692403\tvalid_1's rmse: 0.834683\tvalid_1's l2: 0.696695\n",
      "[50]\ttraining's rmse: 0.831142\ttraining's l2: 0.690797\tvalid_1's rmse: 0.834128\tvalid_1's l2: 0.69577\n",
      "[51]\ttraining's rmse: 0.83044\ttraining's l2: 0.689631\tvalid_1's rmse: 0.833811\tvalid_1's l2: 0.695241\n",
      "[52]\ttraining's rmse: 0.829777\ttraining's l2: 0.68853\tvalid_1's rmse: 0.83334\tvalid_1's l2: 0.694456\n",
      "[53]\ttraining's rmse: 0.829045\ttraining's l2: 0.687315\tvalid_1's rmse: 0.832967\tvalid_1's l2: 0.693834\n",
      "[54]\ttraining's rmse: 0.828195\ttraining's l2: 0.685907\tvalid_1's rmse: 0.832355\tvalid_1's l2: 0.692815\n",
      "[55]\ttraining's rmse: 0.82762\ttraining's l2: 0.684954\tvalid_1's rmse: 0.832114\tvalid_1's l2: 0.692414\n",
      "[56]\ttraining's rmse: 0.826842\ttraining's l2: 0.683667\tvalid_1's rmse: 0.831581\tvalid_1's l2: 0.691526\n",
      "[57]\ttraining's rmse: 0.826134\ttraining's l2: 0.682497\tvalid_1's rmse: 0.831098\tvalid_1's l2: 0.690725\n",
      "[58]\ttraining's rmse: 0.825436\ttraining's l2: 0.681345\tvalid_1's rmse: 0.830702\tvalid_1's l2: 0.690065\n",
      "[59]\ttraining's rmse: 0.824695\ttraining's l2: 0.680122\tvalid_1's rmse: 0.830249\tvalid_1's l2: 0.689314\n",
      "[60]\ttraining's rmse: 0.823984\ttraining's l2: 0.67895\tvalid_1's rmse: 0.829775\tvalid_1's l2: 0.688526\n",
      "[61]\ttraining's rmse: 0.823376\ttraining's l2: 0.677948\tvalid_1's rmse: 0.829429\tvalid_1's l2: 0.687953\n",
      "[62]\ttraining's rmse: 0.822729\ttraining's l2: 0.676884\tvalid_1's rmse: 0.829208\tvalid_1's l2: 0.687586\n",
      "[63]\ttraining's rmse: 0.822139\ttraining's l2: 0.675912\tvalid_1's rmse: 0.828976\tvalid_1's l2: 0.687201\n",
      "[64]\ttraining's rmse: 0.821689\ttraining's l2: 0.675172\tvalid_1's rmse: 0.828837\tvalid_1's l2: 0.68697\n",
      "[65]\ttraining's rmse: 0.821104\ttraining's l2: 0.674212\tvalid_1's rmse: 0.828541\tvalid_1's l2: 0.68648\n",
      "[66]\ttraining's rmse: 0.820577\ttraining's l2: 0.673347\tvalid_1's rmse: 0.828407\tvalid_1's l2: 0.686258\n",
      "[67]\ttraining's rmse: 0.820079\ttraining's l2: 0.672529\tvalid_1's rmse: 0.828073\tvalid_1's l2: 0.685705\n",
      "[68]\ttraining's rmse: 0.819553\ttraining's l2: 0.671667\tvalid_1's rmse: 0.827808\tvalid_1's l2: 0.685266\n",
      "[69]\ttraining's rmse: 0.819059\ttraining's l2: 0.670857\tvalid_1's rmse: 0.827362\tvalid_1's l2: 0.684528\n",
      "[70]\ttraining's rmse: 0.818554\ttraining's l2: 0.670031\tvalid_1's rmse: 0.826853\tvalid_1's l2: 0.683685\n",
      "[71]\ttraining's rmse: 0.81813\ttraining's l2: 0.669336\tvalid_1's rmse: 0.826647\tvalid_1's l2: 0.683346\n",
      "[72]\ttraining's rmse: 0.817592\ttraining's l2: 0.668456\tvalid_1's rmse: 0.826409\tvalid_1's l2: 0.682952\n",
      "[73]\ttraining's rmse: 0.817108\ttraining's l2: 0.667665\tvalid_1's rmse: 0.825925\tvalid_1's l2: 0.682151\n",
      "[74]\ttraining's rmse: 0.816669\ttraining's l2: 0.666948\tvalid_1's rmse: 0.825469\tvalid_1's l2: 0.6814\n",
      "[75]\ttraining's rmse: 0.816198\ttraining's l2: 0.666179\tvalid_1's rmse: 0.825231\tvalid_1's l2: 0.681007\n",
      "[76]\ttraining's rmse: 0.815773\ttraining's l2: 0.665485\tvalid_1's rmse: 0.824999\tvalid_1's l2: 0.680624\n",
      "[77]\ttraining's rmse: 0.815372\ttraining's l2: 0.664832\tvalid_1's rmse: 0.824659\tvalid_1's l2: 0.680062\n",
      "[78]\ttraining's rmse: 0.815018\ttraining's l2: 0.664254\tvalid_1's rmse: 0.824525\tvalid_1's l2: 0.679841\n",
      "[79]\ttraining's rmse: 0.814624\ttraining's l2: 0.663613\tvalid_1's rmse: 0.82428\tvalid_1's l2: 0.679438\n",
      "[80]\ttraining's rmse: 0.814251\ttraining's l2: 0.663005\tvalid_1's rmse: 0.82418\tvalid_1's l2: 0.679273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[81]\ttraining's rmse: 0.813867\ttraining's l2: 0.66238\tvalid_1's rmse: 0.82393\tvalid_1's l2: 0.678861\n",
      "[82]\ttraining's rmse: 0.813504\ttraining's l2: 0.661789\tvalid_1's rmse: 0.823723\tvalid_1's l2: 0.67852\n",
      "[83]\ttraining's rmse: 0.813139\ttraining's l2: 0.661195\tvalid_1's rmse: 0.823484\tvalid_1's l2: 0.678125\n",
      "[84]\ttraining's rmse: 0.812802\ttraining's l2: 0.660648\tvalid_1's rmse: 0.823452\tvalid_1's l2: 0.678073\n",
      "[85]\ttraining's rmse: 0.812457\ttraining's l2: 0.660087\tvalid_1's rmse: 0.82334\tvalid_1's l2: 0.677889\n",
      "[86]\ttraining's rmse: 0.81209\ttraining's l2: 0.65949\tvalid_1's rmse: 0.822969\tvalid_1's l2: 0.677278\n",
      "[87]\ttraining's rmse: 0.811798\ttraining's l2: 0.659016\tvalid_1's rmse: 0.82287\tvalid_1's l2: 0.677116\n",
      "[88]\ttraining's rmse: 0.811438\ttraining's l2: 0.658432\tvalid_1's rmse: 0.822552\tvalid_1's l2: 0.676592\n",
      "[89]\ttraining's rmse: 0.811109\ttraining's l2: 0.657898\tvalid_1's rmse: 0.822564\tvalid_1's l2: 0.676612\n",
      "[90]\ttraining's rmse: 0.81074\ttraining's l2: 0.6573\tvalid_1's rmse: 0.822137\tvalid_1's l2: 0.675909\n",
      "[91]\ttraining's rmse: 0.810436\ttraining's l2: 0.656807\tvalid_1's rmse: 0.821953\tvalid_1's l2: 0.675607\n",
      "[92]\ttraining's rmse: 0.810097\ttraining's l2: 0.656256\tvalid_1's rmse: 0.821853\tvalid_1's l2: 0.675442\n",
      "[93]\ttraining's rmse: 0.809855\ttraining's l2: 0.655865\tvalid_1's rmse: 0.821772\tvalid_1's l2: 0.675309\n",
      "[94]\ttraining's rmse: 0.809574\ttraining's l2: 0.655409\tvalid_1's rmse: 0.82166\tvalid_1's l2: 0.675125\n",
      "[95]\ttraining's rmse: 0.809308\ttraining's l2: 0.65498\tvalid_1's rmse: 0.821586\tvalid_1's l2: 0.675003\n",
      "[96]\ttraining's rmse: 0.809011\ttraining's l2: 0.654499\tvalid_1's rmse: 0.821354\tvalid_1's l2: 0.674622\n",
      "[97]\ttraining's rmse: 0.808738\ttraining's l2: 0.654058\tvalid_1's rmse: 0.821171\tvalid_1's l2: 0.674321\n",
      "[98]\ttraining's rmse: 0.808445\ttraining's l2: 0.653583\tvalid_1's rmse: 0.821088\tvalid_1's l2: 0.674186\n",
      "[99]\ttraining's rmse: 0.80813\ttraining's l2: 0.653074\tvalid_1's rmse: 0.821075\tvalid_1's l2: 0.674163\n",
      "[100]\ttraining's rmse: 0.807866\ttraining's l2: 0.652647\tvalid_1's rmse: 0.820991\tvalid_1's l2: 0.674026\n",
      "[101]\ttraining's rmse: 0.807581\ttraining's l2: 0.652187\tvalid_1's rmse: 0.820995\tvalid_1's l2: 0.674033\n",
      "[102]\ttraining's rmse: 0.807318\ttraining's l2: 0.651762\tvalid_1's rmse: 0.820956\tvalid_1's l2: 0.673969\n",
      "[103]\ttraining's rmse: 0.807031\ttraining's l2: 0.651299\tvalid_1's rmse: 0.820971\tvalid_1's l2: 0.673993\n",
      "[104]\ttraining's rmse: 0.806736\ttraining's l2: 0.650823\tvalid_1's rmse: 0.82089\tvalid_1's l2: 0.673861\n",
      "[105]\ttraining's rmse: 0.806487\ttraining's l2: 0.650421\tvalid_1's rmse: 0.820777\tvalid_1's l2: 0.673675\n",
      "[106]\ttraining's rmse: 0.806226\ttraining's l2: 0.650001\tvalid_1's rmse: 0.820547\tvalid_1's l2: 0.673297\n",
      "[107]\ttraining's rmse: 0.805976\ttraining's l2: 0.649597\tvalid_1's rmse: 0.820637\tvalid_1's l2: 0.673445\n",
      "[108]\ttraining's rmse: 0.805745\ttraining's l2: 0.649226\tvalid_1's rmse: 0.820565\tvalid_1's l2: 0.673327\n",
      "[109]\ttraining's rmse: 0.805489\ttraining's l2: 0.648812\tvalid_1's rmse: 0.820453\tvalid_1's l2: 0.673144\n",
      "[110]\ttraining's rmse: 0.805268\ttraining's l2: 0.648457\tvalid_1's rmse: 0.820419\tvalid_1's l2: 0.673087\n",
      "[111]\ttraining's rmse: 0.805052\ttraining's l2: 0.648109\tvalid_1's rmse: 0.820273\tvalid_1's l2: 0.672847\n",
      "[112]\ttraining's rmse: 0.804783\ttraining's l2: 0.647676\tvalid_1's rmse: 0.820212\tvalid_1's l2: 0.672748\n",
      "[113]\ttraining's rmse: 0.804551\ttraining's l2: 0.647302\tvalid_1's rmse: 0.820068\tvalid_1's l2: 0.672512\n",
      "[114]\ttraining's rmse: 0.804321\ttraining's l2: 0.646932\tvalid_1's rmse: 0.819963\tvalid_1's l2: 0.672339\n",
      "[115]\ttraining's rmse: 0.804073\ttraining's l2: 0.646533\tvalid_1's rmse: 0.819962\tvalid_1's l2: 0.672338\n",
      "[116]\ttraining's rmse: 0.803842\ttraining's l2: 0.646162\tvalid_1's rmse: 0.819986\tvalid_1's l2: 0.672378\n",
      "[117]\ttraining's rmse: 0.803597\ttraining's l2: 0.645768\tvalid_1's rmse: 0.820003\tvalid_1's l2: 0.672404\n",
      "[118]\ttraining's rmse: 0.803378\ttraining's l2: 0.645416\tvalid_1's rmse: 0.820028\tvalid_1's l2: 0.672446\n",
      "[119]\ttraining's rmse: 0.803133\ttraining's l2: 0.645023\tvalid_1's rmse: 0.81979\tvalid_1's l2: 0.672056\n",
      "[120]\ttraining's rmse: 0.802921\ttraining's l2: 0.644682\tvalid_1's rmse: 0.819702\tvalid_1's l2: 0.671912\n",
      "[121]\ttraining's rmse: 0.802687\ttraining's l2: 0.644307\tvalid_1's rmse: 0.819645\tvalid_1's l2: 0.671818\n",
      "[122]\ttraining's rmse: 0.802414\ttraining's l2: 0.643867\tvalid_1's rmse: 0.819437\tvalid_1's l2: 0.671476\n",
      "[123]\ttraining's rmse: 0.80217\ttraining's l2: 0.643476\tvalid_1's rmse: 0.819345\tvalid_1's l2: 0.671327\n",
      "[124]\ttraining's rmse: 0.801952\ttraining's l2: 0.643127\tvalid_1's rmse: 0.819261\tvalid_1's l2: 0.671189\n",
      "[125]\ttraining's rmse: 0.801762\ttraining's l2: 0.642822\tvalid_1's rmse: 0.819008\tvalid_1's l2: 0.670773\n",
      "[126]\ttraining's rmse: 0.801552\ttraining's l2: 0.642485\tvalid_1's rmse: 0.818981\tvalid_1's l2: 0.67073\n",
      "[127]\ttraining's rmse: 0.801367\ttraining's l2: 0.642189\tvalid_1's rmse: 0.818921\tvalid_1's l2: 0.670632\n",
      "[128]\ttraining's rmse: 0.801134\ttraining's l2: 0.641816\tvalid_1's rmse: 0.818895\tvalid_1's l2: 0.670589\n",
      "[129]\ttraining's rmse: 0.800921\ttraining's l2: 0.641474\tvalid_1's rmse: 0.818794\tvalid_1's l2: 0.670424\n",
      "[130]\ttraining's rmse: 0.800709\ttraining's l2: 0.641135\tvalid_1's rmse: 0.818791\tvalid_1's l2: 0.670419\n",
      "[131]\ttraining's rmse: 0.800515\ttraining's l2: 0.640824\tvalid_1's rmse: 0.81876\tvalid_1's l2: 0.670368\n",
      "[132]\ttraining's rmse: 0.800301\ttraining's l2: 0.640482\tvalid_1's rmse: 0.818702\tvalid_1's l2: 0.670274\n",
      "[133]\ttraining's rmse: 0.800115\ttraining's l2: 0.640183\tvalid_1's rmse: 0.818587\tvalid_1's l2: 0.670084\n",
      "[134]\ttraining's rmse: 0.799884\ttraining's l2: 0.639814\tvalid_1's rmse: 0.818675\tvalid_1's l2: 0.670229\n",
      "[135]\ttraining's rmse: 0.799669\ttraining's l2: 0.639471\tvalid_1's rmse: 0.818581\tvalid_1's l2: 0.670074\n",
      "[136]\ttraining's rmse: 0.799481\ttraining's l2: 0.639169\tvalid_1's rmse: 0.818521\tvalid_1's l2: 0.669977\n",
      "[137]\ttraining's rmse: 0.79929\ttraining's l2: 0.638864\tvalid_1's rmse: 0.818485\tvalid_1's l2: 0.669918\n",
      "[138]\ttraining's rmse: 0.799104\ttraining's l2: 0.638568\tvalid_1's rmse: 0.818368\tvalid_1's l2: 0.669727\n",
      "[139]\ttraining's rmse: 0.798884\ttraining's l2: 0.638215\tvalid_1's rmse: 0.818427\tvalid_1's l2: 0.669823\n",
      "[140]\ttraining's rmse: 0.79873\ttraining's l2: 0.63797\tvalid_1's rmse: 0.818378\tvalid_1's l2: 0.669743\n",
      "[141]\ttraining's rmse: 0.798545\ttraining's l2: 0.637674\tvalid_1's rmse: 0.818363\tvalid_1's l2: 0.669718\n",
      "[142]\ttraining's rmse: 0.798349\ttraining's l2: 0.637361\tvalid_1's rmse: 0.818376\tvalid_1's l2: 0.66974\n",
      "[143]\ttraining's rmse: 0.798143\ttraining's l2: 0.637032\tvalid_1's rmse: 0.818381\tvalid_1's l2: 0.669748\n",
      "[144]\ttraining's rmse: 0.797961\ttraining's l2: 0.636742\tvalid_1's rmse: 0.818362\tvalid_1's l2: 0.669716\n",
      "[145]\ttraining's rmse: 0.797791\ttraining's l2: 0.636471\tvalid_1's rmse: 0.818275\tvalid_1's l2: 0.669573\n",
      "[146]\ttraining's rmse: 0.797597\ttraining's l2: 0.636161\tvalid_1's rmse: 0.818215\tvalid_1's l2: 0.669475\n",
      "[147]\ttraining's rmse: 0.797427\ttraining's l2: 0.635889\tvalid_1's rmse: 0.818195\tvalid_1's l2: 0.669443\n",
      "[148]\ttraining's rmse: 0.797258\ttraining's l2: 0.63562\tvalid_1's rmse: 0.818102\tvalid_1's l2: 0.669291\n",
      "[149]\ttraining's rmse: 0.79711\ttraining's l2: 0.635384\tvalid_1's rmse: 0.818039\tvalid_1's l2: 0.669188\n",
      "[150]\ttraining's rmse: 0.796899\ttraining's l2: 0.635048\tvalid_1's rmse: 0.817851\tvalid_1's l2: 0.66888\n",
      "[151]\ttraining's rmse: 0.796741\ttraining's l2: 0.634796\tvalid_1's rmse: 0.817814\tvalid_1's l2: 0.668819\n",
      "[152]\ttraining's rmse: 0.796589\ttraining's l2: 0.634554\tvalid_1's rmse: 0.817699\tvalid_1's l2: 0.668631\n",
      "[153]\ttraining's rmse: 0.796421\ttraining's l2: 0.634286\tvalid_1's rmse: 0.817641\tvalid_1's l2: 0.668537\n",
      "[154]\ttraining's rmse: 0.796249\ttraining's l2: 0.634012\tvalid_1's rmse: 0.817613\tvalid_1's l2: 0.668492\n",
      "[155]\ttraining's rmse: 0.796093\ttraining's l2: 0.633764\tvalid_1's rmse: 0.817661\tvalid_1's l2: 0.66857\n",
      "[156]\ttraining's rmse: 0.795932\ttraining's l2: 0.633508\tvalid_1's rmse: 0.817558\tvalid_1's l2: 0.668402\n",
      "[157]\ttraining's rmse: 0.795774\ttraining's l2: 0.633257\tvalid_1's rmse: 0.817508\tvalid_1's l2: 0.668319\n",
      "[158]\ttraining's rmse: 0.795589\ttraining's l2: 0.632963\tvalid_1's rmse: 0.817445\tvalid_1's l2: 0.668217\n",
      "[159]\ttraining's rmse: 0.795414\ttraining's l2: 0.632683\tvalid_1's rmse: 0.817392\tvalid_1's l2: 0.66813\n",
      "[160]\ttraining's rmse: 0.795253\ttraining's l2: 0.632428\tvalid_1's rmse: 0.817353\tvalid_1's l2: 0.668066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[161]\ttraining's rmse: 0.795087\ttraining's l2: 0.632163\tvalid_1's rmse: 0.817307\tvalid_1's l2: 0.667991\n",
      "[162]\ttraining's rmse: 0.794938\ttraining's l2: 0.631927\tvalid_1's rmse: 0.817293\tvalid_1's l2: 0.667968\n",
      "[163]\ttraining's rmse: 0.794775\ttraining's l2: 0.631667\tvalid_1's rmse: 0.817156\tvalid_1's l2: 0.667745\n",
      "[164]\ttraining's rmse: 0.794581\ttraining's l2: 0.631359\tvalid_1's rmse: 0.817119\tvalid_1's l2: 0.667684\n",
      "[165]\ttraining's rmse: 0.794442\ttraining's l2: 0.631138\tvalid_1's rmse: 0.817163\tvalid_1's l2: 0.667755\n",
      "[166]\ttraining's rmse: 0.794286\ttraining's l2: 0.63089\tvalid_1's rmse: 0.817044\tvalid_1's l2: 0.667561\n",
      "[167]\ttraining's rmse: 0.794135\ttraining's l2: 0.630651\tvalid_1's rmse: 0.817093\tvalid_1's l2: 0.667641\n",
      "[168]\ttraining's rmse: 0.79398\ttraining's l2: 0.630404\tvalid_1's rmse: 0.817084\tvalid_1's l2: 0.667626\n",
      "[169]\ttraining's rmse: 0.793788\ttraining's l2: 0.6301\tvalid_1's rmse: 0.817105\tvalid_1's l2: 0.667661\n",
      "[170]\ttraining's rmse: 0.793617\ttraining's l2: 0.629828\tvalid_1's rmse: 0.817084\tvalid_1's l2: 0.667626\n",
      "[171]\ttraining's rmse: 0.793479\ttraining's l2: 0.629609\tvalid_1's rmse: 0.817056\tvalid_1's l2: 0.667581\n",
      "[172]\ttraining's rmse: 0.793325\ttraining's l2: 0.629364\tvalid_1's rmse: 0.816816\tvalid_1's l2: 0.667188\n",
      "[173]\ttraining's rmse: 0.793185\ttraining's l2: 0.629143\tvalid_1's rmse: 0.816826\tvalid_1's l2: 0.667205\n",
      "[174]\ttraining's rmse: 0.793053\ttraining's l2: 0.628934\tvalid_1's rmse: 0.816916\tvalid_1's l2: 0.667352\n",
      "[175]\ttraining's rmse: 0.792888\ttraining's l2: 0.628671\tvalid_1's rmse: 0.816775\tvalid_1's l2: 0.667122\n",
      "[176]\ttraining's rmse: 0.792714\ttraining's l2: 0.628395\tvalid_1's rmse: 0.816716\tvalid_1's l2: 0.667025\n",
      "[177]\ttraining's rmse: 0.792544\ttraining's l2: 0.628126\tvalid_1's rmse: 0.816669\tvalid_1's l2: 0.666948\n",
      "[178]\ttraining's rmse: 0.792402\ttraining's l2: 0.627901\tvalid_1's rmse: 0.816686\tvalid_1's l2: 0.666976\n",
      "[179]\ttraining's rmse: 0.79225\ttraining's l2: 0.62766\tvalid_1's rmse: 0.816647\tvalid_1's l2: 0.666912\n",
      "[180]\ttraining's rmse: 0.792077\ttraining's l2: 0.627385\tvalid_1's rmse: 0.816682\tvalid_1's l2: 0.66697\n",
      "[181]\ttraining's rmse: 0.791923\ttraining's l2: 0.627142\tvalid_1's rmse: 0.816553\tvalid_1's l2: 0.666759\n",
      "[182]\ttraining's rmse: 0.791796\ttraining's l2: 0.626941\tvalid_1's rmse: 0.816498\tvalid_1's l2: 0.666669\n",
      "[183]\ttraining's rmse: 0.791617\ttraining's l2: 0.626657\tvalid_1's rmse: 0.816318\tvalid_1's l2: 0.666375\n",
      "[184]\ttraining's rmse: 0.791462\ttraining's l2: 0.626412\tvalid_1's rmse: 0.816342\tvalid_1's l2: 0.666414\n",
      "[185]\ttraining's rmse: 0.791291\ttraining's l2: 0.626141\tvalid_1's rmse: 0.816203\tvalid_1's l2: 0.666187\n",
      "[186]\ttraining's rmse: 0.791164\ttraining's l2: 0.62594\tvalid_1's rmse: 0.816187\tvalid_1's l2: 0.666162\n",
      "[187]\ttraining's rmse: 0.791006\ttraining's l2: 0.625691\tvalid_1's rmse: 0.816049\tvalid_1's l2: 0.665936\n",
      "[188]\ttraining's rmse: 0.790881\ttraining's l2: 0.625492\tvalid_1's rmse: 0.815965\tvalid_1's l2: 0.665799\n",
      "[189]\ttraining's rmse: 0.790725\ttraining's l2: 0.625246\tvalid_1's rmse: 0.815922\tvalid_1's l2: 0.665729\n",
      "[190]\ttraining's rmse: 0.790601\ttraining's l2: 0.625049\tvalid_1's rmse: 0.815913\tvalid_1's l2: 0.665714\n",
      "[191]\ttraining's rmse: 0.79045\ttraining's l2: 0.624811\tvalid_1's rmse: 0.815891\tvalid_1's l2: 0.665678\n",
      "[192]\ttraining's rmse: 0.790266\ttraining's l2: 0.62452\tvalid_1's rmse: 0.815928\tvalid_1's l2: 0.665738\n",
      "[193]\ttraining's rmse: 0.790122\ttraining's l2: 0.624292\tvalid_1's rmse: 0.81594\tvalid_1's l2: 0.665758\n",
      "[194]\ttraining's rmse: 0.789994\ttraining's l2: 0.62409\tvalid_1's rmse: 0.815938\tvalid_1's l2: 0.665755\n",
      "[195]\ttraining's rmse: 0.789829\ttraining's l2: 0.623829\tvalid_1's rmse: 0.815983\tvalid_1's l2: 0.665828\n",
      "[196]\ttraining's rmse: 0.789694\ttraining's l2: 0.623617\tvalid_1's rmse: 0.815919\tvalid_1's l2: 0.665723\n",
      "[197]\ttraining's rmse: 0.78956\ttraining's l2: 0.623404\tvalid_1's rmse: 0.815917\tvalid_1's l2: 0.665721\n",
      "[198]\ttraining's rmse: 0.789425\ttraining's l2: 0.623191\tvalid_1's rmse: 0.815831\tvalid_1's l2: 0.66558\n",
      "[199]\ttraining's rmse: 0.789286\ttraining's l2: 0.622972\tvalid_1's rmse: 0.815798\tvalid_1's l2: 0.665526\n",
      "[200]\ttraining's rmse: 0.789129\ttraining's l2: 0.622724\tvalid_1's rmse: 0.815783\tvalid_1's l2: 0.665502\n",
      "[201]\ttraining's rmse: 0.788998\ttraining's l2: 0.622517\tvalid_1's rmse: 0.815725\tvalid_1's l2: 0.665408\n",
      "[202]\ttraining's rmse: 0.788878\ttraining's l2: 0.622328\tvalid_1's rmse: 0.815767\tvalid_1's l2: 0.665477\n",
      "[203]\ttraining's rmse: 0.788754\ttraining's l2: 0.622133\tvalid_1's rmse: 0.815683\tvalid_1's l2: 0.665338\n",
      "[204]\ttraining's rmse: 0.788603\ttraining's l2: 0.621894\tvalid_1's rmse: 0.81567\tvalid_1's l2: 0.665318\n",
      "[205]\ttraining's rmse: 0.788474\ttraining's l2: 0.621691\tvalid_1's rmse: 0.815561\tvalid_1's l2: 0.66514\n",
      "[206]\ttraining's rmse: 0.788358\ttraining's l2: 0.621508\tvalid_1's rmse: 0.815603\tvalid_1's l2: 0.665209\n",
      "[207]\ttraining's rmse: 0.78823\ttraining's l2: 0.621306\tvalid_1's rmse: 0.815483\tvalid_1's l2: 0.665012\n",
      "[208]\ttraining's rmse: 0.788106\ttraining's l2: 0.62111\tvalid_1's rmse: 0.815494\tvalid_1's l2: 0.66503\n",
      "[209]\ttraining's rmse: 0.787974\ttraining's l2: 0.620903\tvalid_1's rmse: 0.815389\tvalid_1's l2: 0.664859\n",
      "[210]\ttraining's rmse: 0.787857\ttraining's l2: 0.620719\tvalid_1's rmse: 0.815421\tvalid_1's l2: 0.664911\n",
      "[211]\ttraining's rmse: 0.787728\ttraining's l2: 0.620515\tvalid_1's rmse: 0.815414\tvalid_1's l2: 0.6649\n",
      "[212]\ttraining's rmse: 0.787614\ttraining's l2: 0.620336\tvalid_1's rmse: 0.815374\tvalid_1's l2: 0.664835\n",
      "[213]\ttraining's rmse: 0.787498\ttraining's l2: 0.620153\tvalid_1's rmse: 0.815357\tvalid_1's l2: 0.664807\n",
      "[214]\ttraining's rmse: 0.787356\ttraining's l2: 0.619929\tvalid_1's rmse: 0.815156\tvalid_1's l2: 0.664479\n",
      "[215]\ttraining's rmse: 0.787201\ttraining's l2: 0.619686\tvalid_1's rmse: 0.81485\tvalid_1's l2: 0.663981\n",
      "[216]\ttraining's rmse: 0.787066\ttraining's l2: 0.619473\tvalid_1's rmse: 0.814835\tvalid_1's l2: 0.663956\n",
      "[217]\ttraining's rmse: 0.786957\ttraining's l2: 0.619301\tvalid_1's rmse: 0.814781\tvalid_1's l2: 0.663868\n",
      "[218]\ttraining's rmse: 0.786861\ttraining's l2: 0.61915\tvalid_1's rmse: 0.814776\tvalid_1's l2: 0.66386\n",
      "[219]\ttraining's rmse: 0.786765\ttraining's l2: 0.618999\tvalid_1's rmse: 0.81474\tvalid_1's l2: 0.663801\n",
      "[220]\ttraining's rmse: 0.786652\ttraining's l2: 0.618822\tvalid_1's rmse: 0.814687\tvalid_1's l2: 0.663715\n",
      "[221]\ttraining's rmse: 0.786542\ttraining's l2: 0.618649\tvalid_1's rmse: 0.814649\tvalid_1's l2: 0.663653\n",
      "[222]\ttraining's rmse: 0.786437\ttraining's l2: 0.618483\tvalid_1's rmse: 0.814556\tvalid_1's l2: 0.663501\n",
      "[223]\ttraining's rmse: 0.786325\ttraining's l2: 0.618308\tvalid_1's rmse: 0.814512\tvalid_1's l2: 0.66343\n",
      "[224]\ttraining's rmse: 0.786229\ttraining's l2: 0.618156\tvalid_1's rmse: 0.81451\tvalid_1's l2: 0.663427\n",
      "[225]\ttraining's rmse: 0.786104\ttraining's l2: 0.61796\tvalid_1's rmse: 0.814502\tvalid_1's l2: 0.663413\n",
      "[226]\ttraining's rmse: 0.785986\ttraining's l2: 0.617774\tvalid_1's rmse: 0.814428\tvalid_1's l2: 0.663292\n",
      "[227]\ttraining's rmse: 0.785879\ttraining's l2: 0.617606\tvalid_1's rmse: 0.814363\tvalid_1's l2: 0.663187\n",
      "[228]\ttraining's rmse: 0.785728\ttraining's l2: 0.617369\tvalid_1's rmse: 0.814396\tvalid_1's l2: 0.663241\n",
      "[229]\ttraining's rmse: 0.785619\ttraining's l2: 0.617198\tvalid_1's rmse: 0.814358\tvalid_1's l2: 0.663178\n",
      "[230]\ttraining's rmse: 0.78552\ttraining's l2: 0.617042\tvalid_1's rmse: 0.814335\tvalid_1's l2: 0.663142\n",
      "[231]\ttraining's rmse: 0.785394\ttraining's l2: 0.616843\tvalid_1's rmse: 0.814302\tvalid_1's l2: 0.663088\n",
      "[232]\ttraining's rmse: 0.785277\ttraining's l2: 0.616659\tvalid_1's rmse: 0.814219\tvalid_1's l2: 0.662952\n",
      "[233]\ttraining's rmse: 0.785149\ttraining's l2: 0.61646\tvalid_1's rmse: 0.814244\tvalid_1's l2: 0.662993\n",
      "[234]\ttraining's rmse: 0.785021\ttraining's l2: 0.616258\tvalid_1's rmse: 0.814241\tvalid_1's l2: 0.662988\n",
      "[235]\ttraining's rmse: 0.784924\ttraining's l2: 0.616106\tvalid_1's rmse: 0.814225\tvalid_1's l2: 0.662963\n",
      "[236]\ttraining's rmse: 0.78481\ttraining's l2: 0.615926\tvalid_1's rmse: 0.81428\tvalid_1's l2: 0.663052\n",
      "[237]\ttraining's rmse: 0.784683\ttraining's l2: 0.615727\tvalid_1's rmse: 0.814268\tvalid_1's l2: 0.663033\n",
      "[238]\ttraining's rmse: 0.784578\ttraining's l2: 0.615563\tvalid_1's rmse: 0.814217\tvalid_1's l2: 0.66295\n",
      "[239]\ttraining's rmse: 0.784466\ttraining's l2: 0.615387\tvalid_1's rmse: 0.814217\tvalid_1's l2: 0.662949\n",
      "[240]\ttraining's rmse: 0.784338\ttraining's l2: 0.615187\tvalid_1's rmse: 0.814239\tvalid_1's l2: 0.662986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[241]\ttraining's rmse: 0.78423\ttraining's l2: 0.615017\tvalid_1's rmse: 0.814209\tvalid_1's l2: 0.662937\n",
      "[242]\ttraining's rmse: 0.78413\ttraining's l2: 0.61486\tvalid_1's rmse: 0.814166\tvalid_1's l2: 0.662866\n",
      "[243]\ttraining's rmse: 0.784034\ttraining's l2: 0.614709\tvalid_1's rmse: 0.814141\tvalid_1's l2: 0.662826\n",
      "[244]\ttraining's rmse: 0.783934\ttraining's l2: 0.614553\tvalid_1's rmse: 0.814059\tvalid_1's l2: 0.662692\n",
      "[245]\ttraining's rmse: 0.783843\ttraining's l2: 0.61441\tvalid_1's rmse: 0.814019\tvalid_1's l2: 0.662627\n",
      "[246]\ttraining's rmse: 0.783724\ttraining's l2: 0.614223\tvalid_1's rmse: 0.81393\tvalid_1's l2: 0.662482\n",
      "[247]\ttraining's rmse: 0.783621\ttraining's l2: 0.614062\tvalid_1's rmse: 0.813859\tvalid_1's l2: 0.662367\n",
      "[248]\ttraining's rmse: 0.78352\ttraining's l2: 0.613904\tvalid_1's rmse: 0.81382\tvalid_1's l2: 0.662303\n",
      "[249]\ttraining's rmse: 0.78341\ttraining's l2: 0.613731\tvalid_1's rmse: 0.813847\tvalid_1's l2: 0.662347\n",
      "[250]\ttraining's rmse: 0.783316\ttraining's l2: 0.613583\tvalid_1's rmse: 0.813838\tvalid_1's l2: 0.662332\n",
      "[251]\ttraining's rmse: 0.783217\ttraining's l2: 0.613429\tvalid_1's rmse: 0.813863\tvalid_1's l2: 0.662373\n",
      "[252]\ttraining's rmse: 0.783125\ttraining's l2: 0.613284\tvalid_1's rmse: 0.813822\tvalid_1's l2: 0.662305\n",
      "[253]\ttraining's rmse: 0.783036\ttraining's l2: 0.613145\tvalid_1's rmse: 0.8138\tvalid_1's l2: 0.66227\n",
      "[254]\ttraining's rmse: 0.782918\ttraining's l2: 0.61296\tvalid_1's rmse: 0.813765\tvalid_1's l2: 0.662213\n",
      "[255]\ttraining's rmse: 0.782819\ttraining's l2: 0.612806\tvalid_1's rmse: 0.813731\tvalid_1's l2: 0.662158\n",
      "[256]\ttraining's rmse: 0.782721\ttraining's l2: 0.612652\tvalid_1's rmse: 0.813806\tvalid_1's l2: 0.662281\n",
      "[257]\ttraining's rmse: 0.782629\ttraining's l2: 0.612508\tvalid_1's rmse: 0.813836\tvalid_1's l2: 0.662329\n",
      "[258]\ttraining's rmse: 0.782528\ttraining's l2: 0.612349\tvalid_1's rmse: 0.813828\tvalid_1's l2: 0.662316\n",
      "[259]\ttraining's rmse: 0.782418\ttraining's l2: 0.612178\tvalid_1's rmse: 0.813783\tvalid_1's l2: 0.662243\n",
      "[260]\ttraining's rmse: 0.782326\ttraining's l2: 0.612034\tvalid_1's rmse: 0.813758\tvalid_1's l2: 0.662202\n",
      "[261]\ttraining's rmse: 0.782218\ttraining's l2: 0.611865\tvalid_1's rmse: 0.813742\tvalid_1's l2: 0.662177\n",
      "[262]\ttraining's rmse: 0.782123\ttraining's l2: 0.611716\tvalid_1's rmse: 0.813674\tvalid_1's l2: 0.662065\n",
      "[263]\ttraining's rmse: 0.782035\ttraining's l2: 0.611578\tvalid_1's rmse: 0.813651\tvalid_1's l2: 0.662028\n",
      "[264]\ttraining's rmse: 0.78194\ttraining's l2: 0.61143\tvalid_1's rmse: 0.813603\tvalid_1's l2: 0.66195\n",
      "[265]\ttraining's rmse: 0.78184\ttraining's l2: 0.611274\tvalid_1's rmse: 0.813598\tvalid_1's l2: 0.661942\n",
      "[266]\ttraining's rmse: 0.781738\ttraining's l2: 0.611114\tvalid_1's rmse: 0.813601\tvalid_1's l2: 0.661947\n",
      "[267]\ttraining's rmse: 0.781648\ttraining's l2: 0.610974\tvalid_1's rmse: 0.813583\tvalid_1's l2: 0.661917\n",
      "[268]\ttraining's rmse: 0.781544\ttraining's l2: 0.610812\tvalid_1's rmse: 0.813626\tvalid_1's l2: 0.661988\n",
      "[269]\ttraining's rmse: 0.781451\ttraining's l2: 0.610666\tvalid_1's rmse: 0.813689\tvalid_1's l2: 0.66209\n",
      "[270]\ttraining's rmse: 0.781363\ttraining's l2: 0.610528\tvalid_1's rmse: 0.813717\tvalid_1's l2: 0.662136\n",
      "[271]\ttraining's rmse: 0.781275\ttraining's l2: 0.610391\tvalid_1's rmse: 0.813667\tvalid_1's l2: 0.662055\n",
      "[272]\ttraining's rmse: 0.781157\ttraining's l2: 0.610206\tvalid_1's rmse: 0.813591\tvalid_1's l2: 0.66193\n",
      "[273]\ttraining's rmse: 0.781059\ttraining's l2: 0.610053\tvalid_1's rmse: 0.813553\tvalid_1's l2: 0.661868\n",
      "[274]\ttraining's rmse: 0.780979\ttraining's l2: 0.609927\tvalid_1's rmse: 0.813532\tvalid_1's l2: 0.661834\n",
      "[275]\ttraining's rmse: 0.780877\ttraining's l2: 0.609769\tvalid_1's rmse: 0.813402\tvalid_1's l2: 0.661623\n",
      "[276]\ttraining's rmse: 0.780781\ttraining's l2: 0.609618\tvalid_1's rmse: 0.813395\tvalid_1's l2: 0.661612\n",
      "[277]\ttraining's rmse: 0.780668\ttraining's l2: 0.609442\tvalid_1's rmse: 0.813369\tvalid_1's l2: 0.661569\n",
      "[278]\ttraining's rmse: 0.780543\ttraining's l2: 0.609247\tvalid_1's rmse: 0.813305\tvalid_1's l2: 0.661466\n",
      "[279]\ttraining's rmse: 0.780444\ttraining's l2: 0.609093\tvalid_1's rmse: 0.813309\tvalid_1's l2: 0.661471\n",
      "[280]\ttraining's rmse: 0.780346\ttraining's l2: 0.60894\tvalid_1's rmse: 0.813282\tvalid_1's l2: 0.661428\n",
      "[281]\ttraining's rmse: 0.780233\ttraining's l2: 0.608763\tvalid_1's rmse: 0.813177\tvalid_1's l2: 0.661257\n",
      "[282]\ttraining's rmse: 0.78015\ttraining's l2: 0.608635\tvalid_1's rmse: 0.813153\tvalid_1's l2: 0.661218\n",
      "[283]\ttraining's rmse: 0.780067\ttraining's l2: 0.608505\tvalid_1's rmse: 0.81314\tvalid_1's l2: 0.661197\n",
      "[284]\ttraining's rmse: 0.779971\ttraining's l2: 0.608355\tvalid_1's rmse: 0.813097\tvalid_1's l2: 0.661127\n",
      "[285]\ttraining's rmse: 0.779888\ttraining's l2: 0.608225\tvalid_1's rmse: 0.813103\tvalid_1's l2: 0.661137\n",
      "[286]\ttraining's rmse: 0.779784\ttraining's l2: 0.608063\tvalid_1's rmse: 0.813102\tvalid_1's l2: 0.661135\n",
      "[287]\ttraining's rmse: 0.779707\ttraining's l2: 0.607943\tvalid_1's rmse: 0.813095\tvalid_1's l2: 0.661123\n",
      "[288]\ttraining's rmse: 0.779602\ttraining's l2: 0.60778\tvalid_1's rmse: 0.813072\tvalid_1's l2: 0.661087\n",
      "[289]\ttraining's rmse: 0.779518\ttraining's l2: 0.607648\tvalid_1's rmse: 0.813069\tvalid_1's l2: 0.661081\n",
      "[290]\ttraining's rmse: 0.779418\ttraining's l2: 0.607492\tvalid_1's rmse: 0.813018\tvalid_1's l2: 0.660999\n",
      "[291]\ttraining's rmse: 0.779335\ttraining's l2: 0.607363\tvalid_1's rmse: 0.812918\tvalid_1's l2: 0.660836\n",
      "[292]\ttraining's rmse: 0.779248\ttraining's l2: 0.607228\tvalid_1's rmse: 0.812911\tvalid_1's l2: 0.660824\n",
      "[293]\ttraining's rmse: 0.779179\ttraining's l2: 0.60712\tvalid_1's rmse: 0.812891\tvalid_1's l2: 0.660793\n",
      "[294]\ttraining's rmse: 0.779091\ttraining's l2: 0.606982\tvalid_1's rmse: 0.81289\tvalid_1's l2: 0.660789\n",
      "[295]\ttraining's rmse: 0.779006\ttraining's l2: 0.60685\tvalid_1's rmse: 0.812853\tvalid_1's l2: 0.660731\n",
      "[296]\ttraining's rmse: 0.778922\ttraining's l2: 0.606719\tvalid_1's rmse: 0.812813\tvalid_1's l2: 0.660665\n",
      "[297]\ttraining's rmse: 0.778823\ttraining's l2: 0.606566\tvalid_1's rmse: 0.812858\tvalid_1's l2: 0.660738\n",
      "[298]\ttraining's rmse: 0.778746\ttraining's l2: 0.606446\tvalid_1's rmse: 0.812832\tvalid_1's l2: 0.660695\n",
      "[299]\ttraining's rmse: 0.778669\ttraining's l2: 0.606325\tvalid_1's rmse: 0.812879\tvalid_1's l2: 0.660772\n",
      "[300]\ttraining's rmse: 0.77858\ttraining's l2: 0.606187\tvalid_1's rmse: 0.81286\tvalid_1's l2: 0.660742\n",
      "[301]\ttraining's rmse: 0.778502\ttraining's l2: 0.606065\tvalid_1's rmse: 0.81284\tvalid_1's l2: 0.660709\n",
      "[302]\ttraining's rmse: 0.778418\ttraining's l2: 0.605935\tvalid_1's rmse: 0.812815\tvalid_1's l2: 0.660669\n",
      "[303]\ttraining's rmse: 0.778343\ttraining's l2: 0.605818\tvalid_1's rmse: 0.812769\tvalid_1's l2: 0.660593\n",
      "[304]\ttraining's rmse: 0.778252\ttraining's l2: 0.605675\tvalid_1's rmse: 0.812796\tvalid_1's l2: 0.660638\n",
      "[305]\ttraining's rmse: 0.77814\ttraining's l2: 0.605502\tvalid_1's rmse: 0.812723\tvalid_1's l2: 0.660518\n",
      "[306]\ttraining's rmse: 0.778058\ttraining's l2: 0.605374\tvalid_1's rmse: 0.812717\tvalid_1's l2: 0.66051\n",
      "[307]\ttraining's rmse: 0.777978\ttraining's l2: 0.60525\tvalid_1's rmse: 0.812709\tvalid_1's l2: 0.660496\n",
      "[308]\ttraining's rmse: 0.7779\ttraining's l2: 0.605129\tvalid_1's rmse: 0.812711\tvalid_1's l2: 0.660499\n",
      "[309]\ttraining's rmse: 0.77782\ttraining's l2: 0.605004\tvalid_1's rmse: 0.812716\tvalid_1's l2: 0.660507\n",
      "[310]\ttraining's rmse: 0.777744\ttraining's l2: 0.604885\tvalid_1's rmse: 0.812643\tvalid_1's l2: 0.660389\n",
      "[311]\ttraining's rmse: 0.777662\ttraining's l2: 0.604759\tvalid_1's rmse: 0.812608\tvalid_1's l2: 0.660332\n",
      "[312]\ttraining's rmse: 0.777581\ttraining's l2: 0.604632\tvalid_1's rmse: 0.812614\tvalid_1's l2: 0.660342\n",
      "[313]\ttraining's rmse: 0.777463\ttraining's l2: 0.604449\tvalid_1's rmse: 0.812598\tvalid_1's l2: 0.660316\n",
      "[314]\ttraining's rmse: 0.77739\ttraining's l2: 0.604336\tvalid_1's rmse: 0.812564\tvalid_1's l2: 0.66026\n",
      "[315]\ttraining's rmse: 0.777292\ttraining's l2: 0.604182\tvalid_1's rmse: 0.812497\tvalid_1's l2: 0.660151\n",
      "[316]\ttraining's rmse: 0.777193\ttraining's l2: 0.604028\tvalid_1's rmse: 0.812428\tvalid_1's l2: 0.660039\n",
      "[317]\ttraining's rmse: 0.777088\ttraining's l2: 0.603866\tvalid_1's rmse: 0.812305\tvalid_1's l2: 0.65984\n",
      "[318]\ttraining's rmse: 0.777005\ttraining's l2: 0.603736\tvalid_1's rmse: 0.812261\tvalid_1's l2: 0.659768\n",
      "[319]\ttraining's rmse: 0.77694\ttraining's l2: 0.603635\tvalid_1's rmse: 0.812246\tvalid_1's l2: 0.659743\n",
      "[320]\ttraining's rmse: 0.776865\ttraining's l2: 0.603519\tvalid_1's rmse: 0.81221\tvalid_1's l2: 0.659686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[321]\ttraining's rmse: 0.776785\ttraining's l2: 0.603396\tvalid_1's rmse: 0.812206\tvalid_1's l2: 0.659678\n",
      "[322]\ttraining's rmse: 0.776714\ttraining's l2: 0.603285\tvalid_1's rmse: 0.81217\tvalid_1's l2: 0.659621\n",
      "[323]\ttraining's rmse: 0.776634\ttraining's l2: 0.60316\tvalid_1's rmse: 0.812168\tvalid_1's l2: 0.659616\n",
      "[324]\ttraining's rmse: 0.776527\ttraining's l2: 0.602994\tvalid_1's rmse: 0.812216\tvalid_1's l2: 0.659694\n",
      "[325]\ttraining's rmse: 0.776453\ttraining's l2: 0.602879\tvalid_1's rmse: 0.812235\tvalid_1's l2: 0.659725\n",
      "[326]\ttraining's rmse: 0.77636\ttraining's l2: 0.602735\tvalid_1's rmse: 0.812096\tvalid_1's l2: 0.6595\n",
      "[327]\ttraining's rmse: 0.776281\ttraining's l2: 0.602612\tvalid_1's rmse: 0.812066\tvalid_1's l2: 0.659451\n",
      "[328]\ttraining's rmse: 0.776187\ttraining's l2: 0.602466\tvalid_1's rmse: 0.812019\tvalid_1's l2: 0.659375\n",
      "[329]\ttraining's rmse: 0.776109\ttraining's l2: 0.602345\tvalid_1's rmse: 0.811976\tvalid_1's l2: 0.659305\n",
      "[330]\ttraining's rmse: 0.776027\ttraining's l2: 0.602218\tvalid_1's rmse: 0.811977\tvalid_1's l2: 0.659307\n",
      "[331]\ttraining's rmse: 0.775942\ttraining's l2: 0.602086\tvalid_1's rmse: 0.811965\tvalid_1's l2: 0.659288\n",
      "[332]\ttraining's rmse: 0.775869\ttraining's l2: 0.601973\tvalid_1's rmse: 0.811958\tvalid_1's l2: 0.659276\n",
      "[333]\ttraining's rmse: 0.775792\ttraining's l2: 0.601853\tvalid_1's rmse: 0.811982\tvalid_1's l2: 0.659314\n",
      "[334]\ttraining's rmse: 0.775718\ttraining's l2: 0.601739\tvalid_1's rmse: 0.811984\tvalid_1's l2: 0.659318\n",
      "[335]\ttraining's rmse: 0.775627\ttraining's l2: 0.601597\tvalid_1's rmse: 0.811778\tvalid_1's l2: 0.658983\n",
      "[336]\ttraining's rmse: 0.775551\ttraining's l2: 0.601479\tvalid_1's rmse: 0.811821\tvalid_1's l2: 0.659054\n",
      "[337]\ttraining's rmse: 0.775462\ttraining's l2: 0.601341\tvalid_1's rmse: 0.811879\tvalid_1's l2: 0.659148\n",
      "[338]\ttraining's rmse: 0.775371\ttraining's l2: 0.601201\tvalid_1's rmse: 0.811876\tvalid_1's l2: 0.659143\n",
      "[339]\ttraining's rmse: 0.775302\ttraining's l2: 0.601093\tvalid_1's rmse: 0.81186\tvalid_1's l2: 0.659116\n",
      "[340]\ttraining's rmse: 0.77523\ttraining's l2: 0.600981\tvalid_1's rmse: 0.81186\tvalid_1's l2: 0.659117\n",
      "[341]\ttraining's rmse: 0.775152\ttraining's l2: 0.60086\tvalid_1's rmse: 0.811836\tvalid_1's l2: 0.659078\n",
      "[342]\ttraining's rmse: 0.775078\ttraining's l2: 0.600746\tvalid_1's rmse: 0.811819\tvalid_1's l2: 0.65905\n",
      "[343]\ttraining's rmse: 0.775004\ttraining's l2: 0.600631\tvalid_1's rmse: 0.811793\tvalid_1's l2: 0.659007\n",
      "[344]\ttraining's rmse: 0.774925\ttraining's l2: 0.600509\tvalid_1's rmse: 0.811803\tvalid_1's l2: 0.659024\n",
      "[345]\ttraining's rmse: 0.774842\ttraining's l2: 0.60038\tvalid_1's rmse: 0.811713\tvalid_1's l2: 0.658878\n",
      "[346]\ttraining's rmse: 0.774771\ttraining's l2: 0.600271\tvalid_1's rmse: 0.811673\tvalid_1's l2: 0.658813\n",
      "[347]\ttraining's rmse: 0.774689\ttraining's l2: 0.600144\tvalid_1's rmse: 0.811597\tvalid_1's l2: 0.658689\n",
      "[348]\ttraining's rmse: 0.774618\ttraining's l2: 0.600033\tvalid_1's rmse: 0.811576\tvalid_1's l2: 0.658655\n",
      "[349]\ttraining's rmse: 0.774545\ttraining's l2: 0.59992\tvalid_1's rmse: 0.811564\tvalid_1's l2: 0.658636\n",
      "[350]\ttraining's rmse: 0.774478\ttraining's l2: 0.599815\tvalid_1's rmse: 0.811571\tvalid_1's l2: 0.658647\n",
      "[351]\ttraining's rmse: 0.774416\ttraining's l2: 0.59972\tvalid_1's rmse: 0.811542\tvalid_1's l2: 0.6586\n",
      "[352]\ttraining's rmse: 0.774352\ttraining's l2: 0.599621\tvalid_1's rmse: 0.811509\tvalid_1's l2: 0.658548\n",
      "[353]\ttraining's rmse: 0.774265\ttraining's l2: 0.599486\tvalid_1's rmse: 0.811506\tvalid_1's l2: 0.658541\n",
      "[354]\ttraining's rmse: 0.774189\ttraining's l2: 0.599369\tvalid_1's rmse: 0.811489\tvalid_1's l2: 0.658514\n",
      "[355]\ttraining's rmse: 0.77412\ttraining's l2: 0.599261\tvalid_1's rmse: 0.811412\tvalid_1's l2: 0.658389\n",
      "[356]\ttraining's rmse: 0.774024\ttraining's l2: 0.599113\tvalid_1's rmse: 0.8114\tvalid_1's l2: 0.658369\n",
      "[357]\ttraining's rmse: 0.77396\ttraining's l2: 0.599014\tvalid_1's rmse: 0.811338\tvalid_1's l2: 0.65827\n",
      "[358]\ttraining's rmse: 0.77389\ttraining's l2: 0.598905\tvalid_1's rmse: 0.811339\tvalid_1's l2: 0.658271\n",
      "[359]\ttraining's rmse: 0.773827\ttraining's l2: 0.598808\tvalid_1's rmse: 0.811327\tvalid_1's l2: 0.658251\n",
      "[360]\ttraining's rmse: 0.773745\ttraining's l2: 0.598681\tvalid_1's rmse: 0.811273\tvalid_1's l2: 0.658163\n",
      "[361]\ttraining's rmse: 0.773684\ttraining's l2: 0.598586\tvalid_1's rmse: 0.811256\tvalid_1's l2: 0.658137\n",
      "[362]\ttraining's rmse: 0.773622\ttraining's l2: 0.598491\tvalid_1's rmse: 0.811228\tvalid_1's l2: 0.65809\n",
      "[363]\ttraining's rmse: 0.773558\ttraining's l2: 0.598392\tvalid_1's rmse: 0.811199\tvalid_1's l2: 0.658043\n",
      "[364]\ttraining's rmse: 0.773471\ttraining's l2: 0.598258\tvalid_1's rmse: 0.811136\tvalid_1's l2: 0.657941\n",
      "[365]\ttraining's rmse: 0.773397\ttraining's l2: 0.598142\tvalid_1's rmse: 0.811121\tvalid_1's l2: 0.657917\n",
      "[366]\ttraining's rmse: 0.773328\ttraining's l2: 0.598036\tvalid_1's rmse: 0.81111\tvalid_1's l2: 0.6579\n",
      "[367]\ttraining's rmse: 0.773259\ttraining's l2: 0.597929\tvalid_1's rmse: 0.811122\tvalid_1's l2: 0.657919\n",
      "[368]\ttraining's rmse: 0.773194\ttraining's l2: 0.59783\tvalid_1's rmse: 0.811134\tvalid_1's l2: 0.657938\n",
      "[369]\ttraining's rmse: 0.773123\ttraining's l2: 0.59772\tvalid_1's rmse: 0.811094\tvalid_1's l2: 0.657874\n",
      "[370]\ttraining's rmse: 0.773052\ttraining's l2: 0.597609\tvalid_1's rmse: 0.811074\tvalid_1's l2: 0.657841\n",
      "[371]\ttraining's rmse: 0.772986\ttraining's l2: 0.597508\tvalid_1's rmse: 0.811062\tvalid_1's l2: 0.657822\n",
      "[372]\ttraining's rmse: 0.772928\ttraining's l2: 0.597417\tvalid_1's rmse: 0.811063\tvalid_1's l2: 0.657823\n",
      "[373]\ttraining's rmse: 0.772866\ttraining's l2: 0.597322\tvalid_1's rmse: 0.811052\tvalid_1's l2: 0.657805\n",
      "[374]\ttraining's rmse: 0.772791\ttraining's l2: 0.597206\tvalid_1's rmse: 0.811031\tvalid_1's l2: 0.657771\n",
      "[375]\ttraining's rmse: 0.772714\ttraining's l2: 0.597087\tvalid_1's rmse: 0.81109\tvalid_1's l2: 0.657868\n",
      "[376]\ttraining's rmse: 0.772636\ttraining's l2: 0.596966\tvalid_1's rmse: 0.811081\tvalid_1's l2: 0.657853\n",
      "[377]\ttraining's rmse: 0.772565\ttraining's l2: 0.596857\tvalid_1's rmse: 0.811105\tvalid_1's l2: 0.657891\n",
      "[378]\ttraining's rmse: 0.772486\ttraining's l2: 0.596734\tvalid_1's rmse: 0.811149\tvalid_1's l2: 0.657962\n",
      "[379]\ttraining's rmse: 0.772419\ttraining's l2: 0.596631\tvalid_1's rmse: 0.811153\tvalid_1's l2: 0.657968\n",
      "[380]\ttraining's rmse: 0.772352\ttraining's l2: 0.596527\tvalid_1's rmse: 0.811117\tvalid_1's l2: 0.65791\n",
      "[381]\ttraining's rmse: 0.772276\ttraining's l2: 0.596411\tvalid_1's rmse: 0.811112\tvalid_1's l2: 0.657902\n",
      "[382]\ttraining's rmse: 0.772209\ttraining's l2: 0.596306\tvalid_1's rmse: 0.811078\tvalid_1's l2: 0.657847\n",
      "[383]\ttraining's rmse: 0.772149\ttraining's l2: 0.596215\tvalid_1's rmse: 0.81105\tvalid_1's l2: 0.657802\n",
      "[384]\ttraining's rmse: 0.77207\ttraining's l2: 0.596092\tvalid_1's rmse: 0.810983\tvalid_1's l2: 0.657694\n",
      "[385]\ttraining's rmse: 0.771998\ttraining's l2: 0.595981\tvalid_1's rmse: 0.810971\tvalid_1's l2: 0.657673\n",
      "[386]\ttraining's rmse: 0.771931\ttraining's l2: 0.595877\tvalid_1's rmse: 0.810984\tvalid_1's l2: 0.657696\n",
      "[387]\ttraining's rmse: 0.771867\ttraining's l2: 0.595779\tvalid_1's rmse: 0.810987\tvalid_1's l2: 0.6577\n",
      "[388]\ttraining's rmse: 0.771795\ttraining's l2: 0.595668\tvalid_1's rmse: 0.811004\tvalid_1's l2: 0.657727\n",
      "[389]\ttraining's rmse: 0.771722\ttraining's l2: 0.595554\tvalid_1's rmse: 0.811007\tvalid_1's l2: 0.657732\n",
      "[390]\ttraining's rmse: 0.771615\ttraining's l2: 0.59539\tvalid_1's rmse: 0.810828\tvalid_1's l2: 0.657443\n",
      "[391]\ttraining's rmse: 0.771544\ttraining's l2: 0.595281\tvalid_1's rmse: 0.810842\tvalid_1's l2: 0.657466\n",
      "[392]\ttraining's rmse: 0.771468\ttraining's l2: 0.595163\tvalid_1's rmse: 0.810837\tvalid_1's l2: 0.657456\n",
      "[393]\ttraining's rmse: 0.771392\ttraining's l2: 0.595046\tvalid_1's rmse: 0.810828\tvalid_1's l2: 0.657443\n",
      "[394]\ttraining's rmse: 0.771321\ttraining's l2: 0.594935\tvalid_1's rmse: 0.810844\tvalid_1's l2: 0.657467\n",
      "[395]\ttraining's rmse: 0.771259\ttraining's l2: 0.594841\tvalid_1's rmse: 0.810853\tvalid_1's l2: 0.657483\n",
      "[396]\ttraining's rmse: 0.771195\ttraining's l2: 0.594742\tvalid_1's rmse: 0.810835\tvalid_1's l2: 0.657454\n",
      "[397]\ttraining's rmse: 0.771122\ttraining's l2: 0.594629\tvalid_1's rmse: 0.810892\tvalid_1's l2: 0.657546\n",
      "[398]\ttraining's rmse: 0.771037\ttraining's l2: 0.594498\tvalid_1's rmse: 0.810906\tvalid_1's l2: 0.657569\n",
      "[399]\ttraining's rmse: 0.770983\ttraining's l2: 0.594415\tvalid_1's rmse: 0.810894\tvalid_1's l2: 0.657549\n",
      "[400]\ttraining's rmse: 0.770917\ttraining's l2: 0.594313\tvalid_1's rmse: 0.81088\tvalid_1's l2: 0.657526\n",
      "Early stopping, best iteration is:\n",
      "[390]\ttraining's rmse: 0.771615\ttraining's l2: 0.59539\tvalid_1's rmse: 0.810828\tvalid_1's l2: 0.657443\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(bagging_fraction=0.2, boosting_type='gbdt', class_weight=None,\n",
       "              colsample_bytree=1.0, feature_fraction=0.8,\n",
       "              importance_type='split', learning_rate=0.05, max_depth=-1,\n",
       "              min_child_samples=20, min_child_weight=0.001,\n",
       "              min_data_in_leaf=300, min_split_gain=0.0, n_estimators=1000,\n",
       "              n_jobs=-1, num_leaves=128, objective=None, random_state=None,\n",
       "              reg_alpha=0.0, reg_lambda=0.0, seed=42, silent=True,\n",
       "              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try with categorical features named\n",
    "\n",
    "cat_features = ['cat_subtype_id', 'cat_type_id', 'city_id', 'item_category_id', 'item_id', 'item_subtype_id', 'item_type_id', 'shop_id', 'shop_type_id']\n",
    "    \n",
    "lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "lgb_model.fit(\n",
    "    X_train, \n",
    "    Y_train,\n",
    "    categorical_feature=cat_features,\n",
    "    eval_metric=\"rmse\", \n",
    "    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
    "    verbose=True, \n",
    "    early_stopping_rounds = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.2,\n",
       " 'feature_fraction': 0.8,\n",
       " 'learning_rate': 0.05,\n",
       " 'max_depth': -1,\n",
       " 'min_data_in_leaf': 300,\n",
       " 'num_leaves': 128,\n",
       " 'seed': 42,\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_params['n_estimators'] = 200\n",
    "lgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save untrained model to file \n",
    "\n",
    "Pkl_Filename = \"LBG_Params.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(lgb_params, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:  8.6min remaining: 43.2min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed: 46.5min remaining: 15.5min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed: 64.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8773244574839543\n",
      "{'bootstrap': True, 'max_features': 'sqrt'}\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor()\n",
    "param_grid={\n",
    "    'bootstrap':[True, False],\n",
    "    'max_features': ['auto', 'sqrt'], \n",
    "    #'max_depth': [None, 5, 10, 20, 50, 100],\n",
    "    #'min_samples_leaf': [1, 2, 4],\n",
    "    #'min_samples_split': [2, 5, 10],\n",
    "    #'n_estimators': [100,200,500,1000],\n",
    "}\n",
    "\n",
    "model, pred = algorithm_pipeline(X_train, X_valid, Y_train, Y_valid, model, \n",
    "                                 param_grid, cv=3)\n",
    "\n",
    "print(np.sqrt(-model.best_score_))\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done  51 out of  54 | elapsed: 28.4min remaining:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed: 29.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8395895077874228\n",
      "{'bootstrap': True, 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 4}\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor()\n",
    "param_grid={\n",
    "    'bootstrap':[True],\n",
    "    'max_features': ['sqrt'], \n",
    "    'max_depth': [None, 5, 10, 20, 50, 100],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    #'min_samples_split': [2, 5, 10],\n",
    "    #'n_estimators': [100,200,500,1000],\n",
    "}\n",
    "\n",
    "model, pred = algorithm_pipeline(X_train, X_valid, Y_train, Y_valid, model, \n",
    "                                 param_grid, cv=3)\n",
    "\n",
    "print(np.sqrt(-model.best_score_))\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed: 87.7min\n",
      "[Parallel(n_jobs=-1)]: Done  42 out of  48 | elapsed: 316.0min remaining: 45.1min\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed: 350.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8239613504079909\n",
      "{'bootstrap': True, 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor()\n",
    "param_grid={\n",
    "    'bootstrap':[True],\n",
    "    'max_features': ['sqrt'], \n",
    "    'max_depth': [20],\n",
    "    'min_samples_leaf': [4],\n",
    "    'min_samples_split': [2, 5, 8, 10],\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "}\n",
    "\n",
    "model, pred = algorithm_pipeline(X_train, X_valid, Y_train, Y_valid, model, \n",
    "                                 param_grid, cv=3)\n",
    "\n",
    "print(np.sqrt(-model.best_score_))\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 20,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 4,\n",
       " 'min_samples_split': 5,\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_params = model.best_params_\n",
    "rf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save untrained model to file \n",
    "\n",
    "Pkl_Filename = \"RF_Params.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(rf_params, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(neurons=50, activation='relu',dropout_rate=0.0, init='uniform',optimizer='adam',activation2='sigmoid'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=74, kernel_initializer=init, activation=activation))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, kernel_initializer=init, activation=activation2))\n",
    "    # Compile model\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "dropout_rate = [0.0, 0.2, 0.4, 0.6, 0.8]\n",
    "#weight_constraints = [1, 2, 3, 4, 5]\n",
    "neurons = [20, 50, 100, 200]\n",
    "init = ['uniform','lecun_uniform','normal','zero','glorot_normal','glorot_uniform','he_uniform','he_normal']\n",
    "optimizer = ['SGD','RMSprop','Adagrad','Adadelta','Adam','Adamax','Nadam']\n",
    "activation = ['relu','tanh','sigmoid','hard_sigmoid','linear','softmax']\n",
    "activation2 = ['relu','tanh','sigmoid','hard_sigmoid','linear','softmax']\n",
    "epochs = [5,10,30,50,100,150,200]\n",
    "batch_size=[1000, 5000, 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = X_train.values\n",
    "train_y = Y_train.ravel()\n",
    "valid_x = X_valid.values\n",
    "valid_y = Y_valid.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "param_grid = dict(epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "model = KerasRegressor(build_fn=create_model, verbose=1)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(train_x, train_y, validation_data=(valid_x, valid_y))\n",
    "\n",
    "print(np.sqrt(-grid_result.best_score_))\n",
    "print(grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.4525 - mse: 1.4525 - val_loss: 1.1312 - val_mse: 1.1312\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 12s 3us/step - loss: 1.3584 - mse: 1.3584 - val_loss: 1.0720 - val_mse: 1.0720\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 1.3165 - mse: 1.3165 - val_loss: 1.0610 - val_mse: 1.0610\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 1.3057 - mse: 1.3057 - val_loss: 1.0564 - val_mse: 1.0564\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 1.3000 - mse: 1.3000 - val_loss: 1.0525 - val_mse: 1.0525\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.3055 - mse: 1.3055 - val_loss: 1.1259 - val_mse: 1.1259\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 1.2126 - mse: 1.2126 - val_loss: 1.0712 - val_mse: 1.0712\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 12s 3us/step - loss: 1.1758 - mse: 1.1758 - val_loss: 1.0606 - val_mse: 1.0606\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 1.1663 - mse: 1.1663 - val_loss: 1.0556 - val_mse: 1.0556\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 1.1615 - mse: 1.1615 - val_loss: 1.0520 - val_mse: 1.0520\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.5667 - mse: 1.5667 - val_loss: 1.1352 - val_mse: 1.1352\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.4599 - mse: 1.4599 - val_loss: 1.0722 - val_mse: 1.0722\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 12s 4us/step - loss: 1.4120 - mse: 1.4120 - val_loss: 1.0614 - val_mse: 1.0614\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 11s 3us/step - loss: 1.4008 - mse: 1.4008 - val_loss: 1.0561 - val_mse: 1.0561\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 11s 3us/step - loss: 1.3954 - mse: 1.3954 - val_loss: 1.0536 - val_mse: 1.0536\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2780 - mse: 1.2780 - val_loss: 1.0280 - val_mse: 1.0280\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 12s 3us/step - loss: 1.2604 - mse: 1.2604 - val_loss: 1.0294 - val_mse: 1.0294\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 12s 3us/step - loss: 1.2569 - mse: 1.2569 - val_loss: 1.0302 - val_mse: 1.0302\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 12s 3us/step - loss: 1.2544 - mse: 1.2544 - val_loss: 1.0280 - val_mse: 1.0280\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 12s 3us/step - loss: 1.2527 - mse: 1.2527 - val_loss: 1.0276 - val_mse: 1.0276\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.1480 - mse: 1.1480 - val_loss: 1.0333 - val_mse: 1.0333\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 1.1353 - mse: 1.1353 - val_loss: 1.0311 - val_mse: 1.0311\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.1327 - mse: 1.1327 - val_loss: 1.0333 - val_mse: 1.0333\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.1311 - mse: 1.1311 - val_loss: 1.0296 - val_mse: 1.0296\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.1293 - mse: 1.1293 - val_loss: 1.0274 - val_mse: 1.0274\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3797 - mse: 1.3797 - val_loss: 1.0358 - val_mse: 1.0358\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 12s 3us/step - loss: 1.3682 - mse: 1.3682 - val_loss: 1.0417 - val_mse: 1.0417\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 12s 4us/step - loss: 1.3675 - mse: 1.3675 - val_loss: 1.0404 - val_mse: 1.0404\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 12s 4us/step - loss: 1.3672 - mse: 1.3672 - val_loss: 1.0409 - val_mse: 1.0409\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 12s 3us/step - loss: 1.3670 - mse: 1.3670 - val_loss: 1.0384 - val_mse: 1.0384\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2862 - mse: 1.2862 - val_loss: 1.0387 - val_mse: 1.0387\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 1.2718 - mse: 1.2718 - val_loss: 1.0327 - val_mse: 1.0327\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 12s 3us/step - loss: 1.2669 - mse: 1.2669 - val_loss: 1.0286 - val_mse: 1.0286\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.2644 - mse: 1.2644 - val_loss: 1.0279 - val_mse: 1.0279\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 1.2629 - mse: 1.2629 - val_loss: 1.0282 - val_mse: 1.0282\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1500 - mse: 1.1500 - val_loss: 1.0327 - val_mse: 1.0327\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 1.1357 - mse: 1.1357 - val_loss: 1.0287 - val_mse: 1.0287\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 1.1324 - mse: 1.1324 - val_loss: 1.0281 - val_mse: 1.0281\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 12s 3us/step - loss: 1.1305 - mse: 1.1305 - val_loss: 1.0277 - val_mse: 1.0277\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 1.1294 - mse: 1.1294 - val_loss: 1.0272 - val_mse: 1.0272\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 12s 4us/step - loss: 1.3846 - mse: 1.3846 - val_loss: 1.0362 - val_mse: 1.0362\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 11s 3us/step - loss: 1.3700 - mse: 1.3700 - val_loss: 1.0323 - val_mse: 1.0323\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 12s 4us/step - loss: 1.3644 - mse: 1.3644 - val_loss: 1.0294 - val_mse: 1.0294\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 11s 3us/step - loss: 1.3608 - mse: 1.3608 - val_loss: 1.0282 - val_mse: 1.0282\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 12s 3us/step - loss: 1.3588 - mse: 1.3588 - val_loss: 1.0286 - val_mse: 1.0286\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2849 - mse: 1.2849 - val_loss: 1.0377 - val_mse: 1.0377\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2634 - mse: 1.2634 - val_loss: 1.0323 - val_mse: 1.0323\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.2576 - mse: 1.2576 - val_loss: 1.0239 - val_mse: 1.0239\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.2542 - mse: 1.2542 - val_loss: 1.0228 - val_mse: 1.0228\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2516 - mse: 1.2516 - val_loss: 1.0251 - val_mse: 1.0251\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.1505 - mse: 1.1505 - val_loss: 1.0349 - val_mse: 1.0349\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1319 - mse: 1.1319 - val_loss: 1.0255 - val_mse: 1.0255\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1263 - mse: 1.1263 - val_loss: 1.0307 - val_mse: 1.0307\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.1228 - mse: 1.1228 - val_loss: 1.0310 - val_mse: 1.0310\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1205 - mse: 1.1205 - val_loss: 1.0258 - val_mse: 1.0258\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3813 - mse: 1.3813 - val_loss: 1.0286 - val_mse: 1.0286\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3606 - mse: 1.3606 - val_loss: 1.0381 - val_mse: 1.0381\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3532 - mse: 1.3532 - val_loss: 1.0256 - val_mse: 1.0256\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3488 - mse: 1.3488 - val_loss: 1.0269 - val_mse: 1.0269\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3456 - mse: 1.3456 - val_loss: 1.0291 - val_mse: 1.0291\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.2790 - mse: 1.2790 - val_loss: 1.0288 - val_mse: 1.0288\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.2581 - mse: 1.2581 - val_loss: 1.0264 - val_mse: 1.0264\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.2542 - mse: 1.2542 - val_loss: 1.0289 - val_mse: 1.0289\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.2519 - mse: 1.2519 - val_loss: 1.0241 - val_mse: 1.0241\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.2504 - mse: 1.2504 - val_loss: 1.0252 - val_mse: 1.0252\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1452 - mse: 1.1452 - val_loss: 1.0285 - val_mse: 1.0285\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.1272 - mse: 1.1272 - val_loss: 1.0261 - val_mse: 1.0261\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.1240 - mse: 1.1240 - val_loss: 1.0266 - val_mse: 1.0266\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1219 - mse: 1.1219 - val_loss: 1.0266 - val_mse: 1.0266\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1201 - mse: 1.1201 - val_loss: 1.0251 - val_mse: 1.0251\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3756 - mse: 1.3756 - val_loss: 1.0277 - val_mse: 1.0277\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 12s 4us/step - loss: 1.3538 - mse: 1.3538 - val_loss: 1.0289 - val_mse: 1.0289\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3490 - mse: 1.3490 - val_loss: 1.0277 - val_mse: 1.0277\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3455 - mse: 1.3455 - val_loss: 1.0276 - val_mse: 1.0276\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 12s 4us/step - loss: 1.3432 - mse: 1.3432 - val_loss: 1.0281 - val_mse: 1.0281\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2871 - mse: 1.2871 - val_loss: 1.0372 - val_mse: 1.0372\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 12s 3us/step - loss: 1.2677 - mse: 1.2677 - val_loss: 1.0300 - val_mse: 1.0300\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 12s 3us/step - loss: 1.2629 - mse: 1.2629 - val_loss: 1.0272 - val_mse: 1.0272\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 1.2600 - mse: 1.2600 - val_loss: 1.0267 - val_mse: 1.0267\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.2577 - mse: 1.2577 - val_loss: 1.0279 - val_mse: 1.0279\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1529 - mse: 1.1529 - val_loss: 1.0342 - val_mse: 1.0342\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.1358 - mse: 1.1358 - val_loss: 1.0307 - val_mse: 1.0307\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.1323 - mse: 1.1323 - val_loss: 1.0313 - val_mse: 1.0313\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.1303 - mse: 1.1303 - val_loss: 1.0272 - val_mse: 1.0272\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.1288 - mse: 1.1288 - val_loss: 1.0252 - val_mse: 1.0252\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3840 - mse: 1.3840 - val_loss: 1.0326 - val_mse: 1.0326\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3611 - mse: 1.3611 - val_loss: 1.0293 - val_mse: 1.0293\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3559 - mse: 1.3559 - val_loss: 1.0299 - val_mse: 1.0299\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 12s 4us/step - loss: 1.3526 - mse: 1.3526 - val_loss: 1.0273 - val_mse: 1.0273\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3502 - mse: 1.3502 - val_loss: 1.0264 - val_mse: 1.0264\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.2794 - mse: 1.2794 - val_loss: 1.0356 - val_mse: 1.0356\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2715 - mse: 1.2715 - val_loss: 1.0354 - val_mse: 1.0354\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2710 - mse: 1.2710 - val_loss: 1.0343 - val_mse: 1.0343\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 15s 4us/step - loss: 1.2708 - mse: 1.2708 - val_loss: 1.0351 - val_mse: 1.0351\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 1.2679 - mse: 1.2679 - val_loss: 1.0294 - val_mse: 1.0294\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 15s 4us/step - loss: 1.1337 - mse: 1.1337 - val_loss: 1.0270 - val_mse: 1.0270\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.1198 - mse: 1.1198 - val_loss: 1.0242 - val_mse: 1.0242\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.1159 - mse: 1.1159 - val_loss: 1.0223 - val_mse: 1.0223\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1128 - mse: 1.1128 - val_loss: 1.0197 - val_mse: 1.0197\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.1108 - mse: 1.1108 - val_loss: 1.0187 - val_mse: 1.0187\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3676 - mse: 1.3676 - val_loss: 1.0315 - val_mse: 1.0315\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 12s 4us/step - loss: 1.3505 - mse: 1.3505 - val_loss: 1.0282 - val_mse: 1.0282\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 12s 4us/step - loss: 1.3445 - mse: 1.3445 - val_loss: 1.0317 - val_mse: 1.0317\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 12s 4us/step - loss: 1.3416 - mse: 1.3416 - val_loss: 1.0329 - val_mse: 1.0329\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3397 - mse: 1.3397 - val_loss: 1.0278 - val_mse: 1.0278\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 5029976 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "5029976/5029976 [==============================] - 27s 5us/step - loss: 1.2626 - mse: 1.2626 - val_loss: 1.0285 - val_mse: 1.0285\n",
      "Epoch 2/5\n",
      "5029976/5029976 [==============================] - 25s 5us/step - loss: 1.2468 - mse: 1.2468 - val_loss: 1.0263 - val_mse: 1.0263\n",
      "Epoch 3/5\n",
      "5029976/5029976 [==============================] - 27s 5us/step - loss: 1.2430 - mse: 1.2430 - val_loss: 1.0264 - val_mse: 1.0264\n",
      "Epoch 4/5\n",
      "5029976/5029976 [==============================] - 26s 5us/step - loss: 1.2390 - mse: 1.2390 - val_loss: 1.0246 - val_mse: 1.0246\n",
      "Epoch 5/5\n",
      "5029976/5029976 [==============================] - 27s 5us/step - loss: 1.2359 - mse: 1.2359 - val_loss: 1.0226 - val_mse: 1.0226\n",
      "1.114456920082972\n",
      "{'batch_size': 1000, 'epochs': 5, 'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(optimizer=optimizer, epochs=[5], batch_size=[1000])\n",
    "\n",
    "model = KerasRegressor(build_fn=create_model, verbose=1)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(train_x, train_y, validation_data=(valid_x, valid_y))\n",
    "\n",
    "print(np.sqrt(-grid_result.best_score_))\n",
    "print(grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2827 - mse: 1.2827 - val_loss: 1.0333 - val_mse: 1.0333\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.2649 - mse: 1.2649 - val_loss: 1.0299 - val_mse: 1.0299\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.2632 - mse: 1.2632 - val_loss: 1.0315 - val_mse: 1.0315\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.2623 - mse: 1.2623 - val_loss: 1.0280 - val_mse: 1.0280\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.2616 - mse: 1.2616 - val_loss: 1.0277 - val_mse: 1.0277\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.1441 - mse: 1.1441 - val_loss: 1.0278 - val_mse: 1.0278\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.1250 - mse: 1.1250 - val_loss: 1.0243 - val_mse: 1.0243\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.1213 - mse: 1.1213 - val_loss: 1.0232 - val_mse: 1.0232\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.1190 - mse: 1.1190 - val_loss: 1.0234 - val_mse: 1.0234\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.1173 - mse: 1.1173 - val_loss: 1.0234 - val_mse: 1.0234\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 14s 4us/step - loss: 1.3769 - mse: 1.3769 - val_loss: 1.0299 - val_mse: 1.0299\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3555 - mse: 1.3555 - val_loss: 1.0281 - val_mse: 1.0281\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3518 - mse: 1.3519 - val_loss: 1.0282 - val_mse: 1.0282\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3497 - mse: 1.3497 - val_loss: 1.0277 - val_mse: 1.0277\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 12s 4us/step - loss: 1.3479 - mse: 1.3479 - val_loss: 1.0286 - val_mse: 1.0286\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.2746 - mse: 1.2746 - val_loss: 1.0278 - val_mse: 1.0278\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2538 - mse: 1.2538 - val_loss: 1.0257 - val_mse: 1.0257\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2490 - mse: 1.2490 - val_loss: 1.0256 - val_mse: 1.0256\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2457 - mse: 1.2457 - val_loss: 1.0196 - val_mse: 1.0196\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2430 - mse: 1.2430 - val_loss: 1.0165 - val_mse: 1.0165\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.1423 - mse: 1.1423 - val_loss: 1.0268 - val_mse: 1.0268\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1243 - mse: 1.1243 - val_loss: 1.0242 - val_mse: 1.0242\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1198 - mse: 1.1198 - val_loss: 1.0231 - val_mse: 1.0231\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1170 - mse: 1.1170 - val_loss: 1.0200 - val_mse: 1.0200\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1146 - mse: 1.1146 - val_loss: 1.0192 - val_mse: 1.0192\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 14s 4us/step - loss: 1.3710 - mse: 1.3710 - val_loss: 1.0261 - val_mse: 1.0261\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 14s 4us/step - loss: 1.3492 - mse: 1.3492 - val_loss: 1.0249 - val_mse: 1.0249\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3435 - mse: 1.3435 - val_loss: 1.0258 - val_mse: 1.0258\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3400 - mse: 1.3400 - val_loss: 1.0252 - val_mse: 1.0252\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 14s 4us/step - loss: 1.3378 - mse: 1.3378 - val_loss: 1.0234 - val_mse: 1.0234\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.2774 - mse: 1.2774 - val_loss: 1.0264 - val_mse: 1.0264\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2560 - mse: 1.2560 - val_loss: 1.0238 - val_mse: 1.0238\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2521 - mse: 1.2521 - val_loss: 1.0240 - val_mse: 1.0240\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2494 - mse: 1.2494 - val_loss: 1.0234 - val_mse: 1.0234\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2472 - mse: 1.2472 - val_loss: 1.0231 - val_mse: 1.0231\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.1438 - mse: 1.1438 - val_loss: 1.0289 - val_mse: 1.0289\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1256 - mse: 1.1256 - val_loss: 1.0259 - val_mse: 1.0259\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1221 - mse: 1.1221 - val_loss: 1.0260 - val_mse: 1.0260\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1196 - mse: 1.1196 - val_loss: 1.0238 - val_mse: 1.0238\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1177 - mse: 1.1177 - val_loss: 1.0227 - val_mse: 1.0227\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 14s 4us/step - loss: 1.3756 - mse: 1.3756 - val_loss: 1.0263 - val_mse: 1.0263\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3528 - mse: 1.3528 - val_loss: 1.0266 - val_mse: 1.0266\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3489 - mse: 1.3489 - val_loss: 1.0268 - val_mse: 1.0268\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3463 - mse: 1.3463 - val_loss: 1.0272 - val_mse: 1.0272\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3438 - mse: 1.3438 - val_loss: 1.0254 - val_mse: 1.0254\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.4656 - mse: 1.4656 - val_loss: 1.1683 - val_mse: 1.1683\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.4615 - mse: 1.4615 - val_loss: 1.1687 - val_mse: 1.1687\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.4615 - mse: 1.4615 - val_loss: 1.1688 - val_mse: 1.1688\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.4615 - mse: 1.4615 - val_loss: 1.1685 - val_mse: 1.1685\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.4615 - mse: 1.4615 - val_loss: 1.1684 - val_mse: 1.1684\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.3209 - mse: 1.3209 - val_loss: 1.1676 - val_mse: 1.1676\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.3160 - mse: 1.3161 - val_loss: 1.1676 - val_mse: 1.1676\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.3161 - mse: 1.3161 - val_loss: 1.1675 - val_mse: 1.1675\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.3160 - mse: 1.3160 - val_loss: 1.1673 - val_mse: 1.1673\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.3161 - mse: 1.3161 - val_loss: 1.1677 - val_mse: 1.1677\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 14s 4us/step - loss: 1.5785 - mse: 1.5785 - val_loss: 1.1698 - val_mse: 1.1698\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.5752 - mse: 1.5752 - val_loss: 1.1700 - val_mse: 1.1700\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.5752 - mse: 1.5752 - val_loss: 1.1695 - val_mse: 1.1695\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.5752 - mse: 1.5752 - val_loss: 1.1698 - val_mse: 1.1698\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.5752 - mse: 1.5752 - val_loss: 1.1697 - val_mse: 1.1697\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2763 - mse: 1.2763 - val_loss: 1.0274 - val_mse: 1.0274\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.2547 - mse: 1.2547 - val_loss: 1.0237 - val_mse: 1.0237\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.2499 - mse: 1.2499 - val_loss: 1.0221 - val_mse: 1.0221\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.2461 - mse: 1.2461 - val_loss: 1.0187 - val_mse: 1.0187\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.2432 - mse: 1.2432 - val_loss: 1.0174 - val_mse: 1.0174\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1407 - mse: 1.1407 - val_loss: 1.0244 - val_mse: 1.0244\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1228 - mse: 1.1228 - val_loss: 1.0237 - val_mse: 1.0237\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.1184 - mse: 1.1184 - val_loss: 1.0219 - val_mse: 1.0219\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1156 - mse: 1.1156 - val_loss: 1.0201 - val_mse: 1.0201\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.1136 - mse: 1.1136 - val_loss: 1.0210 - val_mse: 1.0210\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3695 - mse: 1.3695 - val_loss: 1.0255 - val_mse: 1.0255\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3487 - mse: 1.3487 - val_loss: 1.0248 - val_mse: 1.0248\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3438 - mse: 1.3438 - val_loss: 1.0246 - val_mse: 1.0246\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3406 - mse: 1.3406 - val_loss: 1.0238 - val_mse: 1.0238\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3383 - mse: 1.3383 - val_loss: 1.0234 - val_mse: 1.0234\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2747 - mse: 1.2747 - val_loss: 1.0277 - val_mse: 1.0277\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.2537 - mse: 1.2537 - val_loss: 1.0208 - val_mse: 1.0208\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.2485 - mse: 1.2485 - val_loss: 1.0194 - val_mse: 1.0194\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.2445 - mse: 1.2445 - val_loss: 1.0167 - val_mse: 1.0167\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2413 - mse: 1.2413 - val_loss: 1.0139 - val_mse: 1.0139\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1392 - mse: 1.1392 - val_loss: 1.0260 - val_mse: 1.0260\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.1229 - mse: 1.1229 - val_loss: 1.0247 - val_mse: 1.0247\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.1191 - mse: 1.1191 - val_loss: 1.0249 - val_mse: 1.0249\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.1167 - mse: 1.1167 - val_loss: 1.0231 - val_mse: 1.0231\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.1149 - mse: 1.1149 - val_loss: 1.0241 - val_mse: 1.0241\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3705 - mse: 1.3705 - val_loss: 1.0254 - val_mse: 1.0254\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 14s 4us/step - loss: 1.3492 - mse: 1.3492 - val_loss: 1.0256 - val_mse: 1.0256\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 15s 4us/step - loss: 1.3443 - mse: 1.3443 - val_loss: 1.0245 - val_mse: 1.0245\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 14s 4us/step - loss: 1.3411 - mse: 1.3411 - val_loss: 1.0241 - val_mse: 1.0241\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3388 - mse: 1.3388 - val_loss: 1.0213 - val_mse: 1.0213\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.2740 - mse: 1.2740 - val_loss: 1.0246 - val_mse: 1.0246\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2536 - mse: 1.2536 - val_loss: 1.0235 - val_mse: 1.0235\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2489 - mse: 1.2489 - val_loss: 1.0195 - val_mse: 1.0195\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.2454 - mse: 1.2454 - val_loss: 1.0214 - val_mse: 1.0214\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.2428 - mse: 1.2428 - val_loss: 1.0223 - val_mse: 1.0223\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.1399 - mse: 1.1399 - val_loss: 1.0260 - val_mse: 1.0260\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1238 - mse: 1.1238 - val_loss: 1.0236 - val_mse: 1.0236\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.1201 - mse: 1.1201 - val_loss: 1.0225 - val_mse: 1.0225\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1174 - mse: 1.1174 - val_loss: 1.0227 - val_mse: 1.0227\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.1154 - mse: 1.1154 - val_loss: 1.0199 - val_mse: 1.0199\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3705 - mse: 1.3705 - val_loss: 1.0255 - val_mse: 1.0255\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3498 - mse: 1.3498 - val_loss: 1.0244 - val_mse: 1.0244\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3449 - mse: 1.3449 - val_loss: 1.0242 - val_mse: 1.0242\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3415 - mse: 1.3415 - val_loss: 1.0223 - val_mse: 1.0223\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 14s 4us/step - loss: 1.3389 - mse: 1.3389 - val_loss: 1.0214 - val_mse: 1.0214\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.2723 - mse: 1.2723 - val_loss: 1.0273 - val_mse: 1.0273\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2531 - mse: 1.2531 - val_loss: 1.0260 - val_mse: 1.0260\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2482 - mse: 1.2482 - val_loss: 1.0211 - val_mse: 1.0211\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2446 - mse: 1.2446 - val_loss: 1.0177 - val_mse: 1.0177\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2419 - mse: 1.2419 - val_loss: 1.0169 - val_mse: 1.0169\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1402 - mse: 1.1402 - val_loss: 1.0280 - val_mse: 1.0280\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1228 - mse: 1.1228 - val_loss: 1.0237 - val_mse: 1.0237\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1185 - mse: 1.1185 - val_loss: 1.0197 - val_mse: 1.0197\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1153 - mse: 1.1153 - val_loss: 1.0198 - val_mse: 1.0198\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1129 - mse: 1.1129 - val_loss: 1.0172 - val_mse: 1.0172\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3734 - mse: 1.3734 - val_loss: 1.0251 - val_mse: 1.0251\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 12s 4us/step - loss: 1.3507 - mse: 1.3507 - val_loss: 1.0263 - val_mse: 1.0263\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 12s 4us/step - loss: 1.3452 - mse: 1.3452 - val_loss: 1.0272 - val_mse: 1.0272\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3414 - mse: 1.3414 - val_loss: 1.0238 - val_mse: 1.0238\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 12s 4us/step - loss: 1.3388 - mse: 1.3388 - val_loss: 1.0240 - val_mse: 1.0240\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 5029976 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "5029976/5029976 [==============================] - 26s 5us/step - loss: 1.2571 - mse: 1.2571 - val_loss: 1.0274 - val_mse: 1.0274\n",
      "Epoch 2/5\n",
      "5029976/5029976 [==============================] - 27s 5us/step - loss: 1.2390 - mse: 1.2391 - val_loss: 1.0203 - val_mse: 1.0203\n",
      "Epoch 3/5\n",
      "5029976/5029976 [==============================] - 27s 5us/step - loss: 1.2338 - mse: 1.2338 - val_loss: 1.0189 - val_mse: 1.0189\n",
      "Epoch 4/5\n",
      "5029976/5029976 [==============================] - 28s 5us/step - loss: 1.2306 - mse: 1.2306 - val_loss: 1.0172 - val_mse: 1.0172\n",
      "Epoch 5/5\n",
      "5029976/5029976 [==============================] - 29s 6us/step - loss: 1.2282 - mse: 1.2282 - val_loss: 1.0144 - val_mse: 1.0144\n",
      "1.1115695249190403\n",
      "{'batch_size': 1000, 'epochs': 5, 'init': 'he_normal', 'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(optimizer=['Adam'], epochs=[5], batch_size=[1000], init=init)\n",
    "\n",
    "model = KerasRegressor(build_fn=create_model, verbose=1)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(train_x, train_y, validation_data=(valid_x, valid_y))\n",
    "\n",
    "print(np.sqrt(-grid_result.best_score_))\n",
    "print(grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2745 - mse: 1.2745 - val_loss: 1.0303 - val_mse: 1.0303\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2527 - mse: 1.2527 - val_loss: 1.0225 - val_mse: 1.0225\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2477 - mse: 1.2477 - val_loss: 1.0198 - val_mse: 1.0198\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.2442 - mse: 1.2442 - val_loss: 1.0170 - val_mse: 1.0170\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.2412 - mse: 1.2412 - val_loss: 1.0196 - val_mse: 1.0196\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1401 - mse: 1.1401 - val_loss: 1.0247 - val_mse: 1.0247\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.1229 - mse: 1.1229 - val_loss: 1.0236 - val_mse: 1.0236\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.1186 - mse: 1.1186 - val_loss: 1.0218 - val_mse: 1.0218\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.1154 - mse: 1.1154 - val_loss: 1.0202 - val_mse: 1.0202\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.1131 - mse: 1.1131 - val_loss: 1.0186 - val_mse: 1.0186\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 1626s 485us/step - loss: 1.3698 - mse: 1.3698 - val_loss: 1.0282 - val_mse: 1.0282\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 508s 152us/step - loss: 1.3482 - mse: 1.3482 - val_loss: 1.0242 - val_mse: 1.0242\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3426 - mse: 1.3426 - val_loss: 1.0237 - val_mse: 1.0237\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 12s 4us/step - loss: 1.3393 - mse: 1.3393 - val_loss: 1.0215 - val_mse: 1.0215\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 12s 4us/step - loss: 1.3369 - mse: 1.3369 - val_loss: 1.0240 - val_mse: 1.0240\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 15s 4us/step - loss: 1.2740 - mse: 1.2740 - val_loss: 1.0304 - val_mse: 1.0304\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.2539 - mse: 1.2539 - val_loss: 1.0230 - val_mse: 1.0230\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.2500 - mse: 1.2500 - val_loss: 1.0227 - val_mse: 1.0227\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.2471 - mse: 1.2471 - val_loss: 1.0200 - val_mse: 1.0200\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.2444 - mse: 1.2444 - val_loss: 1.0187 - val_mse: 1.0187\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 15s 4us/step - loss: 1.1429 - mse: 1.1429 - val_loss: 1.0255 - val_mse: 1.0255\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.1241 - mse: 1.1241 - val_loss: 1.0320 - val_mse: 1.0320\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.1204 - mse: 1.1204 - val_loss: 1.0254 - val_mse: 1.0254\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.1178 - mse: 1.1178 - val_loss: 1.0215 - val_mse: 1.0215\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.1158 - mse: 1.1158 - val_loss: 1.0203 - val_mse: 1.0203\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 14s 4us/step - loss: 1.3718 - mse: 1.3718 - val_loss: 1.0271 - val_mse: 1.0271\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 14s 4us/step - loss: 1.3498 - mse: 1.3498 - val_loss: 1.0275 - val_mse: 1.0275\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 14s 4us/step - loss: 1.3456 - mse: 1.3456 - val_loss: 1.0257 - val_mse: 1.0257\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 15s 4us/step - loss: 1.3428 - mse: 1.3428 - val_loss: 1.0236 - val_mse: 1.0236\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 14s 4us/step - loss: 1.3406 - mse: 1.3406 - val_loss: 1.0219 - val_mse: 1.0219\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.2998 - mse: 1.2998 - val_loss: 1.0344 - val_mse: 1.0344\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.2665 - mse: 1.2665 - val_loss: 1.0284 - val_mse: 1.0284\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.2581 - mse: 1.2581 - val_loss: 1.0262 - val_mse: 1.0262\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.2550 - mse: 1.2550 - val_loss: 1.0262 - val_mse: 1.0262\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.2530 - mse: 1.2530 - val_loss: 1.0251 - val_mse: 1.0251\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 15s 4us/step - loss: 1.1606 - mse: 1.1606 - val_loss: 1.0344 - val_mse: 1.0344\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.1327 - mse: 1.1327 - val_loss: 1.0260 - val_mse: 1.0260\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.1266 - mse: 1.1266 - val_loss: 1.0260 - val_mse: 1.0260\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.1243 - mse: 1.1243 - val_loss: 1.0257 - val_mse: 1.0257\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.1227 - mse: 1.1227 - val_loss: 1.0256 - val_mse: 1.0256\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 1.3972 - mse: 1.3972 - val_loss: 1.0371 - val_mse: 1.0371\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 15s 4us/step - loss: 1.3638 - mse: 1.3638 - val_loss: 1.0273 - val_mse: 1.0273\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 15s 4us/step - loss: 1.3546 - mse: 1.3546 - val_loss: 1.0269 - val_mse: 1.0269\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 15s 4us/step - loss: 1.3510 - mse: 1.3510 - val_loss: 1.0269 - val_mse: 1.0269\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 15s 4us/step - loss: 1.3488 - mse: 1.3488 - val_loss: 1.0269 - val_mse: 1.0269\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 20s 6us/step - loss: 1.3013 - mse: 1.3013 - val_loss: 1.0374 - val_mse: 1.0374\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 20s 6us/step - loss: 1.2757 - mse: 1.2757 - val_loss: 1.0373 - val_mse: 1.0373\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 1.2727 - mse: 1.2727 - val_loss: 1.0368 - val_mse: 1.0368\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 20s 6us/step - loss: 1.2719 - mse: 1.2719 - val_loss: 1.0356 - val_mse: 1.0356\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 20s 6us/step - loss: 1.2714 - mse: 1.2714 - val_loss: 1.0362 - val_mse: 1.0362\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 1.1632 - mse: 1.1632 - val_loss: 1.0379 - val_mse: 1.0379\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 1.1412 - mse: 1.1412 - val_loss: 1.0344 - val_mse: 1.0344\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 1.1391 - mse: 1.1391 - val_loss: 1.0341 - val_mse: 1.0341\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 1.1385 - mse: 1.1385 - val_loss: 1.0348 - val_mse: 1.0348\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 1.1382 - mse: 1.1382 - val_loss: 1.0344 - val_mse: 1.0344\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 19s 6us/step - loss: 1.4002 - mse: 1.4002 - val_loss: 1.0389 - val_mse: 1.0389\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 20s 6us/step - loss: 1.3722 - mse: 1.3722 - val_loss: 1.0350 - val_mse: 1.0350\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 20s 6us/step - loss: 1.3690 - mse: 1.3690 - val_loss: 1.0355 - val_mse: 1.0355\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 20s 6us/step - loss: 1.3681 - mse: 1.3681 - val_loss: 1.0361 - val_mse: 1.0361\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 20s 6us/step - loss: 1.3675 - mse: 1.3675 - val_loss: 1.0374 - val_mse: 1.0374\n",
      "1676658/1676658 [==============================] - 4s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.2847 - mse: 1.2847 - val_loss: 1.0342 - val_mse: 1.0342\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2719 - mse: 1.2719 - val_loss: 1.0351 - val_mse: 1.0351\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2712 - mse: 1.2712 - val_loss: 1.0337 - val_mse: 1.0337\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2710 - mse: 1.2710 - val_loss: 1.0384 - val_mse: 1.0384\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2709 - mse: 1.2709 - val_loss: 1.0360 - val_mse: 1.0360\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1473 - mse: 1.1473 - val_loss: 1.0344 - val_mse: 1.0344\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1386 - mse: 1.1386 - val_loss: 1.0350 - val_mse: 1.0350\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1381 - mse: 1.1381 - val_loss: 1.0356 - val_mse: 1.0356\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1379 - mse: 1.1379 - val_loss: 1.0367 - val_mse: 1.0367\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1378 - mse: 1.1378 - val_loss: 1.0358 - val_mse: 1.0358\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 12s 4us/step - loss: 1.3816 - mse: 1.3816 - val_loss: 1.0355 - val_mse: 1.0355\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 12s 4us/step - loss: 1.3681 - mse: 1.3681 - val_loss: 1.0376 - val_mse: 1.0376\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3673 - mse: 1.3673 - val_loss: 1.0399 - val_mse: 1.0399\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3670 - mse: 1.3670 - val_loss: 1.0388 - val_mse: 1.0388\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3668 - mse: 1.3668 - val_loss: 1.0382 - val_mse: 1.0382\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 1.3254 - mse: 1.3254 - val_loss: 1.0375 - val_mse: 1.0375\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 1.2706 - mse: 1.2706 - val_loss: 1.0303 - val_mse: 1.0303\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.2614 - mse: 1.2614 - val_loss: 1.0291 - val_mse: 1.0291\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 1.2579 - mse: 1.2579 - val_loss: 1.0280 - val_mse: 1.0280\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 1.2553 - mse: 1.2553 - val_loss: 1.0257 - val_mse: 1.0257\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 1.1890 - mse: 1.1890 - val_loss: 1.0391 - val_mse: 1.0391\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 1.1385 - mse: 1.1385 - val_loss: 1.0292 - val_mse: 1.0292\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 1.1301 - mse: 1.1301 - val_loss: 1.0273 - val_mse: 1.0273\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 1.1270 - mse: 1.1270 - val_loss: 1.0275 - val_mse: 1.0275\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 1.1248 - mse: 1.1248 - val_loss: 1.0270 - val_mse: 1.0270\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 18s 5us/step - loss: 1.4268 - mse: 1.4268 - val_loss: 1.0428 - val_mse: 1.0428\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 18s 5us/step - loss: 1.3692 - mse: 1.3692 - val_loss: 1.0306 - val_mse: 1.0306\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 18s 5us/step - loss: 1.3585 - mse: 1.3585 - val_loss: 1.0290 - val_mse: 1.0290\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 18s 5us/step - loss: 1.3536 - mse: 1.3536 - val_loss: 1.0290 - val_mse: 1.0290\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 1.3506 - mse: 1.3506 - val_loss: 1.0302 - val_mse: 1.0302\n",
      "1676658/1676658 [==============================] - 4s 2us/step\n",
      "Train on 5029976 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "5029976/5029976 [==============================] - 29s 6us/step - loss: 1.2588 - mse: 1.2588 - val_loss: 1.0266 - val_mse: 1.0266\n",
      "Epoch 2/5\n",
      "5029976/5029976 [==============================] - 29s 6us/step - loss: 1.2405 - mse: 1.2405 - val_loss: 1.0229 - val_mse: 1.0229\n",
      "Epoch 3/5\n",
      "5029976/5029976 [==============================] - 29s 6us/step - loss: 1.2362 - mse: 1.2362 - val_loss: 1.0210 - val_mse: 1.0210\n",
      "Epoch 4/5\n",
      "5029976/5029976 [==============================] - 29s 6us/step - loss: 1.2333 - mse: 1.2333 - val_loss: 1.0186 - val_mse: 1.0186\n",
      "Epoch 5/5\n",
      "5029976/5029976 [==============================] - 29s 6us/step - loss: 1.2306 - mse: 1.2306 - val_loss: 1.0145 - val_mse: 1.0145\n",
      "1.1119618093410104\n",
      "{'activation': 'tanh', 'batch_size': 1000, 'epochs': 5, 'init': 'he_normal', 'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(optimizer=['Adam'], epochs=[5], batch_size=[1000], init=['he_normal'], activation=activation)\n",
    "\n",
    "model = KerasRegressor(build_fn=create_model, verbose=1)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(train_x, train_y, validation_data=(valid_x, valid_y))\n",
    "\n",
    "print(np.sqrt(-grid_result.best_score_))\n",
    "print(grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 15s 4us/step - loss: 1.2829 - mse: 1.2829 - val_loss: 1.0278 - val_mse: 1.0278\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2573 - mse: 1.2573 - val_loss: 1.0271 - val_mse: 1.0271\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2528 - mse: 1.2528 - val_loss: 1.0241 - val_mse: 1.0241\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2501 - mse: 1.2501 - val_loss: 1.0230 - val_mse: 1.0230\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.2481 - mse: 1.2481 - val_loss: 1.0211 - val_mse: 1.0211\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1489 - mse: 1.1489 - val_loss: 1.0276 - val_mse: 1.0276\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 12s 4us/step - loss: 1.1258 - mse: 1.1258 - val_loss: 1.0254 - val_mse: 1.0254\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1222 - mse: 1.1222 - val_loss: 1.0249 - val_mse: 1.0249\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1197 - mse: 1.1197 - val_loss: 1.0235 - val_mse: 1.0235\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 13s 4us/step - loss: 1.1175 - mse: 1.1175 - val_loss: 1.0216 - val_mse: 1.0216\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3801 - mse: 1.3801 - val_loss: 1.0264 - val_mse: 1.0264\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 12s 4us/step - loss: 1.3527 - mse: 1.3527 - val_loss: 1.0262 - val_mse: 1.0262\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3479 - mse: 1.3479 - val_loss: 1.0275 - val_mse: 1.0275\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3451 - mse: 1.3451 - val_loss: 1.0259 - val_mse: 1.0259\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 13s 4us/step - loss: 1.3431 - mse: 1.3431 - val_loss: 1.0260 - val_mse: 1.0260\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.2747 - mse: 1.2747 - val_loss: 1.0276 - val_mse: 1.0276\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.2539 - mse: 1.2539 - val_loss: 1.0247 - val_mse: 1.0247\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 15s 4us/step - loss: 1.2498 - mse: 1.2498 - val_loss: 1.0287 - val_mse: 1.0287\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.2471 - mse: 1.2471 - val_loss: 1.0223 - val_mse: 1.0223\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.2446 - mse: 1.2446 - val_loss: 1.0200 - val_mse: 1.0200\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.1414 - mse: 1.1414 - val_loss: 1.0263 - val_mse: 1.0263\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 15s 4us/step - loss: 1.1237 - mse: 1.1237 - val_loss: 1.0251 - val_mse: 1.0251\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 1.1203 - mse: 1.1203 - val_loss: 1.0245 - val_mse: 1.0245\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 15s 4us/step - loss: 1.1180 - mse: 1.1180 - val_loss: 1.0230 - val_mse: 1.0230\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 15s 4us/step - loss: 1.1159 - mse: 1.1159 - val_loss: 1.0242 - val_mse: 1.0242\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 14s 4us/step - loss: 1.3740 - mse: 1.3740 - val_loss: 1.0264 - val_mse: 1.0264\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 15s 4us/step - loss: 1.3503 - mse: 1.3503 - val_loss: 1.0266 - val_mse: 1.0266\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 15s 4us/step - loss: 1.3458 - mse: 1.3458 - val_loss: 1.0247 - val_mse: 1.0247\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 15s 4us/step - loss: 1.3429 - mse: 1.3429 - val_loss: 1.0249 - val_mse: 1.0249\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 15s 4us/step - loss: 1.3407 - mse: 1.3407 - val_loss: 1.0236 - val_mse: 1.0236\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 14s 4us/step - loss: 1.2711 - mse: 1.2711 - val_loss: 1.0285 - val_mse: 1.0285\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 15s 4us/step - loss: 1.2527 - mse: 1.2527 - val_loss: 1.0230 - val_mse: 1.0230\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 15s 4us/step - loss: 1.2487 - mse: 1.2487 - val_loss: 1.0211 - val_mse: 1.0211\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 15s 4us/step - loss: 1.2458 - mse: 1.2458 - val_loss: 1.0188 - val_mse: 1.0188\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 15s 4us/step - loss: 1.2430 - mse: 1.2430 - val_loss: 1.0161 - val_mse: 1.0161\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 15s 4us/step - loss: 1.1379 - mse: 1.1379 - val_loss: 1.0261 - val_mse: 1.0261\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 1.1225 - mse: 1.1225 - val_loss: 1.0255 - val_mse: 1.0255\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 1.1191 - mse: 1.1191 - val_loss: 1.0231 - val_mse: 1.0231\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 1.1167 - mse: 1.1167 - val_loss: 1.0224 - val_mse: 1.0224\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 1.1146 - mse: 1.1146 - val_loss: 1.0236 - val_mse: 1.0236\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 14s 4us/step - loss: 1.3691 - mse: 1.3691 - val_loss: 1.0270 - val_mse: 1.0270\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 1.3483 - mse: 1.3483 - val_loss: 1.0261 - val_mse: 1.0261\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 1.3442 - mse: 1.3442 - val_loss: 1.0269 - val_mse: 1.0269\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 1.3416 - mse: 1.3416 - val_loss: 1.0245 - val_mse: 1.0245\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 1.3393 - mse: 1.3393 - val_loss: 1.0252 - val_mse: 1.0252\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 1.2672 - mse: 1.2672 - val_loss: 1.0349 - val_mse: 1.0349\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.2515 - mse: 1.2515 - val_loss: 1.0232 - val_mse: 1.0232\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.2475 - mse: 1.2475 - val_loss: 1.0191 - val_mse: 1.0191\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.2445 - mse: 1.2445 - val_loss: 1.0170 - val_mse: 1.0170\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.2415 - mse: 1.2415 - val_loss: 1.0146 - val_mse: 1.0146\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 1.1357 - mse: 1.1357 - val_loss: 1.0253 - val_mse: 1.0253\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.1215 - mse: 1.1215 - val_loss: 1.0239 - val_mse: 1.0239\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.1183 - mse: 1.1183 - val_loss: 1.0246 - val_mse: 1.0246\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.1159 - mse: 1.1159 - val_loss: 1.0251 - val_mse: 1.0251\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.1136 - mse: 1.1136 - val_loss: 1.0178 - val_mse: 1.0178\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 1.3641 - mse: 1.3641 - val_loss: 1.0275 - val_mse: 1.0275\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 18s 5us/step - loss: 1.3468 - mse: 1.3468 - val_loss: 1.0282 - val_mse: 1.0282\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 1.3430 - mse: 1.3430 - val_loss: 1.0277 - val_mse: 1.0277\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 1.3403 - mse: 1.3403 - val_loss: 1.0252 - val_mse: 1.0252\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 1.3384 - mse: 1.3384 - val_loss: 1.0214 - val_mse: 1.0214\n",
      "1676658/1676658 [==============================] - 4s 2us/step\n",
      "Train on 5029976 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "5029976/5029976 [==============================] - 33s 7us/step - loss: 1.2528 - mse: 1.2528 - val_loss: 1.0241 - val_mse: 1.0241\n",
      "Epoch 2/5\n",
      "5029976/5029976 [==============================] - 34s 7us/step - loss: 1.2379 - mse: 1.2379 - val_loss: 1.0230 - val_mse: 1.0230\n",
      "Epoch 3/5\n",
      "5029976/5029976 [==============================] - 33s 7us/step - loss: 1.2337 - mse: 1.2337 - val_loss: 1.0185 - val_mse: 1.0185\n",
      "Epoch 4/5\n",
      "5029976/5029976 [==============================] - 34s 7us/step - loss: 1.2303 - mse: 1.2303 - val_loss: 1.0138 - val_mse: 1.0138\n",
      "Epoch 5/5\n",
      "5029976/5029976 [==============================] - 33s 7us/step - loss: 1.2278 - mse: 1.2278 - val_loss: 1.0112 - val_mse: 1.0112\n",
      "1.1115709604966868\n",
      "{'activation': 'tanh', 'batch_size': 1000, 'epochs': 5, 'init': 'he_normal', 'neurons': 200, 'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(optimizer=['Adam'], epochs=[5], batch_size=[1000], init=['he_normal'], activation=['tanh'], neurons=neurons)\n",
    "\n",
    "model = KerasRegressor(build_fn=create_model, verbose=1)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(train_x, train_y, validation_data=(valid_x, valid_y))\n",
    "\n",
    "print(np.sqrt(-grid_result.best_score_))\n",
    "print(grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.8449 - mse: 0.8449 - val_loss: 0.7668 - val_mse: 0.7668\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.7877 - mse: 0.7877 - val_loss: 0.7575 - val_mse: 0.7575\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 0.7595 - mse: 0.7595 - val_loss: 0.7452 - val_mse: 0.7452\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.7426 - mse: 0.7426 - val_loss: 0.7346 - val_mse: 0.7346\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.7303 - mse: 0.7303 - val_loss: 0.7368 - val_mse: 0.7368\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 0.7480 - mse: 0.7480 - val_loss: 0.7729 - val_mse: 0.7729\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.7083 - mse: 0.7083 - val_loss: 0.7519 - val_mse: 0.7519\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6896 - mse: 0.6896 - val_loss: 0.7502 - val_mse: 0.7502\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6765 - mse: 0.6765 - val_loss: 0.7417 - val_mse: 0.7417\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6680 - mse: 0.6680 - val_loss: 0.7345 - val_mse: 0.7345\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 0.8582 - mse: 0.8582 - val_loss: 0.8241 - val_mse: 0.8241\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 0.8025 - mse: 0.8025 - val_loss: 0.8070 - val_mse: 0.8070\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 0.7813 - mse: 0.7813 - val_loss: 0.7818 - val_mse: 0.7818\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 0.7679 - mse: 0.7679 - val_loss: 0.7736 - val_mse: 0.7736\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 0.7579 - mse: 0.7579 - val_loss: 0.7665 - val_mse: 0.7665\n",
      "1676658/1676658 [==============================] - 4s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 1.2723 - mse: 1.2723 - val_loss: 1.0270 - val_mse: 1.0270\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.2579 - mse: 1.2579 - val_loss: 1.0238 - val_mse: 1.0238\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.2535 - mse: 1.2535 - val_loss: 1.0222 - val_mse: 1.0222\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.2509 - mse: 1.2509 - val_loss: 1.0247 - val_mse: 1.0247\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.2488 - mse: 1.2488 - val_loss: 1.0200 - val_mse: 1.0200\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.1404 - mse: 1.1404 - val_loss: 1.0281 - val_mse: 1.0281\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 18s 6us/step - loss: 1.1283 - mse: 1.1283 - val_loss: 1.0247 - val_mse: 1.0247\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.1240 - mse: 1.1240 - val_loss: 1.0240 - val_mse: 1.0240\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 1.1216 - mse: 1.1216 - val_loss: 1.0225 - val_mse: 1.0225\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 1.1198 - mse: 1.1198 - val_loss: 1.0230 - val_mse: 1.0230\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 18s 5us/step - loss: 1.3703 - mse: 1.3703 - val_loss: 1.0289 - val_mse: 1.0289\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 18s 5us/step - loss: 1.3543 - mse: 1.3543 - val_loss: 1.0300 - val_mse: 1.0300\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 18s 5us/step - loss: 1.3496 - mse: 1.3496 - val_loss: 1.0277 - val_mse: 1.0277\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 18s 5us/step - loss: 1.3470 - mse: 1.3470 - val_loss: 1.0290 - val_mse: 1.0290\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 18s 5us/step - loss: 1.3454 - mse: 1.3454 - val_loss: 1.0273 - val_mse: 1.0273\n",
      "1676658/1676658 [==============================] - 4s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 1.2681 - mse: 1.2681 - val_loss: 1.0253 - val_mse: 1.0253\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.2516 - mse: 1.2516 - val_loss: 1.0241 - val_mse: 1.0241\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 1.2476 - mse: 1.2476 - val_loss: 1.0193 - val_mse: 1.0193\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.2444 - mse: 1.2444 - val_loss: 1.0236 - val_mse: 1.0236\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.2416 - mse: 1.2416 - val_loss: 1.0142 - val_mse: 1.0142\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 1.1364 - mse: 1.1364 - val_loss: 1.0260 - val_mse: 1.0260\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.1216 - mse: 1.1216 - val_loss: 1.0245 - val_mse: 1.0245\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.1182 - mse: 1.1182 - val_loss: 1.0244 - val_mse: 1.0244\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 1.1156 - mse: 1.1156 - val_loss: 1.0205 - val_mse: 1.0205\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 20s 6us/step - loss: 1.1134 - mse: 1.1134 - val_loss: 1.0192 - val_mse: 1.0192\n",
      "1676659/1676659 [==============================] - 4s 3us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 18s 5us/step - loss: 1.3655 - mse: 1.3655 - val_loss: 1.0268 - val_mse: 1.0268\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 20s 6us/step - loss: 1.3472 - mse: 1.3472 - val_loss: 1.0308 - val_mse: 1.0308\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 18s 5us/step - loss: 1.3433 - mse: 1.3433 - val_loss: 1.0266 - val_mse: 1.0266\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 18s 5us/step - loss: 1.3406 - mse: 1.3406 - val_loss: 1.0243 - val_mse: 1.0243\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 18s 5us/step - loss: 1.3384 - mse: 1.3384 - val_loss: 1.0207 - val_mse: 1.0207\n",
      "1676658/1676658 [==============================] - 4s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.2697 - mse: 1.2697 - val_loss: 1.0254 - val_mse: 1.0254\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 1.2518 - mse: 1.2518 - val_loss: 1.0247 - val_mse: 1.0247\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 1.2479 - mse: 1.2479 - val_loss: 1.0221 - val_mse: 1.0221\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 1.2451 - mse: 1.2451 - val_loss: 1.0198 - val_mse: 1.0198\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 1.2427 - mse: 1.2427 - val_loss: 1.0171 - val_mse: 1.0171\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.1355 - mse: 1.1355 - val_loss: 1.0251 - val_mse: 1.0251\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.1214 - mse: 1.1214 - val_loss: 1.0246 - val_mse: 1.0246\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.1185 - mse: 1.1185 - val_loss: 1.0263 - val_mse: 1.0263\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.1164 - mse: 1.1164 - val_loss: 1.0222 - val_mse: 1.0222\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.1147 - mse: 1.1147 - val_loss: 1.0202 - val_mse: 1.0202\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 18s 5us/step - loss: 1.3654 - mse: 1.3654 - val_loss: 1.0288 - val_mse: 1.0288\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 19s 6us/step - loss: 1.3470 - mse: 1.3470 - val_loss: 1.0295 - val_mse: 1.0295\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 19s 6us/step - loss: 1.3435 - mse: 1.3435 - val_loss: 1.0302 - val_mse: 1.0302\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 18s 5us/step - loss: 1.3410 - mse: 1.3410 - val_loss: 1.0301 - val_mse: 1.0301\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 19s 6us/step - loss: 1.3392 - mse: 1.3392 - val_loss: 1.0258 - val_mse: 1.0258\n",
      "1676658/1676658 [==============================] - 4s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.8597 - mse: 0.8597 - val_loss: 0.7627 - val_mse: 0.7627\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 0.8030 - mse: 0.8030 - val_loss: 0.7565 - val_mse: 0.7565\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 0.7778 - mse: 0.7778 - val_loss: 0.7610 - val_mse: 0.7610\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.7592 - mse: 0.7592 - val_loss: 0.7554 - val_mse: 0.7554\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.7454 - mse: 0.7454 - val_loss: 0.7298 - val_mse: 0.7298\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.7571 - mse: 0.7571 - val_loss: 0.7735 - val_mse: 0.7735\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 0.7146 - mse: 0.7146 - val_loss: 0.7573 - val_mse: 0.7573\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 0.6975 - mse: 0.6975 - val_loss: 0.7514 - val_mse: 0.7514\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 0.6845 - mse: 0.6845 - val_loss: 0.7430 - val_mse: 0.7430\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6750 - mse: 0.6750 - val_loss: 0.7449 - val_mse: 0.7449\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 0.8743 - mse: 0.8743 - val_loss: 0.8202 - val_mse: 0.8202\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 0.8204 - mse: 0.8204 - val_loss: 0.8738 - val_mse: 0.8738\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 0.7976 - mse: 0.7976 - val_loss: 0.8053 - val_mse: 0.8053\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 0.7813 - mse: 0.7813 - val_loss: 0.7738 - val_mse: 0.7738\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 0.7694 - mse: 0.7694 - val_loss: 0.7748 - val_mse: 0.7748\n",
      "1676658/1676658 [==============================] - 4s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 1.9505 - mse: 1.9505 - val_loss: 1.7170 - val_mse: 1.7170\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 1.9505 - mse: 1.9505 - val_loss: 1.7170 - val_mse: 1.7170\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 1.9505 - mse: 1.9505 - val_loss: 1.7170 - val_mse: 1.7170\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 1.9505 - mse: 1.9505 - val_loss: 1.7170 - val_mse: 1.7170\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 1.9505 - mse: 1.9505 - val_loss: 1.7170 - val_mse: 1.7170\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 1.8235 - mse: 1.8235 - val_loss: 1.7170 - val_mse: 1.7170\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 20s 6us/step - loss: 1.8235 - mse: 1.8235 - val_loss: 1.7170 - val_mse: 1.7170\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 1.8235 - mse: 1.8235 - val_loss: 1.7170 - val_mse: 1.7170\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 1.8235 - mse: 1.8235 - val_loss: 1.7170 - val_mse: 1.7170\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 1.8235 - mse: 1.8235 - val_loss: 1.7170 - val_mse: 1.7170\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 19s 6us/step - loss: 2.0460 - mse: 2.0460 - val_loss: 1.7170 - val_mse: 1.7170\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 19s 6us/step - loss: 2.0460 - mse: 2.0460 - val_loss: 1.7170 - val_mse: 1.7170\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 19s 6us/step - loss: 2.0460 - mse: 2.0460 - val_loss: 1.7170 - val_mse: 1.7170\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 19s 6us/step - loss: 2.0460 - mse: 2.0460 - val_loss: 1.7170 - val_mse: 1.7170\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 19s 6us/step - loss: 2.0460 - mse: 2.0460 - val_loss: 1.7170 - val_mse: 1.7170\n",
      "1676658/1676658 [==============================] - 4s 3us/step\n",
      "Train on 5029976 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "5029976/5029976 [==============================] - 34s 7us/step - loss: 0.8085 - mse: 0.8085 - val_loss: 0.7662 - val_mse: 0.7662\n",
      "Epoch 2/5\n",
      "5029976/5029976 [==============================] - 34s 7us/step - loss: 0.7567 - mse: 0.7567 - val_loss: 0.7541 - val_mse: 0.7541\n",
      "Epoch 3/5\n",
      "5029976/5029976 [==============================] - 34s 7us/step - loss: 0.7341 - mse: 0.7341 - val_loss: 0.7359 - val_mse: 0.7359\n",
      "Epoch 4/5\n",
      "5029976/5029976 [==============================] - 34s 7us/step - loss: 0.7203 - mse: 0.7203 - val_loss: 0.7358 - val_mse: 0.7358\n",
      "Epoch 5/5\n",
      "5029976/5029976 [==============================] - 34s 7us/step - loss: 0.7110 - mse: 0.7110 - val_loss: 0.7249 - val_mse: 0.7249\n",
      "0.8739637301445746\n",
      "{'activation': 'tanh', 'activation2': 'relu', 'batch_size': 1000, 'epochs': 5, 'init': 'he_normal', 'neurons': 200, 'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(optimizer=['Adam'], epochs=[5], batch_size=[1000], init=['he_normal'], activation=['tanh'], neurons=[200], activation2=activation2)\n",
    "\n",
    "model = KerasRegressor(build_fn=create_model, verbose=1)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(train_x, train_y, validation_data=(valid_x, valid_y))\n",
    "\n",
    "print(np.sqrt(-grid_result.best_score_))\n",
    "print(grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 0.8464 - mse: 0.8464 - val_loss: 0.7970 - val_mse: 0.7970\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 0.7906 - mse: 0.7906 - val_loss: 0.7566 - val_mse: 0.7566\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.7646 - mse: 0.7646 - val_loss: 0.7486 - val_mse: 0.7486\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 0.7461 - mse: 0.7461 - val_loss: 0.7313 - val_mse: 0.7313\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 0.7337 - mse: 0.7337 - val_loss: 0.7395 - val_mse: 0.7395\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 0.7491 - mse: 0.7491 - val_loss: 0.7617 - val_mse: 0.7617\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 0.7055 - mse: 0.7055 - val_loss: 0.7477 - val_mse: 0.7477\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6876 - mse: 0.6876 - val_loss: 0.7451 - val_mse: 0.7451\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6748 - mse: 0.6748 - val_loss: 0.7432 - val_mse: 0.7432\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6657 - mse: 0.6657 - val_loss: 0.7311 - val_mse: 0.7311\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 18s 5us/step - loss: 0.8648 - mse: 0.8648 - val_loss: 0.8409 - val_mse: 0.8409\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 18s 5us/step - loss: 0.8060 - mse: 0.8060 - val_loss: 0.8168 - val_mse: 0.8168\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 18s 5us/step - loss: 0.7832 - mse: 0.7832 - val_loss: 0.7841 - val_mse: 0.7841\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 18s 5us/step - loss: 0.7690 - mse: 0.7690 - val_loss: 0.7941 - val_mse: 0.7941\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 18s 5us/step - loss: 0.7593 - mse: 0.7593 - val_loss: 0.7697 - val_mse: 0.7697\n",
      "1676658/1676658 [==============================] - 4s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 36s 11us/step - loss: 0.8816 - mse: 0.8816 - val_loss: 0.7845 - val_mse: 0.7845\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 36s 11us/step - loss: 0.8394 - mse: 0.8394 - val_loss: 0.7755 - val_mse: 0.7755\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 36s 11us/step - loss: 0.8247 - mse: 0.8247 - val_loss: 0.7706 - val_mse: 0.7706\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 37s 11us/step - loss: 0.8131 - mse: 0.8131 - val_loss: 0.7645 - val_mse: 0.7645\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 37s 11us/step - loss: 0.8044 - mse: 0.8044 - val_loss: 0.7582 - val_mse: 0.7582\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 36s 11us/step - loss: 0.7880 - mse: 0.7880 - val_loss: 0.7892 - val_mse: 0.7892\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 37s 11us/step - loss: 0.7450 - mse: 0.7450 - val_loss: 0.7755 - val_mse: 0.7755\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 36s 11us/step - loss: 0.7330 - mse: 0.7330 - val_loss: 0.7727 - val_mse: 0.7727\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 37s 11us/step - loss: 0.7251 - mse: 0.7251 - val_loss: 0.7673 - val_mse: 0.7673\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 36s 11us/step - loss: 0.7184 - mse: 0.7184 - val_loss: 0.7543 - val_mse: 0.7543\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 37s 11us/step - loss: 0.8950 - mse: 0.8950 - val_loss: 0.8460 - val_mse: 0.8460\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 37s 11us/step - loss: 0.8498 - mse: 0.8498 - val_loss: 0.8414 - val_mse: 0.8414\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 38s 11us/step - loss: 0.8361 - mse: 0.8361 - val_loss: 0.8301 - val_mse: 0.8301\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 37s 11us/step - loss: 0.8275 - mse: 0.8275 - val_loss: 0.8159 - val_mse: 0.8159\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 38s 11us/step - loss: 0.8212 - mse: 0.8212 - val_loss: 0.8173 - val_mse: 0.8173\n",
      "1676658/1676658 [==============================] - 4s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 38s 11us/step - loss: 0.9050 - mse: 0.9050 - val_loss: 0.7895 - val_mse: 0.7895\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 39s 11us/step - loss: 0.8581 - mse: 0.8581 - val_loss: 0.7816 - val_mse: 0.7816\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 39s 12us/step - loss: 0.8439 - mse: 0.8439 - val_loss: 0.8096 - val_mse: 0.8096\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 39s 12us/step - loss: 0.8338 - mse: 0.8338 - val_loss: 0.7746 - val_mse: 0.7746\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 39s 12us/step - loss: 0.8261 - mse: 0.8260 - val_loss: 0.7696 - val_mse: 0.7696\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 38s 11us/step - loss: 0.7962 - mse: 0.7962 - val_loss: 0.7892 - val_mse: 0.7892\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 38s 11us/step - loss: 0.7613 - mse: 0.7613 - val_loss: 0.7840 - val_mse: 0.7840\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 38s 11us/step - loss: 0.7504 - mse: 0.7504 - val_loss: 0.7777 - val_mse: 0.7777\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 38s 11us/step - loss: 0.7423 - mse: 0.7423 - val_loss: 0.7753 - val_mse: 0.7753\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 38s 11us/step - loss: 0.7372 - mse: 0.7372 - val_loss: 0.7673 - val_mse: 0.7673\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 38s 11us/step - loss: 0.9111 - mse: 0.9111 - val_loss: 0.8641 - val_mse: 0.8641\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 38s 11us/step - loss: 0.8669 - mse: 0.8669 - val_loss: 0.8510 - val_mse: 0.8510\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 38s 11us/step - loss: 0.8539 - mse: 0.8539 - val_loss: 0.8430 - val_mse: 0.8430\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 38s 11us/step - loss: 0.8450 - mse: 0.8450 - val_loss: 0.8485 - val_mse: 0.8485\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 39s 11us/step - loss: 0.8402 - mse: 0.8402 - val_loss: 0.8344 - val_mse: 0.8344\n",
      "1676658/1676658 [==============================] - 4s 2us/step\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 38s 11us/step - loss: 0.9187 - mse: 0.9187 - val_loss: 0.8004 - val_mse: 0.8004\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 39s 12us/step - loss: 0.8741 - mse: 0.8741 - val_loss: 0.7967 - val_mse: 0.7967\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 39s 12us/step - loss: 0.8622 - mse: 0.8622 - val_loss: 0.7906 - val_mse: 0.7906\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 39s 12us/step - loss: 0.8543 - mse: 0.8543 - val_loss: 0.7875 - val_mse: 0.7875\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 38s 11us/step - loss: 0.8484 - mse: 0.8484 - val_loss: 0.7854 - val_mse: 0.7854\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 37s 11us/step - loss: 0.8121 - mse: 0.8121 - val_loss: 0.8074 - val_mse: 0.8074\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 37s 11us/step - loss: 0.7738 - mse: 0.7738 - val_loss: 0.8066 - val_mse: 0.8066\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 38s 11us/step - loss: 0.7628 - mse: 0.7628 - val_loss: 0.7972 - val_mse: 0.7972\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 38s 11us/step - loss: 0.7565 - mse: 0.7565 - val_loss: 0.7973 - val_mse: 0.7973\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 38s 11us/step - loss: 0.7519 - mse: 0.7519 - val_loss: 0.7929 - val_mse: 0.7929\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 38s 11us/step - loss: 0.9489 - mse: 0.9489 - val_loss: 0.8534 - val_mse: 0.8534\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 38s 11us/step - loss: 0.8809 - mse: 0.8809 - val_loss: 0.8677 - val_mse: 0.8677\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 38s 11us/step - loss: 0.8684 - mse: 0.8684 - val_loss: 0.8622 - val_mse: 0.8622\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 38s 11us/step - loss: 0.8623 - mse: 0.8623 - val_loss: 0.8639 - val_mse: 0.8639\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 38s 11us/step - loss: 0.8577 - mse: 0.8577 - val_loss: 0.8771 - val_mse: 0.8771\n",
      "1676658/1676658 [==============================] - 4s 2us/step\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 37s 11us/step - loss: 0.9659 - mse: 0.9659 - val_loss: 0.7966 - val_mse: 0.7966\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 37s 11us/step - loss: 0.8949 - mse: 0.8949 - val_loss: 0.8112 - val_mse: 0.8112\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 37s 11us/step - loss: 0.8825 - mse: 0.8825 - val_loss: 0.8091 - val_mse: 0.8091\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 37s 11us/step - loss: 0.8750 - mse: 0.8750 - val_loss: 0.7968 - val_mse: 0.7968\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 37s 11us/step - loss: 0.8704 - mse: 0.8704 - val_loss: 0.7951 - val_mse: 0.7951\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 37s 11us/step - loss: 0.8632 - mse: 0.8632 - val_loss: 0.8194 - val_mse: 0.8194\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 37s 11us/step - loss: 0.7950 - mse: 0.7950 - val_loss: 0.8104 - val_mse: 0.8104\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 37s 11us/step - loss: 0.7814 - mse: 0.7814 - val_loss: 0.8133 - val_mse: 0.8133\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 37s 11us/step - loss: 0.7739 - mse: 0.7739 - val_loss: 0.8015 - val_mse: 0.8015\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 37s 11us/step - loss: 0.7694 - mse: 0.7694 - val_loss: 0.7951 - val_mse: 0.7951\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 36s 11us/step - loss: 0.9801 - mse: 0.9801 - val_loss: 0.8398 - val_mse: 0.8398\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 37s 11us/step - loss: 0.9030 - mse: 0.9030 - val_loss: 0.8661 - val_mse: 0.8661\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 37s 11us/step - loss: 0.8879 - mse: 0.8879 - val_loss: 0.8727 - val_mse: 0.8727\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 37s 11us/step - loss: 0.8816 - mse: 0.8816 - val_loss: 0.8725 - val_mse: 0.8725\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 37s 11us/step - loss: 0.8777 - mse: 0.8777 - val_loss: 0.8731 - val_mse: 0.8731\n",
      "1676658/1676658 [==============================] - 4s 2us/step\n",
      "Train on 5029976 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "5029976/5029976 [==============================] - 35s 7us/step - loss: 0.8101 - mse: 0.8101 - val_loss: 0.7701 - val_mse: 0.7701\n",
      "Epoch 2/5\n",
      "5029976/5029976 [==============================] - 34s 7us/step - loss: 0.7571 - mse: 0.7571 - val_loss: 0.7664 - val_mse: 0.7664\n",
      "Epoch 3/5\n",
      "5029976/5029976 [==============================] - 34s 7us/step - loss: 0.7343 - mse: 0.7343 - val_loss: 0.7310 - val_mse: 0.7310\n",
      "Epoch 4/5\n",
      "5029976/5029976 [==============================] - 34s 7us/step - loss: 0.7209 - mse: 0.7209 - val_loss: 0.7599 - val_mse: 0.7599\n",
      "Epoch 5/5\n",
      "5029976/5029976 [==============================] - 34s 7us/step - loss: 0.7111 - mse: 0.7111 - val_loss: 0.7455 - val_mse: 0.7455\n",
      "0.8749114688256486\n",
      "{'activation': 'tanh', 'activation2': 'relu', 'batch_size': 1000, 'dropout_rate': 0.0, 'epochs': 5, 'init': 'he_normal', 'neurons': 200, 'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(optimizer=['Adam'], epochs=[5], batch_size=[1000], init=['he_normal'], activation=['tanh'], neurons=[200], activation2=['relu'], dropout_rate=dropout_rate)\n",
    "\n",
    "model = KerasRegressor(build_fn=create_model, verbose=1)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(train_x, train_y, validation_data=(valid_x, valid_y))\n",
    "\n",
    "print(np.sqrt(-grid_result.best_score_))\n",
    "print(grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.8460 - mse: 0.8460 - val_loss: 0.7806 - val_mse: 0.7806\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.7900 - mse: 0.7900 - val_loss: 0.7758 - val_mse: 0.7758\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 20s 6us/step - loss: 0.7629 - mse: 0.7629 - val_loss: 0.7413 - val_mse: 0.7413\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.7454 - mse: 0.7454 - val_loss: 0.7526 - val_mse: 0.7526\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.7334 - mse: 0.7334 - val_loss: 0.7449 - val_mse: 0.7449\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.7500 - mse: 0.7500 - val_loss: 0.7681 - val_mse: 0.7681\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.7067 - mse: 0.7067 - val_loss: 0.7623 - val_mse: 0.7623\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.6882 - mse: 0.6882 - val_loss: 0.7562 - val_mse: 0.7562\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.6753 - mse: 0.6753 - val_loss: 0.7425 - val_mse: 0.7425\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 0.6663 - mse: 0.6663 - val_loss: 0.7291 - val_mse: 0.7291\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 19s 6us/step - loss: 0.8604 - mse: 0.8604 - val_loss: 0.8264 - val_mse: 0.8264\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 19s 6us/step - loss: 0.8035 - mse: 0.8035 - val_loss: 0.8087 - val_mse: 0.8087\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 19s 6us/step - loss: 0.7826 - mse: 0.7826 - val_loss: 0.7914 - val_mse: 0.7914\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 19s 6us/step - loss: 0.7685 - mse: 0.7685 - val_loss: 0.7832 - val_mse: 0.7832\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 19s 6us/step - loss: 0.7593 - mse: 0.7593 - val_loss: 0.7706 - val_mse: 0.7706\n",
      "1676658/1676658 [==============================] - 4s 3us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/10\n",
      "3353317/3353317 [==============================] - 20s 6us/step - loss: 0.8473 - mse: 0.8473 - val_loss: 0.7724 - val_mse: 0.7724\n",
      "Epoch 2/10\n",
      "3353317/3353317 [==============================] - 20s 6us/step - loss: 0.7896 - mse: 0.7896 - val_loss: 0.7567 - val_mse: 0.7567\n",
      "Epoch 3/10\n",
      "3353317/3353317 [==============================] - 20s 6us/step - loss: 0.7622 - mse: 0.7622 - val_loss: 0.7658 - val_mse: 0.7658\n",
      "Epoch 4/10\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.7440 - mse: 0.7440 - val_loss: 0.7353 - val_mse: 0.7353\n",
      "Epoch 5/10\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.7317 - mse: 0.7317 - val_loss: 0.7214 - val_mse: 0.7214\n",
      "Epoch 6/10\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.7239 - mse: 0.7239 - val_loss: 0.7162 - val_mse: 0.7162\n",
      "Epoch 7/10\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.7168 - mse: 0.7168 - val_loss: 0.7047 - val_mse: 0.7047\n",
      "Epoch 8/10\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.7108 - mse: 0.7108 - val_loss: 0.7057 - val_mse: 0.7057\n",
      "Epoch 9/10\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.7051 - mse: 0.7051 - val_loss: 0.7108 - val_mse: 0.7108\n",
      "Epoch 10/10\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.6999 - mse: 0.6999 - val_loss: 0.7068 - val_mse: 0.7068\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/10\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.7495 - mse: 0.7495 - val_loss: 0.7634 - val_mse: 0.7634\n",
      "Epoch 2/10\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.7066 - mse: 0.7066 - val_loss: 0.7532 - val_mse: 0.7532\n",
      "Epoch 3/10\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.6879 - mse: 0.6879 - val_loss: 0.7440 - val_mse: 0.7440\n",
      "Epoch 4/10\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.6763 - mse: 0.6763 - val_loss: 0.7445 - val_mse: 0.7445\n",
      "Epoch 5/10\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.6672 - mse: 0.6672 - val_loss: 0.7287 - val_mse: 0.7287\n",
      "Epoch 6/10\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.6605 - mse: 0.6605 - val_loss: 0.7245 - val_mse: 0.7245\n",
      "Epoch 7/10\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.6548 - mse: 0.6548 - val_loss: 0.7207 - val_mse: 0.7207\n",
      "Epoch 8/10\n",
      "3353317/3353317 [==============================] - 19s 6us/step - loss: 0.6497 - mse: 0.6497 - val_loss: 0.7247 - val_mse: 0.7247\n",
      "Epoch 9/10\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 0.6454 - mse: 0.6454 - val_loss: 0.7287 - val_mse: 0.7287\n",
      "Epoch 10/10\n",
      "3353317/3353317 [==============================] - 18s 5us/step - loss: 0.6412 - mse: 0.6412 - val_loss: 0.7091 - val_mse: 0.7091\n",
      "1676659/1676659 [==============================] - 4s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/10\n",
      "3353318/3353318 [==============================] - 18s 5us/step - loss: 0.8601 - mse: 0.8601 - val_loss: 0.8250 - val_mse: 0.8250\n",
      "Epoch 2/10\n",
      "3353318/3353318 [==============================] - 19s 6us/step - loss: 0.8042 - mse: 0.8042 - val_loss: 0.8084 - val_mse: 0.8084\n",
      "Epoch 3/10\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 0.7819 - mse: 0.7819 - val_loss: 0.7870 - val_mse: 0.7870\n",
      "Epoch 4/10\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 0.7681 - mse: 0.7681 - val_loss: 0.7845 - val_mse: 0.7845\n",
      "Epoch 5/10\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 0.7587 - mse: 0.7587 - val_loss: 0.7819 - val_mse: 0.7819\n",
      "Epoch 6/10\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 0.7505 - mse: 0.7505 - val_loss: 0.7709 - val_mse: 0.7709\n",
      "Epoch 7/10\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 0.7450 - mse: 0.7450 - val_loss: 0.7642 - val_mse: 0.7642\n",
      "Epoch 8/10\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 0.7394 - mse: 0.7394 - val_loss: 0.7574 - val_mse: 0.7574\n",
      "Epoch 9/10\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 0.7342 - mse: 0.7342 - val_loss: 0.7445 - val_mse: 0.7445\n",
      "Epoch 10/10\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 0.7297 - mse: 0.7297 - val_loss: 0.7502 - val_mse: 0.7502\n",
      "1676658/1676658 [==============================] - 4s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.8450 - mse: 0.8450 - val_loss: 0.7715 - val_mse: 0.7715\n",
      "Epoch 2/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.7890 - mse: 0.7890 - val_loss: 0.8104 - val_mse: 0.8104\n",
      "Epoch 3/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.7619 - mse: 0.7619 - val_loss: 0.7462 - val_mse: 0.7462\n",
      "Epoch 4/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.7446 - mse: 0.7446 - val_loss: 0.7394 - val_mse: 0.7394\n",
      "Epoch 5/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.7326 - mse: 0.7326 - val_loss: 0.7278 - val_mse: 0.7278\n",
      "Epoch 6/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.7247 - mse: 0.7247 - val_loss: 0.7179 - val_mse: 0.7179\n",
      "Epoch 7/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.7183 - mse: 0.7183 - val_loss: 0.7105 - val_mse: 0.7105\n",
      "Epoch 8/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.7120 - mse: 0.7120 - val_loss: 0.7746 - val_mse: 0.7746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.7067 - mse: 0.7067 - val_loss: 0.7088 - val_mse: 0.7088\n",
      "Epoch 10/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.7015 - mse: 0.7015 - val_loss: 0.7008 - val_mse: 0.7008\n",
      "Epoch 11/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6966 - mse: 0.6966 - val_loss: 0.6970 - val_mse: 0.6970\n",
      "Epoch 12/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6926 - mse: 0.6926 - val_loss: 0.7036 - val_mse: 0.7036\n",
      "Epoch 13/30\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6888 - mse: 0.6888 - val_loss: 0.6913 - val_mse: 0.6913\n",
      "Epoch 14/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6848 - mse: 0.6848 - val_loss: 0.7005 - val_mse: 0.7005\n",
      "Epoch 15/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6815 - mse: 0.6815 - val_loss: 0.6856 - val_mse: 0.6856\n",
      "Epoch 16/30\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6781 - mse: 0.6781 - val_loss: 0.7071 - val_mse: 0.7071\n",
      "Epoch 17/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6750 - mse: 0.6750 - val_loss: 0.6974 - val_mse: 0.6974\n",
      "Epoch 18/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6722 - mse: 0.6722 - val_loss: 0.6815 - val_mse: 0.6815\n",
      "Epoch 19/30\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6695 - mse: 0.6695 - val_loss: 0.6797 - val_mse: 0.6797\n",
      "Epoch 20/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6662 - mse: 0.6662 - val_loss: 0.6862 - val_mse: 0.6862\n",
      "Epoch 21/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6642 - mse: 0.6642 - val_loss: 0.6905 - val_mse: 0.6905\n",
      "Epoch 22/30\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6612 - mse: 0.6612 - val_loss: 0.6737 - val_mse: 0.6737\n",
      "Epoch 23/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6586 - mse: 0.6586 - val_loss: 0.6950 - val_mse: 0.6950\n",
      "Epoch 24/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6559 - mse: 0.6559 - val_loss: 0.6675 - val_mse: 0.6675\n",
      "Epoch 25/30\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6542 - mse: 0.6542 - val_loss: 0.6780 - val_mse: 0.6780\n",
      "Epoch 26/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6517 - mse: 0.6517 - val_loss: 0.6892 - val_mse: 0.6892\n",
      "Epoch 27/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6496 - mse: 0.6496 - val_loss: 0.6876 - val_mse: 0.6876\n",
      "Epoch 28/30\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6476 - mse: 0.6476 - val_loss: 0.6707 - val_mse: 0.6707\n",
      "Epoch 29/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6456 - mse: 0.6456 - val_loss: 0.6691 - val_mse: 0.6691\n",
      "Epoch 30/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6436 - mse: 0.6436 - val_loss: 0.6738 - val_mse: 0.6738\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/30\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7498 - mse: 0.7498 - val_loss: 0.7775 - val_mse: 0.7775\n",
      "Epoch 2/30\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7057 - mse: 0.7057 - val_loss: 0.7459 - val_mse: 0.7459\n",
      "Epoch 3/30\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6873 - mse: 0.6873 - val_loss: 0.7500 - val_mse: 0.7500\n",
      "Epoch 4/30\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6750 - mse: 0.6750 - val_loss: 0.7380 - val_mse: 0.7380\n",
      "Epoch 5/30\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6661 - mse: 0.6661 - val_loss: 0.7400 - val_mse: 0.7400\n",
      "Epoch 6/30\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6593 - mse: 0.6593 - val_loss: 0.7296 - val_mse: 0.7296\n",
      "Epoch 7/30\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6538 - mse: 0.6538 - val_loss: 0.7314 - val_mse: 0.7314\n",
      "Epoch 8/30\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6490 - mse: 0.6490 - val_loss: 0.7191 - val_mse: 0.7191\n",
      "Epoch 9/30\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6449 - mse: 0.6449 - val_loss: 0.7047 - val_mse: 0.7047\n",
      "Epoch 10/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6398 - mse: 0.6398 - val_loss: 0.7129 - val_mse: 0.7129\n",
      "Epoch 11/30\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6362 - mse: 0.6362 - val_loss: 0.7035 - val_mse: 0.7035\n",
      "Epoch 12/30\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6326 - mse: 0.6326 - val_loss: 0.7048 - val_mse: 0.7048\n",
      "Epoch 13/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6296 - mse: 0.6296 - val_loss: 0.7011 - val_mse: 0.7011\n",
      "Epoch 14/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6262 - mse: 0.6262 - val_loss: 0.7050 - val_mse: 0.7050\n",
      "Epoch 15/30\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6231 - mse: 0.6231 - val_loss: 0.7191 - val_mse: 0.7191\n",
      "Epoch 16/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6209 - mse: 0.6209 - val_loss: 0.7023 - val_mse: 0.7023\n",
      "Epoch 17/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6179 - mse: 0.6179 - val_loss: 0.7035 - val_mse: 0.7035\n",
      "Epoch 18/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6159 - mse: 0.6159 - val_loss: 0.7013 - val_mse: 0.7013\n",
      "Epoch 19/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6137 - mse: 0.6137 - val_loss: 0.6898 - val_mse: 0.6898\n",
      "Epoch 20/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6115 - mse: 0.6115 - val_loss: 0.6931 - val_mse: 0.6931\n",
      "Epoch 21/30\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6099 - mse: 0.6099 - val_loss: 0.7094 - val_mse: 0.7094\n",
      "Epoch 22/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6077 - mse: 0.6077 - val_loss: 0.6868 - val_mse: 0.6868\n",
      "Epoch 23/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6063 - mse: 0.6063 - val_loss: 0.6834 - val_mse: 0.6834\n",
      "Epoch 24/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6042 - mse: 0.6042 - val_loss: 0.6929 - val_mse: 0.6929\n",
      "Epoch 25/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6024 - mse: 0.6024 - val_loss: 0.6897 - val_mse: 0.6897\n",
      "Epoch 26/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6010 - mse: 0.6010 - val_loss: 0.6897 - val_mse: 0.6897\n",
      "Epoch 27/30\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5995 - mse: 0.5995 - val_loss: 0.6837 - val_mse: 0.6837\n",
      "Epoch 28/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5977 - mse: 0.5977 - val_loss: 0.6982 - val_mse: 0.6982\n",
      "Epoch 29/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5968 - mse: 0.5968 - val_loss: 0.6805 - val_mse: 0.6805\n",
      "Epoch 30/30\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5950 - mse: 0.5950 - val_loss: 0.6712 - val_mse: 0.6712\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/30\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.8616 - mse: 0.8616 - val_loss: 0.8367 - val_mse: 0.8367\n",
      "Epoch 2/30\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.8052 - mse: 0.8052 - val_loss: 0.8086 - val_mse: 0.8086\n",
      "Epoch 3/30\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7831 - mse: 0.7831 - val_loss: 0.7889 - val_mse: 0.7889\n",
      "Epoch 4/30\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7686 - mse: 0.7686 - val_loss: 0.7827 - val_mse: 0.7827\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7588 - mse: 0.7588 - val_loss: 0.8022 - val_mse: 0.8022\n",
      "Epoch 6/30\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7511 - mse: 0.7511 - val_loss: 0.7740 - val_mse: 0.7740\n",
      "Epoch 7/30\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7448 - mse: 0.7448 - val_loss: 0.7538 - val_mse: 0.7538\n",
      "Epoch 8/30\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7388 - mse: 0.7388 - val_loss: 0.7486 - val_mse: 0.7486\n",
      "Epoch 9/30\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7343 - mse: 0.7343 - val_loss: 0.7714 - val_mse: 0.7714\n",
      "Epoch 10/30\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7290 - mse: 0.7290 - val_loss: 0.7904 - val_mse: 0.7904\n",
      "Epoch 11/30\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.7247 - mse: 0.7247 - val_loss: 0.7506 - val_mse: 0.7506\n",
      "Epoch 12/30\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7210 - mse: 0.7210 - val_loss: 0.7445 - val_mse: 0.7445\n",
      "Epoch 13/30\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.7174 - mse: 0.7174 - val_loss: 0.7439 - val_mse: 0.7439\n",
      "Epoch 14/30\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7141 - mse: 0.7141 - val_loss: 0.7467 - val_mse: 0.7467\n",
      "Epoch 15/30\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7107 - mse: 0.7107 - val_loss: 0.7430 - val_mse: 0.7430\n",
      "Epoch 16/30\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7075 - mse: 0.7075 - val_loss: 0.7410 - val_mse: 0.7410\n",
      "Epoch 17/30\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.7044 - mse: 0.7044 - val_loss: 0.7383 - val_mse: 0.7383\n",
      "Epoch 18/30\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7013 - mse: 0.7013 - val_loss: 0.7353 - val_mse: 0.7353\n",
      "Epoch 19/30\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6989 - mse: 0.6989 - val_loss: 0.7398 - val_mse: 0.7398\n",
      "Epoch 20/30\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6958 - mse: 0.6958 - val_loss: 0.7504 - val_mse: 0.7504\n",
      "Epoch 21/30\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6938 - mse: 0.6938 - val_loss: 0.7547 - val_mse: 0.7547\n",
      "Epoch 22/30\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6915 - mse: 0.6915 - val_loss: 0.7420 - val_mse: 0.7420\n",
      "Epoch 23/30\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6886 - mse: 0.6886 - val_loss: 0.7385 - val_mse: 0.7385\n",
      "Epoch 24/30\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6863 - mse: 0.6863 - val_loss: 0.7334 - val_mse: 0.7334\n",
      "Epoch 25/30\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6846 - mse: 0.6846 - val_loss: 0.7533 - val_mse: 0.7533\n",
      "Epoch 26/30\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6825 - mse: 0.6825 - val_loss: 0.7435 - val_mse: 0.7435\n",
      "Epoch 27/30\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6804 - mse: 0.6804 - val_loss: 0.7317 - val_mse: 0.7317\n",
      "Epoch 28/30\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6777 - mse: 0.6777 - val_loss: 0.7480 - val_mse: 0.7480\n",
      "Epoch 29/30\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6758 - mse: 0.6758 - val_loss: 0.7375 - val_mse: 0.7375\n",
      "Epoch 30/30\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6742 - mse: 0.6742 - val_loss: 0.7388 - val_mse: 0.7388\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.8478 - mse: 0.8478 - val_loss: 0.7657 - val_mse: 0.7657\n",
      "Epoch 2/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7902 - mse: 0.7902 - val_loss: 0.7590 - val_mse: 0.7590\n",
      "Epoch 3/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7626 - mse: 0.7626 - val_loss: 0.7418 - val_mse: 0.7418\n",
      "Epoch 4/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7444 - mse: 0.7444 - val_loss: 0.7287 - val_mse: 0.7287\n",
      "Epoch 5/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7329 - mse: 0.7329 - val_loss: 0.7295 - val_mse: 0.7295\n",
      "Epoch 6/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7243 - mse: 0.7243 - val_loss: 0.7318 - val_mse: 0.7318\n",
      "Epoch 7/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7174 - mse: 0.7174 - val_loss: 0.7177 - val_mse: 0.7177\n",
      "Epoch 8/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7111 - mse: 0.7111 - val_loss: 0.7079 - val_mse: 0.7079\n",
      "Epoch 9/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7054 - mse: 0.7054 - val_loss: 0.7060 - val_mse: 0.7060\n",
      "Epoch 10/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7008 - mse: 0.7008 - val_loss: 0.7015 - val_mse: 0.7015\n",
      "Epoch 11/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6955 - mse: 0.6955 - val_loss: 0.7095 - val_mse: 0.7095\n",
      "Epoch 12/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6919 - mse: 0.6919 - val_loss: 0.7046 - val_mse: 0.7046\n",
      "Epoch 13/50\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6883 - mse: 0.6883 - val_loss: 0.7220 - val_mse: 0.7220\n",
      "Epoch 14/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6842 - mse: 0.6842 - val_loss: 0.6965 - val_mse: 0.6965\n",
      "Epoch 15/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6805 - mse: 0.6805 - val_loss: 0.6908 - val_mse: 0.6908\n",
      "Epoch 16/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6774 - mse: 0.6774 - val_loss: 0.7060 - val_mse: 0.7060\n",
      "Epoch 17/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6737 - mse: 0.6737 - val_loss: 0.6816 - val_mse: 0.6816\n",
      "Epoch 18/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6705 - mse: 0.6705 - val_loss: 0.6736 - val_mse: 0.6736\n",
      "Epoch 19/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6678 - mse: 0.6678 - val_loss: 0.6806 - val_mse: 0.6806\n",
      "Epoch 20/50\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6643 - mse: 0.6643 - val_loss: 0.6832 - val_mse: 0.6832\n",
      "Epoch 21/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6618 - mse: 0.6618 - val_loss: 0.6745 - val_mse: 0.6745\n",
      "Epoch 22/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6593 - mse: 0.6593 - val_loss: 0.6828 - val_mse: 0.6828\n",
      "Epoch 23/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6566 - mse: 0.6566 - val_loss: 0.6775 - val_mse: 0.6775\n",
      "Epoch 24/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6547 - mse: 0.6547 - val_loss: 0.6918 - val_mse: 0.6918\n",
      "Epoch 25/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6527 - mse: 0.6527 - val_loss: 0.6768 - val_mse: 0.6768\n",
      "Epoch 26/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6506 - mse: 0.6506 - val_loss: 0.6977 - val_mse: 0.6977\n",
      "Epoch 27/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6488 - mse: 0.6488 - val_loss: 0.6682 - val_mse: 0.6682\n",
      "Epoch 28/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6465 - mse: 0.6465 - val_loss: 0.6857 - val_mse: 0.6857\n",
      "Epoch 29/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6453 - mse: 0.6453 - val_loss: 0.6868 - val_mse: 0.6868\n",
      "Epoch 30/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6435 - mse: 0.6435 - val_loss: 0.6782 - val_mse: 0.6782\n",
      "Epoch 31/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6413 - mse: 0.6413 - val_loss: 0.6664 - val_mse: 0.6664\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6402 - mse: 0.6402 - val_loss: 0.6867 - val_mse: 0.6867\n",
      "Epoch 33/50\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6386 - mse: 0.6386 - val_loss: 0.6741 - val_mse: 0.6741\n",
      "Epoch 34/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6369 - mse: 0.6369 - val_loss: 0.6601 - val_mse: 0.6601\n",
      "Epoch 35/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6354 - mse: 0.6354 - val_loss: 0.6671 - val_mse: 0.6671\n",
      "Epoch 36/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6342 - mse: 0.6342 - val_loss: 0.6649 - val_mse: 0.6649\n",
      "Epoch 37/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6331 - mse: 0.6331 - val_loss: 0.6665 - val_mse: 0.6665\n",
      "Epoch 38/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6314 - mse: 0.6314 - val_loss: 0.6669 - val_mse: 0.6669\n",
      "Epoch 39/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6303 - mse: 0.6303 - val_loss: 0.6556 - val_mse: 0.6556\n",
      "Epoch 40/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6292 - mse: 0.6292 - val_loss: 0.6609 - val_mse: 0.6609\n",
      "Epoch 41/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6275 - mse: 0.6275 - val_loss: 0.6800 - val_mse: 0.6800\n",
      "Epoch 42/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6262 - mse: 0.6262 - val_loss: 0.6623 - val_mse: 0.6623\n",
      "Epoch 43/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6250 - mse: 0.6250 - val_loss: 0.6637 - val_mse: 0.6637\n",
      "Epoch 44/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6235 - mse: 0.6235 - val_loss: 0.6647 - val_mse: 0.6647\n",
      "Epoch 45/50\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6228 - mse: 0.6228 - val_loss: 0.6672 - val_mse: 0.6672\n",
      "Epoch 46/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6217 - mse: 0.6217 - val_loss: 0.6707 - val_mse: 0.6707\n",
      "Epoch 47/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6207 - mse: 0.6207 - val_loss: 0.6809 - val_mse: 0.6809\n",
      "Epoch 48/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6191 - mse: 0.6191 - val_loss: 0.6526 - val_mse: 0.6526\n",
      "Epoch 49/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6184 - mse: 0.6184 - val_loss: 0.6640 - val_mse: 0.6640\n",
      "Epoch 50/50\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6169 - mse: 0.6169 - val_loss: 0.6646 - val_mse: 0.6646\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7470 - mse: 0.7470 - val_loss: 0.7663 - val_mse: 0.7663\n",
      "Epoch 2/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7073 - mse: 0.7073 - val_loss: 0.7537 - val_mse: 0.7537\n",
      "Epoch 3/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6896 - mse: 0.6896 - val_loss: 0.7553 - val_mse: 0.7553\n",
      "Epoch 4/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6767 - mse: 0.6767 - val_loss: 0.7622 - val_mse: 0.7622\n",
      "Epoch 5/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6680 - mse: 0.6680 - val_loss: 0.7322 - val_mse: 0.7322\n",
      "Epoch 6/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6610 - mse: 0.6610 - val_loss: 0.7319 - val_mse: 0.7319\n",
      "Epoch 7/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6550 - mse: 0.6550 - val_loss: 0.7216 - val_mse: 0.7216\n",
      "Epoch 8/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6496 - mse: 0.6496 - val_loss: 0.7229 - val_mse: 0.7229\n",
      "Epoch 9/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6449 - mse: 0.6449 - val_loss: 0.7191 - val_mse: 0.7191\n",
      "Epoch 10/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6410 - mse: 0.6410 - val_loss: 0.7116 - val_mse: 0.7116\n",
      "Epoch 11/50\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6377 - mse: 0.6377 - val_loss: 0.7105 - val_mse: 0.7105\n",
      "Epoch 12/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6333 - mse: 0.6333 - val_loss: 0.7015 - val_mse: 0.7015\n",
      "Epoch 13/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6304 - mse: 0.6304 - val_loss: 0.7227 - val_mse: 0.7227\n",
      "Epoch 14/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6272 - mse: 0.6272 - val_loss: 0.7053 - val_mse: 0.7053\n",
      "Epoch 15/50\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6244 - mse: 0.6244 - val_loss: 0.7074 - val_mse: 0.7074\n",
      "Epoch 16/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6217 - mse: 0.6217 - val_loss: 0.7038 - val_mse: 0.7038\n",
      "Epoch 17/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6191 - mse: 0.6191 - val_loss: 0.7007 - val_mse: 0.7007\n",
      "Epoch 18/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6170 - mse: 0.6170 - val_loss: 0.7026 - val_mse: 0.7026\n",
      "Epoch 19/50\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6146 - mse: 0.6146 - val_loss: 0.6935 - val_mse: 0.6935\n",
      "Epoch 20/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6124 - mse: 0.6124 - val_loss: 0.7308 - val_mse: 0.7308\n",
      "Epoch 21/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6104 - mse: 0.6104 - val_loss: 0.7005 - val_mse: 0.7005\n",
      "Epoch 22/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6087 - mse: 0.6087 - val_loss: 0.6873 - val_mse: 0.6873\n",
      "Epoch 23/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6068 - mse: 0.6068 - val_loss: 0.7069 - val_mse: 0.7069\n",
      "Epoch 24/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6053 - mse: 0.6053 - val_loss: 0.6894 - val_mse: 0.6894\n",
      "Epoch 25/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6036 - mse: 0.6036 - val_loss: 0.6806 - val_mse: 0.6806\n",
      "Epoch 26/50\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6018 - mse: 0.6018 - val_loss: 0.6937 - val_mse: 0.6937\n",
      "Epoch 27/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6000 - mse: 0.6000 - val_loss: 0.6866 - val_mse: 0.6866\n",
      "Epoch 28/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5988 - mse: 0.5988 - val_loss: 0.6774 - val_mse: 0.6774\n",
      "Epoch 29/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5976 - mse: 0.5976 - val_loss: 0.6971 - val_mse: 0.6971\n",
      "Epoch 30/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5963 - mse: 0.5963 - val_loss: 0.6826 - val_mse: 0.6826\n",
      "Epoch 31/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5945 - mse: 0.5945 - val_loss: 0.6833 - val_mse: 0.6833\n",
      "Epoch 32/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5933 - mse: 0.5933 - val_loss: 0.6817 - val_mse: 0.6817\n",
      "Epoch 33/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5923 - mse: 0.5923 - val_loss: 0.6967 - val_mse: 0.6967\n",
      "Epoch 34/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5910 - mse: 0.5910 - val_loss: 0.6862 - val_mse: 0.6862\n",
      "Epoch 35/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5897 - mse: 0.5897 - val_loss: 0.6809 - val_mse: 0.6809\n",
      "Epoch 36/50\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5883 - mse: 0.5883 - val_loss: 0.6798 - val_mse: 0.6798\n",
      "Epoch 37/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5873 - mse: 0.5873 - val_loss: 0.6807 - val_mse: 0.6807\n",
      "Epoch 38/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5864 - mse: 0.5864 - val_loss: 0.7036 - val_mse: 0.7036\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5851 - mse: 0.5851 - val_loss: 0.6860 - val_mse: 0.6860\n",
      "Epoch 40/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5840 - mse: 0.5840 - val_loss: 0.6740 - val_mse: 0.6740\n",
      "Epoch 41/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5830 - mse: 0.5830 - val_loss: 0.6881 - val_mse: 0.6881\n",
      "Epoch 42/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5820 - mse: 0.5820 - val_loss: 0.6809 - val_mse: 0.6809\n",
      "Epoch 43/50\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5808 - mse: 0.5807 - val_loss: 0.6681 - val_mse: 0.6681\n",
      "Epoch 44/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5801 - mse: 0.5801 - val_loss: 0.6843 - val_mse: 0.6843\n",
      "Epoch 45/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5790 - mse: 0.5790 - val_loss: 0.6859 - val_mse: 0.6859\n",
      "Epoch 46/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5777 - mse: 0.5777 - val_loss: 0.6709 - val_mse: 0.6709\n",
      "Epoch 47/50\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5769 - mse: 0.5769 - val_loss: 0.6691 - val_mse: 0.6691\n",
      "Epoch 48/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5758 - mse: 0.5758 - val_loss: 0.6703 - val_mse: 0.6703\n",
      "Epoch 49/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5748 - mse: 0.5748 - val_loss: 0.6845 - val_mse: 0.6845\n",
      "Epoch 50/50\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5741 - mse: 0.5741 - val_loss: 0.6889 - val_mse: 0.6889\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.8599 - mse: 0.8599 - val_loss: 0.8232 - val_mse: 0.8232\n",
      "Epoch 2/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.8052 - mse: 0.8052 - val_loss: 0.8070 - val_mse: 0.8070\n",
      "Epoch 3/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7835 - mse: 0.7835 - val_loss: 0.7824 - val_mse: 0.7824\n",
      "Epoch 4/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7694 - mse: 0.7694 - val_loss: 0.7715 - val_mse: 0.7715\n",
      "Epoch 5/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7605 - mse: 0.7605 - val_loss: 0.7600 - val_mse: 0.7600\n",
      "Epoch 6/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7530 - mse: 0.7530 - val_loss: 0.7625 - val_mse: 0.7625\n",
      "Epoch 7/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7462 - mse: 0.7462 - val_loss: 0.7665 - val_mse: 0.7665\n",
      "Epoch 8/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7406 - mse: 0.7406 - val_loss: 0.7454 - val_mse: 0.7454\n",
      "Epoch 9/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7354 - mse: 0.7354 - val_loss: 0.7546 - val_mse: 0.7546\n",
      "Epoch 10/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7306 - mse: 0.7306 - val_loss: 0.7346 - val_mse: 0.7346\n",
      "Epoch 11/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7269 - mse: 0.7269 - val_loss: 0.7356 - val_mse: 0.7356\n",
      "Epoch 12/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7221 - mse: 0.7221 - val_loss: 0.7371 - val_mse: 0.7371\n",
      "Epoch 13/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7193 - mse: 0.7193 - val_loss: 0.7398 - val_mse: 0.7398\n",
      "Epoch 14/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7154 - mse: 0.7154 - val_loss: 0.7452 - val_mse: 0.7452\n",
      "Epoch 15/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7124 - mse: 0.7124 - val_loss: 0.7506 - val_mse: 0.7506\n",
      "Epoch 16/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7092 - mse: 0.7092 - val_loss: 0.7437 - val_mse: 0.7437\n",
      "Epoch 17/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7061 - mse: 0.7061 - val_loss: 0.7350 - val_mse: 0.7350\n",
      "Epoch 18/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7041 - mse: 0.7041 - val_loss: 0.7601 - val_mse: 0.7601\n",
      "Epoch 19/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7011 - mse: 0.7011 - val_loss: 0.7314 - val_mse: 0.7314\n",
      "Epoch 20/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6989 - mse: 0.6989 - val_loss: 0.7804 - val_mse: 0.7804\n",
      "Epoch 21/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6963 - mse: 0.6963 - val_loss: 0.7537 - val_mse: 0.7537\n",
      "Epoch 22/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6943 - mse: 0.6943 - val_loss: 0.7525 - val_mse: 0.7525\n",
      "Epoch 23/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6919 - mse: 0.6919 - val_loss: 0.7497 - val_mse: 0.7497\n",
      "Epoch 24/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6898 - mse: 0.6898 - val_loss: 0.7486 - val_mse: 0.7486\n",
      "Epoch 25/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6876 - mse: 0.6876 - val_loss: 0.7566 - val_mse: 0.7566\n",
      "Epoch 26/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6860 - mse: 0.6860 - val_loss: 0.7544 - val_mse: 0.7544\n",
      "Epoch 27/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6835 - mse: 0.6835 - val_loss: 0.7693 - val_mse: 0.7693\n",
      "Epoch 28/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6819 - mse: 0.6819 - val_loss: 0.7838 - val_mse: 0.7838\n",
      "Epoch 29/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6793 - mse: 0.6793 - val_loss: 0.7766 - val_mse: 0.7766\n",
      "Epoch 30/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6775 - mse: 0.6775 - val_loss: 0.7652 - val_mse: 0.7652\n",
      "Epoch 31/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6761 - mse: 0.6761 - val_loss: 0.7521 - val_mse: 0.7521\n",
      "Epoch 32/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6745 - mse: 0.6745 - val_loss: 0.7504 - val_mse: 0.7504\n",
      "Epoch 33/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6729 - mse: 0.6729 - val_loss: 0.7610 - val_mse: 0.7610\n",
      "Epoch 34/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6709 - mse: 0.6709 - val_loss: 0.7686 - val_mse: 0.7686\n",
      "Epoch 35/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6696 - mse: 0.6696 - val_loss: 0.7485 - val_mse: 0.7485\n",
      "Epoch 36/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6680 - mse: 0.6680 - val_loss: 0.7758 - val_mse: 0.7758\n",
      "Epoch 37/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6666 - mse: 0.6666 - val_loss: 0.7707 - val_mse: 0.7707\n",
      "Epoch 38/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6645 - mse: 0.6645 - val_loss: 0.7593 - val_mse: 0.7593\n",
      "Epoch 39/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6634 - mse: 0.6634 - val_loss: 0.7785 - val_mse: 0.7785\n",
      "Epoch 40/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6620 - mse: 0.6620 - val_loss: 0.7623 - val_mse: 0.7623\n",
      "Epoch 41/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6608 - mse: 0.6608 - val_loss: 0.7919 - val_mse: 0.7919\n",
      "Epoch 42/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6592 - mse: 0.6592 - val_loss: 0.7701 - val_mse: 0.7701\n",
      "Epoch 43/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6576 - mse: 0.6576 - val_loss: 0.7790 - val_mse: 0.7790\n",
      "Epoch 44/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6568 - mse: 0.6568 - val_loss: 0.7654 - val_mse: 0.7654\n",
      "Epoch 45/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6551 - mse: 0.6551 - val_loss: 0.7630 - val_mse: 0.7630\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6541 - mse: 0.6541 - val_loss: 0.7783 - val_mse: 0.7783\n",
      "Epoch 47/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6531 - mse: 0.6531 - val_loss: 0.7722 - val_mse: 0.7722\n",
      "Epoch 48/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6521 - mse: 0.6521 - val_loss: 0.7949 - val_mse: 0.7949\n",
      "Epoch 49/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6505 - mse: 0.6505 - val_loss: 0.7875 - val_mse: 0.7875\n",
      "Epoch 50/50\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6497 - mse: 0.6497 - val_loss: 0.7844 - val_mse: 0.7844\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.8486 - mse: 0.8486 - val_loss: 0.7829 - val_mse: 0.7829\n",
      "Epoch 2/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7885 - mse: 0.7885 - val_loss: 0.7888 - val_mse: 0.7888\n",
      "Epoch 3/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7623 - mse: 0.7623 - val_loss: 0.7423 - val_mse: 0.7423\n",
      "Epoch 4/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7440 - mse: 0.7440 - val_loss: 0.7343 - val_mse: 0.7343\n",
      "Epoch 5/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7323 - mse: 0.7323 - val_loss: 0.7268 - val_mse: 0.7268\n",
      "Epoch 6/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7242 - mse: 0.7242 - val_loss: 0.7146 - val_mse: 0.7146\n",
      "Epoch 7/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7173 - mse: 0.7173 - val_loss: 0.7182 - val_mse: 0.7182\n",
      "Epoch 8/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7100 - mse: 0.7100 - val_loss: 0.7158 - val_mse: 0.7158\n",
      "Epoch 9/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7046 - mse: 0.7046 - val_loss: 0.7029 - val_mse: 0.7029\n",
      "Epoch 10/100\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6997 - mse: 0.6997 - val_loss: 0.6992 - val_mse: 0.6992\n",
      "Epoch 11/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6946 - mse: 0.6946 - val_loss: 0.6996 - val_mse: 0.6996\n",
      "Epoch 12/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6900 - mse: 0.6900 - val_loss: 0.6898 - val_mse: 0.6898\n",
      "Epoch 13/100\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6860 - mse: 0.6860 - val_loss: 0.6951 - val_mse: 0.6951\n",
      "Epoch 14/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6818 - mse: 0.6818 - val_loss: 0.6916 - val_mse: 0.6916\n",
      "Epoch 15/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6784 - mse: 0.6784 - val_loss: 0.6884 - val_mse: 0.6884\n",
      "Epoch 16/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6748 - mse: 0.6748 - val_loss: 0.6808 - val_mse: 0.6808\n",
      "Epoch 17/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6717 - mse: 0.6717 - val_loss: 0.6910 - val_mse: 0.6910\n",
      "Epoch 18/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6684 - mse: 0.6684 - val_loss: 0.6818 - val_mse: 0.6818\n",
      "Epoch 19/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6653 - mse: 0.6653 - val_loss: 0.6853 - val_mse: 0.6853\n",
      "Epoch 20/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6628 - mse: 0.6628 - val_loss: 0.6828 - val_mse: 0.6828\n",
      "Epoch 21/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6602 - mse: 0.6602 - val_loss: 0.6860 - val_mse: 0.6860\n",
      "Epoch 22/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6578 - mse: 0.6578 - val_loss: 0.6805 - val_mse: 0.6805\n",
      "Epoch 23/100\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6559 - mse: 0.6559 - val_loss: 0.7028 - val_mse: 0.7028\n",
      "Epoch 24/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6537 - mse: 0.6537 - val_loss: 0.6753 - val_mse: 0.6753\n",
      "Epoch 25/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6516 - mse: 0.6516 - val_loss: 0.6653 - val_mse: 0.6653\n",
      "Epoch 26/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6494 - mse: 0.6494 - val_loss: 0.6740 - val_mse: 0.6740\n",
      "Epoch 27/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6476 - mse: 0.6476 - val_loss: 0.6767 - val_mse: 0.6767\n",
      "Epoch 28/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6458 - mse: 0.6458 - val_loss: 0.6724 - val_mse: 0.6724\n",
      "Epoch 29/100\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6441 - mse: 0.6441 - val_loss: 0.6948 - val_mse: 0.6948\n",
      "Epoch 30/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6423 - mse: 0.6423 - val_loss: 0.6849 - val_mse: 0.6849\n",
      "Epoch 31/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6406 - mse: 0.6406 - val_loss: 0.7070 - val_mse: 0.7070\n",
      "Epoch 32/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6392 - mse: 0.6392 - val_loss: 0.6899 - val_mse: 0.6899\n",
      "Epoch 33/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6374 - mse: 0.6374 - val_loss: 0.6925 - val_mse: 0.6925\n",
      "Epoch 34/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6364 - mse: 0.6364 - val_loss: 0.6727 - val_mse: 0.6727\n",
      "Epoch 35/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6343 - mse: 0.6343 - val_loss: 0.6657 - val_mse: 0.6657\n",
      "Epoch 36/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6326 - mse: 0.6326 - val_loss: 0.6658 - val_mse: 0.6658\n",
      "Epoch 37/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6316 - mse: 0.6316 - val_loss: 0.6675 - val_mse: 0.6675\n",
      "Epoch 38/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6304 - mse: 0.6304 - val_loss: 0.6802 - val_mse: 0.6802\n",
      "Epoch 39/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6284 - mse: 0.6284 - val_loss: 0.6784 - val_mse: 0.6784\n",
      "Epoch 40/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6274 - mse: 0.6274 - val_loss: 0.6888 - val_mse: 0.6888\n",
      "Epoch 41/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6264 - mse: 0.6264 - val_loss: 0.7123 - val_mse: 0.7123\n",
      "Epoch 42/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6248 - mse: 0.6248 - val_loss: 0.6909 - val_mse: 0.6909\n",
      "Epoch 43/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6230 - mse: 0.6230 - val_loss: 0.6935 - val_mse: 0.6935\n",
      "Epoch 44/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6224 - mse: 0.6224 - val_loss: 0.6625 - val_mse: 0.6625\n",
      "Epoch 45/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6210 - mse: 0.6210 - val_loss: 0.6774 - val_mse: 0.6774\n",
      "Epoch 46/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6199 - mse: 0.6199 - val_loss: 0.6715 - val_mse: 0.6715\n",
      "Epoch 47/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6185 - mse: 0.6185 - val_loss: 0.6733 - val_mse: 0.6733\n",
      "Epoch 48/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6180 - mse: 0.6180 - val_loss: 0.6952 - val_mse: 0.6952\n",
      "Epoch 49/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6166 - mse: 0.6166 - val_loss: 0.7174 - val_mse: 0.7174\n",
      "Epoch 50/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6153 - mse: 0.6153 - val_loss: 0.6683 - val_mse: 0.6683\n",
      "Epoch 51/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6143 - mse: 0.6143 - val_loss: 0.6701 - val_mse: 0.6701\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6133 - mse: 0.6133 - val_loss: 0.7129 - val_mse: 0.7129\n",
      "Epoch 53/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6124 - mse: 0.6124 - val_loss: 0.6769 - val_mse: 0.6769\n",
      "Epoch 54/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6110 - mse: 0.6110 - val_loss: 0.6855 - val_mse: 0.6855\n",
      "Epoch 55/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6101 - mse: 0.6101 - val_loss: 0.6998 - val_mse: 0.6998\n",
      "Epoch 56/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6086 - mse: 0.6086 - val_loss: 0.6672 - val_mse: 0.6672\n",
      "Epoch 57/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6083 - mse: 0.6083 - val_loss: 0.6732 - val_mse: 0.6732\n",
      "Epoch 58/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6072 - mse: 0.6072 - val_loss: 0.6536 - val_mse: 0.6536\n",
      "Epoch 59/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6056 - mse: 0.6056 - val_loss: 0.7208 - val_mse: 0.7208\n",
      "Epoch 60/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6049 - mse: 0.6049 - val_loss: 0.6582 - val_mse: 0.6582\n",
      "Epoch 61/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6035 - mse: 0.6035 - val_loss: 0.6979 - val_mse: 0.6979\n",
      "Epoch 62/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6028 - mse: 0.6028 - val_loss: 0.6944 - val_mse: 0.6944\n",
      "Epoch 63/100\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6018 - mse: 0.6018 - val_loss: 0.6613 - val_mse: 0.6613\n",
      "Epoch 64/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6010 - mse: 0.6010 - val_loss: 0.6697 - val_mse: 0.6697\n",
      "Epoch 65/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5999 - mse: 0.5999 - val_loss: 0.6786 - val_mse: 0.6786\n",
      "Epoch 66/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5990 - mse: 0.5990 - val_loss: 0.6573 - val_mse: 0.6573\n",
      "Epoch 67/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5981 - mse: 0.5981 - val_loss: 0.6643 - val_mse: 0.6643\n",
      "Epoch 68/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5970 - mse: 0.5970 - val_loss: 0.6798 - val_mse: 0.6798\n",
      "Epoch 69/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5966 - mse: 0.5966 - val_loss: 0.6848 - val_mse: 0.6848\n",
      "Epoch 70/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5954 - mse: 0.5954 - val_loss: 0.6546 - val_mse: 0.6546\n",
      "Epoch 71/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5948 - mse: 0.5948 - val_loss: 0.6923 - val_mse: 0.6923\n",
      "Epoch 72/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5939 - mse: 0.5939 - val_loss: 0.6789 - val_mse: 0.6789\n",
      "Epoch 73/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5931 - mse: 0.5931 - val_loss: 0.6742 - val_mse: 0.6742\n",
      "Epoch 74/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5924 - mse: 0.5924 - val_loss: 0.6633 - val_mse: 0.6633\n",
      "Epoch 75/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5918 - mse: 0.5918 - val_loss: 0.6765 - val_mse: 0.6765\n",
      "Epoch 76/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5909 - mse: 0.5909 - val_loss: 0.6845 - val_mse: 0.6845\n",
      "Epoch 77/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5900 - mse: 0.5901 - val_loss: 0.6678 - val_mse: 0.6678\n",
      "Epoch 78/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5892 - mse: 0.5892 - val_loss: 0.6824 - val_mse: 0.6824\n",
      "Epoch 79/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5888 - mse: 0.5888 - val_loss: 0.6736 - val_mse: 0.6736\n",
      "Epoch 80/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5880 - mse: 0.5880 - val_loss: 0.6868 - val_mse: 0.6868\n",
      "Epoch 81/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5873 - mse: 0.5873 - val_loss: 0.6775 - val_mse: 0.6775\n",
      "Epoch 82/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5868 - mse: 0.5868 - val_loss: 0.6618 - val_mse: 0.6618\n",
      "Epoch 83/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5856 - mse: 0.5856 - val_loss: 0.6762 - val_mse: 0.6762\n",
      "Epoch 84/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5853 - mse: 0.5853 - val_loss: 0.6799 - val_mse: 0.6799\n",
      "Epoch 85/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5848 - mse: 0.5848 - val_loss: 0.6962 - val_mse: 0.6962\n",
      "Epoch 86/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5842 - mse: 0.5842 - val_loss: 0.6981 - val_mse: 0.6981\n",
      "Epoch 87/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5828 - mse: 0.5828 - val_loss: 0.6690 - val_mse: 0.6690\n",
      "Epoch 88/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5829 - mse: 0.5829 - val_loss: 0.6844 - val_mse: 0.6844\n",
      "Epoch 89/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5820 - mse: 0.5820 - val_loss: 0.6813 - val_mse: 0.6813\n",
      "Epoch 90/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5814 - mse: 0.5814 - val_loss: 0.6774 - val_mse: 0.6774\n",
      "Epoch 91/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5814 - mse: 0.5814 - val_loss: 0.6959 - val_mse: 0.6959\n",
      "Epoch 92/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5807 - mse: 0.5807 - val_loss: 0.7047 - val_mse: 0.7047\n",
      "Epoch 93/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5800 - mse: 0.5800 - val_loss: 0.6998 - val_mse: 0.6998\n",
      "Epoch 94/100\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5790 - mse: 0.5790 - val_loss: 0.6707 - val_mse: 0.6707\n",
      "Epoch 95/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5785 - mse: 0.5785 - val_loss: 0.6801 - val_mse: 0.6801\n",
      "Epoch 96/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5782 - mse: 0.5782 - val_loss: 0.7001 - val_mse: 0.7001\n",
      "Epoch 97/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5778 - mse: 0.5778 - val_loss: 0.6580 - val_mse: 0.6580\n",
      "Epoch 98/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5768 - mse: 0.5768 - val_loss: 0.6721 - val_mse: 0.6721\n",
      "Epoch 99/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5766 - mse: 0.5766 - val_loss: 0.6605 - val_mse: 0.6605\n",
      "Epoch 100/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5759 - mse: 0.5759 - val_loss: 0.7004 - val_mse: 0.7004\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7541 - mse: 0.7541 - val_loss: 0.7638 - val_mse: 0.7638\n",
      "Epoch 2/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7079 - mse: 0.7079 - val_loss: 0.7527 - val_mse: 0.7527\n",
      "Epoch 3/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6887 - mse: 0.6887 - val_loss: 0.7515 - val_mse: 0.7515\n",
      "Epoch 4/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6755 - mse: 0.6755 - val_loss: 0.7342 - val_mse: 0.7342\n",
      "Epoch 5/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6658 - mse: 0.6658 - val_loss: 0.7332 - val_mse: 0.7332\n",
      "Epoch 6/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6590 - mse: 0.6590 - val_loss: 0.7195 - val_mse: 0.7195\n",
      "Epoch 7/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6529 - mse: 0.6529 - val_loss: 0.7196 - val_mse: 0.7196\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6477 - mse: 0.6477 - val_loss: 0.7294 - val_mse: 0.7294\n",
      "Epoch 9/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6434 - mse: 0.6434 - val_loss: 0.7427 - val_mse: 0.7427\n",
      "Epoch 10/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6390 - mse: 0.6390 - val_loss: 0.7231 - val_mse: 0.7231\n",
      "Epoch 11/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6356 - mse: 0.6356 - val_loss: 0.7133 - val_mse: 0.7133\n",
      "Epoch 12/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6319 - mse: 0.6319 - val_loss: 0.7042 - val_mse: 0.7042\n",
      "Epoch 13/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6288 - mse: 0.6288 - val_loss: 0.7141 - val_mse: 0.7141\n",
      "Epoch 14/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6259 - mse: 0.6259 - val_loss: 0.7025 - val_mse: 0.7025\n",
      "Epoch 15/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6232 - mse: 0.6232 - val_loss: 0.6944 - val_mse: 0.6944\n",
      "Epoch 16/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6203 - mse: 0.6203 - val_loss: 0.6993 - val_mse: 0.6993\n",
      "Epoch 17/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6184 - mse: 0.6184 - val_loss: 0.7094 - val_mse: 0.7094\n",
      "Epoch 18/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6160 - mse: 0.6160 - val_loss: 0.7013 - val_mse: 0.7013\n",
      "Epoch 19/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6143 - mse: 0.6143 - val_loss: 0.6959 - val_mse: 0.6959\n",
      "Epoch 20/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6122 - mse: 0.6122 - val_loss: 0.6921 - val_mse: 0.6921\n",
      "Epoch 21/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6099 - mse: 0.6099 - val_loss: 0.6884 - val_mse: 0.6884\n",
      "Epoch 22/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6081 - mse: 0.6081 - val_loss: 0.6839 - val_mse: 0.6839\n",
      "Epoch 23/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6061 - mse: 0.6061 - val_loss: 0.6841 - val_mse: 0.6841\n",
      "Epoch 24/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6045 - mse: 0.6045 - val_loss: 0.6792 - val_mse: 0.6792\n",
      "Epoch 25/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6030 - mse: 0.6030 - val_loss: 0.6844 - val_mse: 0.6844\n",
      "Epoch 26/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6011 - mse: 0.6011 - val_loss: 0.6769 - val_mse: 0.6769\n",
      "Epoch 27/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5998 - mse: 0.5998 - val_loss: 0.6801 - val_mse: 0.6801\n",
      "Epoch 28/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5984 - mse: 0.5984 - val_loss: 0.6785 - val_mse: 0.6785\n",
      "Epoch 29/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5967 - mse: 0.5967 - val_loss: 0.6886 - val_mse: 0.6886\n",
      "Epoch 30/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5956 - mse: 0.5956 - val_loss: 0.6759 - val_mse: 0.6759\n",
      "Epoch 31/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5943 - mse: 0.5943 - val_loss: 0.7079 - val_mse: 0.7079\n",
      "Epoch 32/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5928 - mse: 0.5928 - val_loss: 0.6734 - val_mse: 0.6734\n",
      "Epoch 33/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5917 - mse: 0.5917 - val_loss: 0.6777 - val_mse: 0.6777\n",
      "Epoch 34/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5902 - mse: 0.5902 - val_loss: 0.6727 - val_mse: 0.6727\n",
      "Epoch 35/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5896 - mse: 0.5896 - val_loss: 0.6690 - val_mse: 0.6690\n",
      "Epoch 36/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5882 - mse: 0.5882 - val_loss: 0.6663 - val_mse: 0.6663\n",
      "Epoch 37/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5874 - mse: 0.5874 - val_loss: 0.6699 - val_mse: 0.6699\n",
      "Epoch 38/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5862 - mse: 0.5862 - val_loss: 0.6715 - val_mse: 0.6715\n",
      "Epoch 39/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5850 - mse: 0.5850 - val_loss: 0.6730 - val_mse: 0.6730\n",
      "Epoch 40/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5836 - mse: 0.5836 - val_loss: 0.6641 - val_mse: 0.6641\n",
      "Epoch 41/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5826 - mse: 0.5826 - val_loss: 0.6776 - val_mse: 0.6776\n",
      "Epoch 42/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5814 - mse: 0.5814 - val_loss: 0.6763 - val_mse: 0.6763\n",
      "Epoch 43/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5806 - mse: 0.5806 - val_loss: 0.6759 - val_mse: 0.6759\n",
      "Epoch 44/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5800 - mse: 0.5800 - val_loss: 0.6785 - val_mse: 0.6785\n",
      "Epoch 45/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5787 - mse: 0.5787 - val_loss: 0.6755 - val_mse: 0.6755\n",
      "Epoch 46/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5783 - mse: 0.5783 - val_loss: 0.6759 - val_mse: 0.6759\n",
      "Epoch 47/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5774 - mse: 0.5774 - val_loss: 0.6738 - val_mse: 0.6738\n",
      "Epoch 48/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5765 - mse: 0.5765 - val_loss: 0.6552 - val_mse: 0.6552\n",
      "Epoch 49/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5752 - mse: 0.5752 - val_loss: 0.6681 - val_mse: 0.6681\n",
      "Epoch 50/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5744 - mse: 0.5744 - val_loss: 0.6681 - val_mse: 0.6681\n",
      "Epoch 51/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5735 - mse: 0.5735 - val_loss: 0.6821 - val_mse: 0.6821\n",
      "Epoch 52/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5726 - mse: 0.5726 - val_loss: 0.6779 - val_mse: 0.6779\n",
      "Epoch 53/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5721 - mse: 0.5721 - val_loss: 0.6617 - val_mse: 0.6617\n",
      "Epoch 54/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5710 - mse: 0.5710 - val_loss: 0.6553 - val_mse: 0.6553\n",
      "Epoch 55/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5703 - mse: 0.5703 - val_loss: 0.6610 - val_mse: 0.6610\n",
      "Epoch 56/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5696 - mse: 0.5696 - val_loss: 0.6689 - val_mse: 0.6689\n",
      "Epoch 57/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5684 - mse: 0.5684 - val_loss: 0.6670 - val_mse: 0.6670\n",
      "Epoch 58/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5674 - mse: 0.5674 - val_loss: 0.6732 - val_mse: 0.6732\n",
      "Epoch 59/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5669 - mse: 0.5669 - val_loss: 0.6622 - val_mse: 0.6622\n",
      "Epoch 60/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5659 - mse: 0.5659 - val_loss: 0.6531 - val_mse: 0.6531\n",
      "Epoch 61/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5654 - mse: 0.5654 - val_loss: 0.6886 - val_mse: 0.6886\n",
      "Epoch 62/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5644 - mse: 0.5644 - val_loss: 0.6570 - val_mse: 0.6570\n",
      "Epoch 63/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5636 - mse: 0.5636 - val_loss: 0.6571 - val_mse: 0.6571\n",
      "Epoch 64/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5626 - mse: 0.5626 - val_loss: 0.6670 - val_mse: 0.6670\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5620 - mse: 0.5620 - val_loss: 0.6607 - val_mse: 0.6607\n",
      "Epoch 66/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5612 - mse: 0.5612 - val_loss: 0.6620 - val_mse: 0.6620\n",
      "Epoch 67/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5605 - mse: 0.5605 - val_loss: 0.6686 - val_mse: 0.6686\n",
      "Epoch 68/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5598 - mse: 0.5598 - val_loss: 0.6599 - val_mse: 0.6599\n",
      "Epoch 69/100\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5589 - mse: 0.5589 - val_loss: 0.6642 - val_mse: 0.6642\n",
      "Epoch 70/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5584 - mse: 0.5584 - val_loss: 0.6556 - val_mse: 0.6556\n",
      "Epoch 71/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5575 - mse: 0.5575 - val_loss: 0.6695 - val_mse: 0.6695\n",
      "Epoch 72/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5571 - mse: 0.5571 - val_loss: 0.6566 - val_mse: 0.6566\n",
      "Epoch 73/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5560 - mse: 0.5560 - val_loss: 0.6576 - val_mse: 0.6576\n",
      "Epoch 74/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5555 - mse: 0.5555 - val_loss: 0.6521 - val_mse: 0.6521\n",
      "Epoch 75/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5551 - mse: 0.5551 - val_loss: 0.6658 - val_mse: 0.6658\n",
      "Epoch 76/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5541 - mse: 0.5541 - val_loss: 0.6742 - val_mse: 0.6742\n",
      "Epoch 77/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5532 - mse: 0.5532 - val_loss: 0.6508 - val_mse: 0.6508\n",
      "Epoch 78/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5529 - mse: 0.5529 - val_loss: 0.6562 - val_mse: 0.6562\n",
      "Epoch 79/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5517 - mse: 0.5517 - val_loss: 0.6620 - val_mse: 0.6620\n",
      "Epoch 80/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5518 - mse: 0.5518 - val_loss: 0.6611 - val_mse: 0.6611\n",
      "Epoch 81/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5508 - mse: 0.5508 - val_loss: 0.6664 - val_mse: 0.6664\n",
      "Epoch 82/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5501 - mse: 0.5501 - val_loss: 0.6808 - val_mse: 0.6808\n",
      "Epoch 83/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5502 - mse: 0.5502 - val_loss: 0.6475 - val_mse: 0.6475\n",
      "Epoch 84/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5487 - mse: 0.5487 - val_loss: 0.6635 - val_mse: 0.6635\n",
      "Epoch 85/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5481 - mse: 0.5481 - val_loss: 0.6671 - val_mse: 0.6671\n",
      "Epoch 86/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5476 - mse: 0.5476 - val_loss: 0.6560 - val_mse: 0.6560\n",
      "Epoch 87/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5469 - mse: 0.5469 - val_loss: 0.6735 - val_mse: 0.6735\n",
      "Epoch 88/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5466 - mse: 0.5466 - val_loss: 0.6697 - val_mse: 0.6697\n",
      "Epoch 89/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5459 - mse: 0.5459 - val_loss: 0.6650 - val_mse: 0.6650\n",
      "Epoch 90/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5451 - mse: 0.5451 - val_loss: 0.6693 - val_mse: 0.6693\n",
      "Epoch 91/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5447 - mse: 0.5447 - val_loss: 0.6597 - val_mse: 0.6597\n",
      "Epoch 92/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5443 - mse: 0.5443 - val_loss: 0.6572 - val_mse: 0.6572\n",
      "Epoch 93/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5436 - mse: 0.5436 - val_loss: 0.6638 - val_mse: 0.6638\n",
      "Epoch 94/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5428 - mse: 0.5428 - val_loss: 0.6548 - val_mse: 0.6548\n",
      "Epoch 95/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5428 - mse: 0.5428 - val_loss: 0.6747 - val_mse: 0.6747\n",
      "Epoch 96/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5421 - mse: 0.5421 - val_loss: 0.6578 - val_mse: 0.6578\n",
      "Epoch 97/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5413 - mse: 0.5413 - val_loss: 0.6610 - val_mse: 0.6610\n",
      "Epoch 98/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5409 - mse: 0.5409 - val_loss: 0.6568 - val_mse: 0.6568\n",
      "Epoch 99/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5404 - mse: 0.5404 - val_loss: 0.6791 - val_mse: 0.6791\n",
      "Epoch 100/100\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5400 - mse: 0.5400 - val_loss: 0.6850 - val_mse: 0.6850\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.8671 - mse: 0.8671 - val_loss: 0.8374 - val_mse: 0.8374\n",
      "Epoch 2/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.8066 - mse: 0.8066 - val_loss: 0.8115 - val_mse: 0.8115\n",
      "Epoch 3/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7836 - mse: 0.7836 - val_loss: 0.8012 - val_mse: 0.8012\n",
      "Epoch 4/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7699 - mse: 0.7699 - val_loss: 0.7853 - val_mse: 0.7853\n",
      "Epoch 5/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7603 - mse: 0.7603 - val_loss: 0.7787 - val_mse: 0.7787\n",
      "Epoch 6/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7532 - mse: 0.7533 - val_loss: 0.7552 - val_mse: 0.7552\n",
      "Epoch 7/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7467 - mse: 0.7467 - val_loss: 0.7580 - val_mse: 0.7580\n",
      "Epoch 8/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7410 - mse: 0.7410 - val_loss: 0.7510 - val_mse: 0.7510\n",
      "Epoch 9/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7365 - mse: 0.7365 - val_loss: 0.7507 - val_mse: 0.7507\n",
      "Epoch 10/100\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.7325 - mse: 0.7325 - val_loss: 0.7466 - val_mse: 0.7466\n",
      "Epoch 11/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7280 - mse: 0.7280 - val_loss: 0.7374 - val_mse: 0.7374\n",
      "Epoch 12/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7243 - mse: 0.7243 - val_loss: 0.7515 - val_mse: 0.7515\n",
      "Epoch 13/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7206 - mse: 0.7206 - val_loss: 0.7409 - val_mse: 0.7409\n",
      "Epoch 14/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7170 - mse: 0.7170 - val_loss: 0.7409 - val_mse: 0.7409\n",
      "Epoch 15/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7139 - mse: 0.7139 - val_loss: 0.7498 - val_mse: 0.7498\n",
      "Epoch 16/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7106 - mse: 0.7106 - val_loss: 0.7438 - val_mse: 0.7438\n",
      "Epoch 17/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7075 - mse: 0.7075 - val_loss: 0.7395 - val_mse: 0.7395\n",
      "Epoch 18/100\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.7049 - mse: 0.7049 - val_loss: 0.7492 - val_mse: 0.7492\n",
      "Epoch 19/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7020 - mse: 0.7020 - val_loss: 0.7487 - val_mse: 0.7487\n",
      "Epoch 20/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6994 - mse: 0.6994 - val_loss: 0.7407 - val_mse: 0.7407\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6966 - mse: 0.6966 - val_loss: 0.7453 - val_mse: 0.7453\n",
      "Epoch 22/100\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6940 - mse: 0.6940 - val_loss: 0.7452 - val_mse: 0.7452\n",
      "Epoch 23/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6914 - mse: 0.6914 - val_loss: 0.7610 - val_mse: 0.7610\n",
      "Epoch 24/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6890 - mse: 0.6890 - val_loss: 0.7585 - val_mse: 0.7585\n",
      "Epoch 25/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6868 - mse: 0.6868 - val_loss: 0.7483 - val_mse: 0.7483\n",
      "Epoch 26/100\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6848 - mse: 0.6848 - val_loss: 0.7510 - val_mse: 0.7510\n",
      "Epoch 27/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6824 - mse: 0.6824 - val_loss: 0.7425 - val_mse: 0.7425\n",
      "Epoch 28/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6805 - mse: 0.6805 - val_loss: 0.7559 - val_mse: 0.7559\n",
      "Epoch 29/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6782 - mse: 0.6782 - val_loss: 0.7619 - val_mse: 0.7619\n",
      "Epoch 30/100\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6757 - mse: 0.6757 - val_loss: 0.7795 - val_mse: 0.7795\n",
      "Epoch 31/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6742 - mse: 0.6742 - val_loss: 0.7624 - val_mse: 0.7624\n",
      "Epoch 32/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6727 - mse: 0.6727 - val_loss: 0.7667 - val_mse: 0.7667\n",
      "Epoch 33/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6705 - mse: 0.6705 - val_loss: 0.7686 - val_mse: 0.7686\n",
      "Epoch 34/100\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6688 - mse: 0.6688 - val_loss: 0.7588 - val_mse: 0.7588\n",
      "Epoch 35/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6669 - mse: 0.6669 - val_loss: 0.7694 - val_mse: 0.7694\n",
      "Epoch 36/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6652 - mse: 0.6652 - val_loss: 0.7753 - val_mse: 0.7753\n",
      "Epoch 37/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6635 - mse: 0.6635 - val_loss: 0.7797 - val_mse: 0.7797\n",
      "Epoch 38/100\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6621 - mse: 0.6621 - val_loss: 0.8011 - val_mse: 0.8011\n",
      "Epoch 39/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6603 - mse: 0.6603 - val_loss: 0.7879 - val_mse: 0.7879\n",
      "Epoch 40/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6588 - mse: 0.6588 - val_loss: 0.7779 - val_mse: 0.7779\n",
      "Epoch 41/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6575 - mse: 0.6575 - val_loss: 0.7784 - val_mse: 0.7784\n",
      "Epoch 42/100\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6557 - mse: 0.6557 - val_loss: 0.7896 - val_mse: 0.7896\n",
      "Epoch 43/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6546 - mse: 0.6546 - val_loss: 0.7935 - val_mse: 0.7935\n",
      "Epoch 44/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6530 - mse: 0.6530 - val_loss: 0.7868 - val_mse: 0.7868\n",
      "Epoch 45/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6516 - mse: 0.6516 - val_loss: 0.7950 - val_mse: 0.7950\n",
      "Epoch 46/100\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6502 - mse: 0.6502 - val_loss: 0.8193 - val_mse: 0.8193\n",
      "Epoch 47/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6487 - mse: 0.6487 - val_loss: 0.7884 - val_mse: 0.7884\n",
      "Epoch 48/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6478 - mse: 0.6478 - val_loss: 0.8405 - val_mse: 0.8405\n",
      "Epoch 49/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6462 - mse: 0.6462 - val_loss: 0.8543 - val_mse: 0.8543\n",
      "Epoch 50/100\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6451 - mse: 0.6451 - val_loss: 0.8046 - val_mse: 0.8046\n",
      "Epoch 51/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6438 - mse: 0.6438 - val_loss: 0.8199 - val_mse: 0.8199\n",
      "Epoch 52/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6424 - mse: 0.6424 - val_loss: 0.7856 - val_mse: 0.7856\n",
      "Epoch 53/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6414 - mse: 0.6414 - val_loss: 0.8491 - val_mse: 0.8491\n",
      "Epoch 54/100\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6403 - mse: 0.6403 - val_loss: 0.8004 - val_mse: 0.8004\n",
      "Epoch 55/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6393 - mse: 0.6393 - val_loss: 0.8172 - val_mse: 0.8172\n",
      "Epoch 56/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6380 - mse: 0.6380 - val_loss: 0.8173 - val_mse: 0.8173\n",
      "Epoch 57/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6369 - mse: 0.6369 - val_loss: 0.8364 - val_mse: 0.8364\n",
      "Epoch 58/100\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6358 - mse: 0.6358 - val_loss: 0.8246 - val_mse: 0.8246\n",
      "Epoch 59/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6344 - mse: 0.6344 - val_loss: 0.8798 - val_mse: 0.8798\n",
      "Epoch 60/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6338 - mse: 0.6338 - val_loss: 0.8748 - val_mse: 0.8748\n",
      "Epoch 61/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6324 - mse: 0.6324 - val_loss: 0.8870 - val_mse: 0.8870\n",
      "Epoch 62/100\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6318 - mse: 0.6318 - val_loss: 0.8893 - val_mse: 0.8893\n",
      "Epoch 63/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6309 - mse: 0.6309 - val_loss: 0.8730 - val_mse: 0.8730\n",
      "Epoch 64/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6302 - mse: 0.6302 - val_loss: 0.8522 - val_mse: 0.8522\n",
      "Epoch 65/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6289 - mse: 0.6289 - val_loss: 0.8350 - val_mse: 0.8350\n",
      "Epoch 66/100\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6280 - mse: 0.6280 - val_loss: 0.9351 - val_mse: 0.9351\n",
      "Epoch 67/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6272 - mse: 0.6272 - val_loss: 0.8553 - val_mse: 0.8553\n",
      "Epoch 68/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6261 - mse: 0.6261 - val_loss: 0.9627 - val_mse: 0.9627\n",
      "Epoch 69/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6253 - mse: 0.6253 - val_loss: 0.9042 - val_mse: 0.9042\n",
      "Epoch 70/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6246 - mse: 0.6246 - val_loss: 0.8681 - val_mse: 0.8681\n",
      "Epoch 71/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6239 - mse: 0.6239 - val_loss: 0.9323 - val_mse: 0.9323\n",
      "Epoch 72/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6230 - mse: 0.6230 - val_loss: 0.9057 - val_mse: 0.9057\n",
      "Epoch 73/100\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6225 - mse: 0.6225 - val_loss: 0.9405 - val_mse: 0.9405\n",
      "Epoch 74/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6217 - mse: 0.6217 - val_loss: 0.9462 - val_mse: 0.9462\n",
      "Epoch 75/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6205 - mse: 0.6205 - val_loss: 1.0038 - val_mse: 1.0038\n",
      "Epoch 76/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6199 - mse: 0.6199 - val_loss: 0.9183 - val_mse: 0.9183\n",
      "Epoch 77/100\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6193 - mse: 0.6193 - val_loss: 0.9971 - val_mse: 0.9971\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6186 - mse: 0.6186 - val_loss: 0.9361 - val_mse: 0.9361\n",
      "Epoch 79/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6172 - mse: 0.6172 - val_loss: 0.9134 - val_mse: 0.9134\n",
      "Epoch 80/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6171 - mse: 0.6171 - val_loss: 0.8872 - val_mse: 0.8872\n",
      "Epoch 81/100\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6166 - mse: 0.6166 - val_loss: 1.0139 - val_mse: 1.0139\n",
      "Epoch 82/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6158 - mse: 0.6158 - val_loss: 0.9432 - val_mse: 0.9432\n",
      "Epoch 83/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6150 - mse: 0.6150 - val_loss: 0.9915 - val_mse: 0.9915\n",
      "Epoch 84/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6148 - mse: 0.6148 - val_loss: 0.9178 - val_mse: 0.9178\n",
      "Epoch 85/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6139 - mse: 0.6139 - val_loss: 1.0275 - val_mse: 1.0275\n",
      "Epoch 86/100\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6131 - mse: 0.6131 - val_loss: 1.0355 - val_mse: 1.0355\n",
      "Epoch 87/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6124 - mse: 0.6124 - val_loss: 1.0419 - val_mse: 1.0419\n",
      "Epoch 88/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6122 - mse: 0.6122 - val_loss: 0.9789 - val_mse: 0.9789\n",
      "Epoch 89/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6111 - mse: 0.6111 - val_loss: 0.9315 - val_mse: 0.9315\n",
      "Epoch 90/100\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6105 - mse: 0.6105 - val_loss: 1.1411 - val_mse: 1.1411\n",
      "Epoch 91/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6100 - mse: 0.6100 - val_loss: 1.0177 - val_mse: 1.0177\n",
      "Epoch 92/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6093 - mse: 0.6093 - val_loss: 1.0269 - val_mse: 1.0269\n",
      "Epoch 93/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6092 - mse: 0.6092 - val_loss: 1.0355 - val_mse: 1.0355\n",
      "Epoch 94/100\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6085 - mse: 0.6085 - val_loss: 1.1267 - val_mse: 1.1267\n",
      "Epoch 95/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6079 - mse: 0.6079 - val_loss: 1.1475 - val_mse: 1.1475\n",
      "Epoch 96/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6075 - mse: 0.6075 - val_loss: 1.0167 - val_mse: 1.0167\n",
      "Epoch 97/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6067 - mse: 0.6067 - val_loss: 1.1625 - val_mse: 1.1625\n",
      "Epoch 98/100\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6065 - mse: 0.6065 - val_loss: 1.0637 - val_mse: 1.0637\n",
      "Epoch 99/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6061 - mse: 0.6061 - val_loss: 1.1233 - val_mse: 1.1233\n",
      "Epoch 100/100\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6050 - mse: 0.6050 - val_loss: 1.0268 - val_mse: 1.0268\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.8450 - mse: 0.8450 - val_loss: 0.7816 - val_mse: 0.7816\n",
      "Epoch 2/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7889 - mse: 0.7889 - val_loss: 0.7572 - val_mse: 0.7572\n",
      "Epoch 3/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7616 - mse: 0.7616 - val_loss: 0.7655 - val_mse: 0.7655\n",
      "Epoch 4/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7439 - mse: 0.7439 - val_loss: 0.7320 - val_mse: 0.7320\n",
      "Epoch 5/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7324 - mse: 0.7324 - val_loss: 0.7330 - val_mse: 0.7330\n",
      "Epoch 6/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7244 - mse: 0.7244 - val_loss: 0.7209 - val_mse: 0.7209\n",
      "Epoch 7/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7173 - mse: 0.7173 - val_loss: 0.7219 - val_mse: 0.7219\n",
      "Epoch 8/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7121 - mse: 0.7121 - val_loss: 0.7219 - val_mse: 0.7219\n",
      "Epoch 9/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7062 - mse: 0.7062 - val_loss: 0.7078 - val_mse: 0.7078\n",
      "Epoch 10/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7021 - mse: 0.7021 - val_loss: 0.7088 - val_mse: 0.7088\n",
      "Epoch 11/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6968 - mse: 0.6968 - val_loss: 0.6962 - val_mse: 0.6962\n",
      "Epoch 12/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6929 - mse: 0.6929 - val_loss: 0.7005 - val_mse: 0.7005\n",
      "Epoch 13/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6886 - mse: 0.6886 - val_loss: 0.6929 - val_mse: 0.6929\n",
      "Epoch 14/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6846 - mse: 0.6846 - val_loss: 0.6939 - val_mse: 0.6939\n",
      "Epoch 15/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6815 - mse: 0.6815 - val_loss: 0.6905 - val_mse: 0.6905\n",
      "Epoch 16/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6778 - mse: 0.6778 - val_loss: 0.6911 - val_mse: 0.6911\n",
      "Epoch 17/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6744 - mse: 0.6744 - val_loss: 0.6803 - val_mse: 0.6803\n",
      "Epoch 18/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6716 - mse: 0.6716 - val_loss: 0.7042 - val_mse: 0.7042\n",
      "Epoch 19/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6684 - mse: 0.6684 - val_loss: 0.6929 - val_mse: 0.6929\n",
      "Epoch 20/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6655 - mse: 0.6655 - val_loss: 0.6802 - val_mse: 0.6802\n",
      "Epoch 21/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6634 - mse: 0.6634 - val_loss: 0.6857 - val_mse: 0.6857\n",
      "Epoch 22/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6606 - mse: 0.6606 - val_loss: 0.7062 - val_mse: 0.7062\n",
      "Epoch 23/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6580 - mse: 0.6580 - val_loss: 0.6863 - val_mse: 0.6863\n",
      "Epoch 24/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6557 - mse: 0.6557 - val_loss: 0.6850 - val_mse: 0.6850\n",
      "Epoch 25/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6537 - mse: 0.6537 - val_loss: 0.6891 - val_mse: 0.6891\n",
      "Epoch 26/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6510 - mse: 0.6510 - val_loss: 0.6770 - val_mse: 0.6770\n",
      "Epoch 27/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6491 - mse: 0.6491 - val_loss: 0.6930 - val_mse: 0.6930\n",
      "Epoch 28/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6470 - mse: 0.6470 - val_loss: 0.6702 - val_mse: 0.6702\n",
      "Epoch 29/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6452 - mse: 0.6452 - val_loss: 0.6874 - val_mse: 0.6874\n",
      "Epoch 30/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6433 - mse: 0.6433 - val_loss: 0.6776 - val_mse: 0.6776\n",
      "Epoch 31/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6413 - mse: 0.6413 - val_loss: 0.6769 - val_mse: 0.6769\n",
      "Epoch 32/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6395 - mse: 0.6395 - val_loss: 0.7263 - val_mse: 0.7263\n",
      "Epoch 33/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6377 - mse: 0.6377 - val_loss: 0.6722 - val_mse: 0.6722\n",
      "Epoch 34/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6363 - mse: 0.6363 - val_loss: 0.6810 - val_mse: 0.6810\n",
      "Epoch 35/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6348 - mse: 0.6348 - val_loss: 0.6861 - val_mse: 0.6861\n",
      "Epoch 36/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6333 - mse: 0.6333 - val_loss: 0.6876 - val_mse: 0.6876\n",
      "Epoch 37/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6318 - mse: 0.6318 - val_loss: 0.6650 - val_mse: 0.6650\n",
      "Epoch 38/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6305 - mse: 0.6305 - val_loss: 0.6708 - val_mse: 0.6708\n",
      "Epoch 39/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6288 - mse: 0.6288 - val_loss: 0.6739 - val_mse: 0.6739\n",
      "Epoch 40/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6270 - mse: 0.6270 - val_loss: 0.6720 - val_mse: 0.6720\n",
      "Epoch 41/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6256 - mse: 0.6256 - val_loss: 0.6756 - val_mse: 0.6756\n",
      "Epoch 42/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6243 - mse: 0.6243 - val_loss: 0.6892 - val_mse: 0.6892\n",
      "Epoch 43/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6235 - mse: 0.6235 - val_loss: 0.6692 - val_mse: 0.6692\n",
      "Epoch 44/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6215 - mse: 0.6215 - val_loss: 0.6739 - val_mse: 0.6739\n",
      "Epoch 45/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6206 - mse: 0.6206 - val_loss: 0.6657 - val_mse: 0.6657\n",
      "Epoch 46/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6192 - mse: 0.6192 - val_loss: 0.6731 - val_mse: 0.6731\n",
      "Epoch 47/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6181 - mse: 0.6181 - val_loss: 0.6817 - val_mse: 0.6817\n",
      "Epoch 48/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6170 - mse: 0.6170 - val_loss: 0.7254 - val_mse: 0.7254\n",
      "Epoch 49/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6157 - mse: 0.6157 - val_loss: 0.6664 - val_mse: 0.6664\n",
      "Epoch 50/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6146 - mse: 0.6146 - val_loss: 0.6804 - val_mse: 0.6804\n",
      "Epoch 51/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6138 - mse: 0.6138 - val_loss: 0.6797 - val_mse: 0.6797\n",
      "Epoch 52/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6119 - mse: 0.6119 - val_loss: 0.6793 - val_mse: 0.6793\n",
      "Epoch 53/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6112 - mse: 0.6112 - val_loss: 0.6602 - val_mse: 0.6602\n",
      "Epoch 54/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6104 - mse: 0.6104 - val_loss: 0.6577 - val_mse: 0.6577\n",
      "Epoch 55/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6088 - mse: 0.6088 - val_loss: 0.6637 - val_mse: 0.6637\n",
      "Epoch 56/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6080 - mse: 0.6080 - val_loss: 0.7069 - val_mse: 0.7069\n",
      "Epoch 57/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6072 - mse: 0.6072 - val_loss: 0.6861 - val_mse: 0.6861\n",
      "Epoch 58/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6064 - mse: 0.6064 - val_loss: 0.6752 - val_mse: 0.6752\n",
      "Epoch 59/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6054 - mse: 0.6054 - val_loss: 0.6687 - val_mse: 0.6687\n",
      "Epoch 60/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6043 - mse: 0.6043 - val_loss: 0.6519 - val_mse: 0.6519\n",
      "Epoch 61/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6035 - mse: 0.6035 - val_loss: 0.6781 - val_mse: 0.6781\n",
      "Epoch 62/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6028 - mse: 0.6028 - val_loss: 0.6844 - val_mse: 0.6844\n",
      "Epoch 63/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6017 - mse: 0.6017 - val_loss: 0.6823 - val_mse: 0.6823\n",
      "Epoch 64/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6009 - mse: 0.6009 - val_loss: 0.6541 - val_mse: 0.6541\n",
      "Epoch 65/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5999 - mse: 0.5999 - val_loss: 0.6761 - val_mse: 0.6761\n",
      "Epoch 66/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5995 - mse: 0.5995 - val_loss: 0.6815 - val_mse: 0.6815\n",
      "Epoch 67/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5983 - mse: 0.5983 - val_loss: 0.6657 - val_mse: 0.6657\n",
      "Epoch 68/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5975 - mse: 0.5975 - val_loss: 0.6868 - val_mse: 0.6868\n",
      "Epoch 69/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5966 - mse: 0.5966 - val_loss: 0.7073 - val_mse: 0.7073\n",
      "Epoch 70/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5958 - mse: 0.5958 - val_loss: 0.6585 - val_mse: 0.6585\n",
      "Epoch 71/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5950 - mse: 0.5950 - val_loss: 0.6862 - val_mse: 0.6862\n",
      "Epoch 72/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5944 - mse: 0.5944 - val_loss: 0.6648 - val_mse: 0.6648\n",
      "Epoch 73/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5939 - mse: 0.5939 - val_loss: 0.6475 - val_mse: 0.6475\n",
      "Epoch 74/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5931 - mse: 0.5931 - val_loss: 0.6697 - val_mse: 0.6697\n",
      "Epoch 75/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5924 - mse: 0.5924 - val_loss: 0.6502 - val_mse: 0.6502\n",
      "Epoch 76/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5915 - mse: 0.5915 - val_loss: 0.6639 - val_mse: 0.6639\n",
      "Epoch 77/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5909 - mse: 0.5909 - val_loss: 0.6816 - val_mse: 0.6816\n",
      "Epoch 78/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5906 - mse: 0.5906 - val_loss: 0.6727 - val_mse: 0.6727\n",
      "Epoch 79/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5899 - mse: 0.5899 - val_loss: 0.6667 - val_mse: 0.6667\n",
      "Epoch 80/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5890 - mse: 0.5890 - val_loss: 0.6927 - val_mse: 0.6927\n",
      "Epoch 81/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5882 - mse: 0.5882 - val_loss: 0.6949 - val_mse: 0.6949\n",
      "Epoch 82/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5878 - mse: 0.5878 - val_loss: 0.6642 - val_mse: 0.6642\n",
      "Epoch 83/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5870 - mse: 0.5870 - val_loss: 0.6762 - val_mse: 0.6762\n",
      "Epoch 84/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5863 - mse: 0.5863 - val_loss: 0.6622 - val_mse: 0.6622\n",
      "Epoch 85/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5862 - mse: 0.5862 - val_loss: 0.6810 - val_mse: 0.6810\n",
      "Epoch 86/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5852 - mse: 0.5852 - val_loss: 0.6598 - val_mse: 0.6598\n",
      "Epoch 87/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5844 - mse: 0.5844 - val_loss: 0.6828 - val_mse: 0.6828\n",
      "Epoch 88/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5843 - mse: 0.5843 - val_loss: 0.6686 - val_mse: 0.6686\n",
      "Epoch 89/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5836 - mse: 0.5836 - val_loss: 0.6658 - val_mse: 0.6658\n",
      "Epoch 90/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5832 - mse: 0.5832 - val_loss: 0.6775 - val_mse: 0.6775\n",
      "Epoch 91/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5827 - mse: 0.5827 - val_loss: 0.6729 - val_mse: 0.6729\n",
      "Epoch 92/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5818 - mse: 0.5818 - val_loss: 0.6780 - val_mse: 0.6780\n",
      "Epoch 93/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5815 - mse: 0.5815 - val_loss: 0.6685 - val_mse: 0.6685\n",
      "Epoch 94/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5811 - mse: 0.5811 - val_loss: 0.6802 - val_mse: 0.6802\n",
      "Epoch 95/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5805 - mse: 0.5805 - val_loss: 0.6598 - val_mse: 0.6598\n",
      "Epoch 96/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5799 - mse: 0.5799 - val_loss: 0.6931 - val_mse: 0.6931\n",
      "Epoch 97/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5790 - mse: 0.5790 - val_loss: 0.6734 - val_mse: 0.6734\n",
      "Epoch 98/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5787 - mse: 0.5787 - val_loss: 0.6607 - val_mse: 0.6607\n",
      "Epoch 99/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5781 - mse: 0.5781 - val_loss: 0.6761 - val_mse: 0.6761\n",
      "Epoch 100/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5776 - mse: 0.5776 - val_loss: 0.6899 - val_mse: 0.6899\n",
      "Epoch 101/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5768 - mse: 0.5768 - val_loss: 0.6553 - val_mse: 0.6553\n",
      "Epoch 102/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5768 - mse: 0.5768 - val_loss: 0.6974 - val_mse: 0.6974\n",
      "Epoch 103/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5761 - mse: 0.5761 - val_loss: 0.6658 - val_mse: 0.6658\n",
      "Epoch 104/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5759 - mse: 0.5759 - val_loss: 0.6842 - val_mse: 0.6842\n",
      "Epoch 105/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5757 - mse: 0.5757 - val_loss: 0.6614 - val_mse: 0.6614\n",
      "Epoch 106/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5752 - mse: 0.5752 - val_loss: 0.6629 - val_mse: 0.6629\n",
      "Epoch 107/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5746 - mse: 0.5746 - val_loss: 0.6752 - val_mse: 0.6752\n",
      "Epoch 108/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5737 - mse: 0.5737 - val_loss: 0.6821 - val_mse: 0.6821\n",
      "Epoch 109/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5734 - mse: 0.5734 - val_loss: 0.6701 - val_mse: 0.6701\n",
      "Epoch 110/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5732 - mse: 0.5732 - val_loss: 0.7006 - val_mse: 0.7006\n",
      "Epoch 111/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5729 - mse: 0.5729 - val_loss: 0.6959 - val_mse: 0.6959\n",
      "Epoch 112/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5724 - mse: 0.5724 - val_loss: 0.6626 - val_mse: 0.6626\n",
      "Epoch 113/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5716 - mse: 0.5716 - val_loss: 0.6679 - val_mse: 0.6679\n",
      "Epoch 114/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5709 - mse: 0.5709 - val_loss: 0.6822 - val_mse: 0.6822\n",
      "Epoch 115/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5709 - mse: 0.5709 - val_loss: 0.6675 - val_mse: 0.6675\n",
      "Epoch 116/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5703 - mse: 0.5703 - val_loss: 0.6612 - val_mse: 0.6612\n",
      "Epoch 117/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5699 - mse: 0.5699 - val_loss: 0.7012 - val_mse: 0.7012\n",
      "Epoch 118/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5698 - mse: 0.5698 - val_loss: 0.6597 - val_mse: 0.6597\n",
      "Epoch 119/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5697 - mse: 0.5697 - val_loss: 0.6657 - val_mse: 0.6657\n",
      "Epoch 120/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5689 - mse: 0.5689 - val_loss: 0.6729 - val_mse: 0.6729\n",
      "Epoch 121/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5684 - mse: 0.5684 - val_loss: 0.6739 - val_mse: 0.6739\n",
      "Epoch 122/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5679 - mse: 0.5679 - val_loss: 0.6902 - val_mse: 0.6902\n",
      "Epoch 123/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5679 - mse: 0.5679 - val_loss: 0.6568 - val_mse: 0.6568\n",
      "Epoch 124/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5678 - mse: 0.5678 - val_loss: 0.6745 - val_mse: 0.6745\n",
      "Epoch 125/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5671 - mse: 0.5671 - val_loss: 0.6868 - val_mse: 0.6868\n",
      "Epoch 126/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5668 - mse: 0.5668 - val_loss: 0.6819 - val_mse: 0.6819\n",
      "Epoch 127/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5663 - mse: 0.5663 - val_loss: 0.6915 - val_mse: 0.6915\n",
      "Epoch 128/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5660 - mse: 0.5660 - val_loss: 0.6680 - val_mse: 0.6680\n",
      "Epoch 129/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5653 - mse: 0.5653 - val_loss: 0.6581 - val_mse: 0.6581\n",
      "Epoch 130/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5653 - mse: 0.5653 - val_loss: 0.6598 - val_mse: 0.6598\n",
      "Epoch 131/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5651 - mse: 0.5651 - val_loss: 0.6898 - val_mse: 0.6898\n",
      "Epoch 132/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5645 - mse: 0.5645 - val_loss: 0.6704 - val_mse: 0.6704\n",
      "Epoch 133/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5643 - mse: 0.5643 - val_loss: 0.6565 - val_mse: 0.6565\n",
      "Epoch 134/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5643 - mse: 0.5643 - val_loss: 0.6733 - val_mse: 0.6733\n",
      "Epoch 135/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5638 - mse: 0.5638 - val_loss: 0.6804 - val_mse: 0.6804\n",
      "Epoch 136/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5632 - mse: 0.5632 - val_loss: 0.6766 - val_mse: 0.6766\n",
      "Epoch 137/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5627 - mse: 0.5627 - val_loss: 0.6726 - val_mse: 0.6726\n",
      "Epoch 138/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5624 - mse: 0.5624 - val_loss: 0.7086 - val_mse: 0.7086\n",
      "Epoch 139/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5622 - mse: 0.5622 - val_loss: 0.6844 - val_mse: 0.6844\n",
      "Epoch 140/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5623 - mse: 0.5623 - val_loss: 0.6660 - val_mse: 0.6660\n",
      "Epoch 141/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5621 - mse: 0.5621 - val_loss: 0.6616 - val_mse: 0.6616\n",
      "Epoch 142/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5615 - mse: 0.5615 - val_loss: 0.6798 - val_mse: 0.6798\n",
      "Epoch 143/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5610 - mse: 0.5610 - val_loss: 0.6804 - val_mse: 0.6804\n",
      "Epoch 144/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5607 - mse: 0.5607 - val_loss: 0.6868 - val_mse: 0.6868\n",
      "Epoch 145/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5602 - mse: 0.5602 - val_loss: 0.6647 - val_mse: 0.6647\n",
      "Epoch 146/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5600 - mse: 0.5600 - val_loss: 0.6712 - val_mse: 0.6712\n",
      "Epoch 147/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5599 - mse: 0.5599 - val_loss: 0.6900 - val_mse: 0.6900\n",
      "Epoch 148/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5595 - mse: 0.5595 - val_loss: 0.6792 - val_mse: 0.6792\n",
      "Epoch 149/150\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5590 - mse: 0.5590 - val_loss: 0.6581 - val_mse: 0.6581\n",
      "Epoch 150/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5587 - mse: 0.5587 - val_loss: 0.7170 - val_mse: 0.7170\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7480 - mse: 0.7480 - val_loss: 0.7622 - val_mse: 0.7622\n",
      "Epoch 2/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7067 - mse: 0.7067 - val_loss: 0.7492 - val_mse: 0.7492\n",
      "Epoch 3/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6881 - mse: 0.6881 - val_loss: 0.7466 - val_mse: 0.7466\n",
      "Epoch 4/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6752 - mse: 0.6752 - val_loss: 0.7386 - val_mse: 0.7386\n",
      "Epoch 5/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6665 - mse: 0.6665 - val_loss: 0.7363 - val_mse: 0.7363\n",
      "Epoch 6/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6598 - mse: 0.6598 - val_loss: 0.7239 - val_mse: 0.7239\n",
      "Epoch 7/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6534 - mse: 0.6534 - val_loss: 0.7244 - val_mse: 0.7244\n",
      "Epoch 8/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6486 - mse: 0.6486 - val_loss: 0.7390 - val_mse: 0.7390\n",
      "Epoch 9/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6438 - mse: 0.6438 - val_loss: 0.7124 - val_mse: 0.7124\n",
      "Epoch 10/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6398 - mse: 0.6398 - val_loss: 0.7116 - val_mse: 0.7116\n",
      "Epoch 11/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6356 - mse: 0.6356 - val_loss: 0.7017 - val_mse: 0.7017\n",
      "Epoch 12/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6320 - mse: 0.6320 - val_loss: 0.7090 - val_mse: 0.7090\n",
      "Epoch 13/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6285 - mse: 0.6285 - val_loss: 0.6997 - val_mse: 0.6997\n",
      "Epoch 14/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6257 - mse: 0.6257 - val_loss: 0.6950 - val_mse: 0.6950\n",
      "Epoch 15/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6231 - mse: 0.6231 - val_loss: 0.6996 - val_mse: 0.6996\n",
      "Epoch 16/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6203 - mse: 0.6203 - val_loss: 0.6974 - val_mse: 0.6974\n",
      "Epoch 17/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6178 - mse: 0.6178 - val_loss: 0.6917 - val_mse: 0.6917\n",
      "Epoch 18/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6159 - mse: 0.6159 - val_loss: 0.6963 - val_mse: 0.6963\n",
      "Epoch 19/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6139 - mse: 0.6139 - val_loss: 0.6903 - val_mse: 0.6903\n",
      "Epoch 20/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6118 - mse: 0.6118 - val_loss: 0.6917 - val_mse: 0.6917\n",
      "Epoch 21/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6096 - mse: 0.6096 - val_loss: 0.6874 - val_mse: 0.6874\n",
      "Epoch 22/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6074 - mse: 0.6074 - val_loss: 0.6902 - val_mse: 0.6902\n",
      "Epoch 23/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6061 - mse: 0.6061 - val_loss: 0.6881 - val_mse: 0.6881\n",
      "Epoch 24/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6045 - mse: 0.6045 - val_loss: 0.6815 - val_mse: 0.6815\n",
      "Epoch 25/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6028 - mse: 0.6028 - val_loss: 0.7068 - val_mse: 0.7068\n",
      "Epoch 26/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6009 - mse: 0.6009 - val_loss: 0.6844 - val_mse: 0.6844\n",
      "Epoch 27/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5995 - mse: 0.5995 - val_loss: 0.6757 - val_mse: 0.6757\n",
      "Epoch 28/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5981 - mse: 0.5981 - val_loss: 0.6739 - val_mse: 0.6739\n",
      "Epoch 29/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5969 - mse: 0.5969 - val_loss: 0.6887 - val_mse: 0.6887\n",
      "Epoch 30/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5956 - mse: 0.5956 - val_loss: 0.6821 - val_mse: 0.6821\n",
      "Epoch 31/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5945 - mse: 0.5945 - val_loss: 0.6857 - val_mse: 0.6857\n",
      "Epoch 32/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5931 - mse: 0.5931 - val_loss: 0.6822 - val_mse: 0.6822\n",
      "Epoch 33/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5922 - mse: 0.5922 - val_loss: 0.6827 - val_mse: 0.6827\n",
      "Epoch 34/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5910 - mse: 0.5910 - val_loss: 0.6771 - val_mse: 0.6771\n",
      "Epoch 35/150\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5899 - mse: 0.5899 - val_loss: 0.6754 - val_mse: 0.6754\n",
      "Epoch 36/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5885 - mse: 0.5885 - val_loss: 0.6814 - val_mse: 0.6814\n",
      "Epoch 37/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5875 - mse: 0.5875 - val_loss: 0.6822 - val_mse: 0.6822\n",
      "Epoch 38/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5861 - mse: 0.5861 - val_loss: 0.6759 - val_mse: 0.6759\n",
      "Epoch 39/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5851 - mse: 0.5851 - val_loss: 0.6841 - val_mse: 0.6841\n",
      "Epoch 40/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5842 - mse: 0.5842 - val_loss: 0.6836 - val_mse: 0.6836\n",
      "Epoch 41/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5831 - mse: 0.5831 - val_loss: 0.6657 - val_mse: 0.6657\n",
      "Epoch 42/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5822 - mse: 0.5822 - val_loss: 0.6850 - val_mse: 0.6850\n",
      "Epoch 43/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5814 - mse: 0.5814 - val_loss: 0.6740 - val_mse: 0.6740\n",
      "Epoch 44/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5801 - mse: 0.5801 - val_loss: 0.6770 - val_mse: 0.6770\n",
      "Epoch 45/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5791 - mse: 0.5791 - val_loss: 0.6844 - val_mse: 0.6844\n",
      "Epoch 46/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5782 - mse: 0.5782 - val_loss: 0.6804 - val_mse: 0.6804\n",
      "Epoch 47/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5772 - mse: 0.5772 - val_loss: 0.6781 - val_mse: 0.6781\n",
      "Epoch 48/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5762 - mse: 0.5762 - val_loss: 0.6691 - val_mse: 0.6691\n",
      "Epoch 49/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5751 - mse: 0.5751 - val_loss: 0.6744 - val_mse: 0.6744\n",
      "Epoch 50/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5747 - mse: 0.5747 - val_loss: 0.6626 - val_mse: 0.6626\n",
      "Epoch 51/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5737 - mse: 0.5737 - val_loss: 0.6603 - val_mse: 0.6603\n",
      "Epoch 52/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5724 - mse: 0.5724 - val_loss: 0.6609 - val_mse: 0.6609\n",
      "Epoch 53/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5715 - mse: 0.5715 - val_loss: 0.6671 - val_mse: 0.6671\n",
      "Epoch 54/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5709 - mse: 0.5709 - val_loss: 0.6721 - val_mse: 0.6721\n",
      "Epoch 55/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5699 - mse: 0.5699 - val_loss: 0.6855 - val_mse: 0.6855\n",
      "Epoch 56/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5690 - mse: 0.5690 - val_loss: 0.6712 - val_mse: 0.6712\n",
      "Epoch 57/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5682 - mse: 0.5682 - val_loss: 0.6785 - val_mse: 0.6785\n",
      "Epoch 58/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5671 - mse: 0.5671 - val_loss: 0.6666 - val_mse: 0.6666\n",
      "Epoch 59/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5662 - mse: 0.5662 - val_loss: 0.6619 - val_mse: 0.6619\n",
      "Epoch 60/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5656 - mse: 0.5656 - val_loss: 0.6709 - val_mse: 0.6709\n",
      "Epoch 61/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5647 - mse: 0.5647 - val_loss: 0.6738 - val_mse: 0.6738\n",
      "Epoch 62/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5642 - mse: 0.5642 - val_loss: 0.6626 - val_mse: 0.6626\n",
      "Epoch 63/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5630 - mse: 0.5630 - val_loss: 0.6601 - val_mse: 0.6601\n",
      "Epoch 64/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5618 - mse: 0.5618 - val_loss: 0.6717 - val_mse: 0.6717\n",
      "Epoch 65/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5611 - mse: 0.5611 - val_loss: 0.6759 - val_mse: 0.6759\n",
      "Epoch 66/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5605 - mse: 0.5605 - val_loss: 0.6801 - val_mse: 0.6801\n",
      "Epoch 67/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5595 - mse: 0.5595 - val_loss: 0.6668 - val_mse: 0.6668\n",
      "Epoch 68/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5590 - mse: 0.5590 - val_loss: 0.6567 - val_mse: 0.6567\n",
      "Epoch 69/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5580 - mse: 0.5580 - val_loss: 0.6600 - val_mse: 0.6600\n",
      "Epoch 70/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5574 - mse: 0.5574 - val_loss: 0.6577 - val_mse: 0.6577\n",
      "Epoch 71/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5568 - mse: 0.5568 - val_loss: 0.6536 - val_mse: 0.6536\n",
      "Epoch 72/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5557 - mse: 0.5557 - val_loss: 0.6502 - val_mse: 0.6502\n",
      "Epoch 73/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5553 - mse: 0.5553 - val_loss: 0.6578 - val_mse: 0.6578\n",
      "Epoch 74/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5547 - mse: 0.5547 - val_loss: 0.6715 - val_mse: 0.6715\n",
      "Epoch 75/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5540 - mse: 0.5540 - val_loss: 0.6588 - val_mse: 0.6588\n",
      "Epoch 76/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5531 - mse: 0.5531 - val_loss: 0.6472 - val_mse: 0.6472\n",
      "Epoch 77/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5526 - mse: 0.5526 - val_loss: 0.6553 - val_mse: 0.6553\n",
      "Epoch 78/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5517 - mse: 0.5517 - val_loss: 0.6665 - val_mse: 0.6665\n",
      "Epoch 79/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5512 - mse: 0.5512 - val_loss: 0.6653 - val_mse: 0.6653\n",
      "Epoch 80/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5505 - mse: 0.5505 - val_loss: 0.6723 - val_mse: 0.6723\n",
      "Epoch 81/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5498 - mse: 0.5498 - val_loss: 0.6606 - val_mse: 0.6606\n",
      "Epoch 82/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5493 - mse: 0.5493 - val_loss: 0.6714 - val_mse: 0.6714\n",
      "Epoch 83/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5486 - mse: 0.5486 - val_loss: 0.6521 - val_mse: 0.6521\n",
      "Epoch 84/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5481 - mse: 0.5481 - val_loss: 0.6651 - val_mse: 0.6651\n",
      "Epoch 85/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5474 - mse: 0.5474 - val_loss: 0.6445 - val_mse: 0.6445\n",
      "Epoch 86/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5469 - mse: 0.5469 - val_loss: 0.6469 - val_mse: 0.6469\n",
      "Epoch 87/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5464 - mse: 0.5464 - val_loss: 0.6561 - val_mse: 0.6561\n",
      "Epoch 88/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5460 - mse: 0.5460 - val_loss: 0.6691 - val_mse: 0.6691\n",
      "Epoch 89/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5451 - mse: 0.5451 - val_loss: 0.6761 - val_mse: 0.6761\n",
      "Epoch 90/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5449 - mse: 0.5449 - val_loss: 0.6521 - val_mse: 0.6521\n",
      "Epoch 91/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5439 - mse: 0.5439 - val_loss: 0.6517 - val_mse: 0.6517\n",
      "Epoch 92/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5435 - mse: 0.5435 - val_loss: 0.6558 - val_mse: 0.6558\n",
      "Epoch 93/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5428 - mse: 0.5428 - val_loss: 0.6582 - val_mse: 0.6582\n",
      "Epoch 94/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5425 - mse: 0.5425 - val_loss: 0.6595 - val_mse: 0.6595\n",
      "Epoch 95/150\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5421 - mse: 0.5421 - val_loss: 0.6628 - val_mse: 0.6628\n",
      "Epoch 96/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5413 - mse: 0.5413 - val_loss: 0.6537 - val_mse: 0.6537\n",
      "Epoch 97/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5409 - mse: 0.5409 - val_loss: 0.6459 - val_mse: 0.6459\n",
      "Epoch 98/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5401 - mse: 0.5401 - val_loss: 0.6650 - val_mse: 0.6650\n",
      "Epoch 99/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5398 - mse: 0.5398 - val_loss: 0.6502 - val_mse: 0.6502\n",
      "Epoch 100/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5396 - mse: 0.5396 - val_loss: 0.6513 - val_mse: 0.6513\n",
      "Epoch 101/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5389 - mse: 0.5389 - val_loss: 0.6720 - val_mse: 0.6720\n",
      "Epoch 102/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5383 - mse: 0.5383 - val_loss: 0.6774 - val_mse: 0.6774\n",
      "Epoch 103/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5378 - mse: 0.5378 - val_loss: 0.6727 - val_mse: 0.6727\n",
      "Epoch 104/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5373 - mse: 0.5373 - val_loss: 0.6660 - val_mse: 0.6660\n",
      "Epoch 105/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5369 - mse: 0.5369 - val_loss: 0.6423 - val_mse: 0.6423\n",
      "Epoch 106/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5368 - mse: 0.5368 - val_loss: 0.6447 - val_mse: 0.6447\n",
      "Epoch 107/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5362 - mse: 0.5362 - val_loss: 0.6491 - val_mse: 0.6491\n",
      "Epoch 108/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5357 - mse: 0.5357 - val_loss: 0.6610 - val_mse: 0.6610\n",
      "Epoch 109/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5352 - mse: 0.5352 - val_loss: 0.6561 - val_mse: 0.6561\n",
      "Epoch 110/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5346 - mse: 0.5346 - val_loss: 0.6510 - val_mse: 0.6510\n",
      "Epoch 111/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5347 - mse: 0.5347 - val_loss: 0.6507 - val_mse: 0.6507\n",
      "Epoch 112/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5339 - mse: 0.5339 - val_loss: 0.6469 - val_mse: 0.6469\n",
      "Epoch 113/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5334 - mse: 0.5334 - val_loss: 0.6549 - val_mse: 0.6549\n",
      "Epoch 114/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5333 - mse: 0.5333 - val_loss: 0.6527 - val_mse: 0.6527\n",
      "Epoch 115/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5325 - mse: 0.5325 - val_loss: 0.6681 - val_mse: 0.6681\n",
      "Epoch 116/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5325 - mse: 0.5325 - val_loss: 0.6585 - val_mse: 0.6585\n",
      "Epoch 117/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5321 - mse: 0.5321 - val_loss: 0.6630 - val_mse: 0.6630\n",
      "Epoch 118/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5317 - mse: 0.5317 - val_loss: 0.6394 - val_mse: 0.6394\n",
      "Epoch 119/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5312 - mse: 0.5312 - val_loss: 0.6456 - val_mse: 0.6456\n",
      "Epoch 120/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5306 - mse: 0.5306 - val_loss: 0.6443 - val_mse: 0.6443\n",
      "Epoch 121/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5303 - mse: 0.5303 - val_loss: 0.6450 - val_mse: 0.6450\n",
      "Epoch 122/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5301 - mse: 0.5301 - val_loss: 0.6487 - val_mse: 0.6487\n",
      "Epoch 123/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5297 - mse: 0.5297 - val_loss: 0.6646 - val_mse: 0.6646\n",
      "Epoch 124/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5291 - mse: 0.5291 - val_loss: 0.6521 - val_mse: 0.6521\n",
      "Epoch 125/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5289 - mse: 0.5289 - val_loss: 0.6532 - val_mse: 0.6532\n",
      "Epoch 126/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5288 - mse: 0.5288 - val_loss: 0.6525 - val_mse: 0.6525\n",
      "Epoch 127/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5282 - mse: 0.5282 - val_loss: 0.6294 - val_mse: 0.6294\n",
      "Epoch 128/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5281 - mse: 0.5281 - val_loss: 0.6444 - val_mse: 0.6444\n",
      "Epoch 129/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5272 - mse: 0.5272 - val_loss: 0.6680 - val_mse: 0.6680\n",
      "Epoch 130/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5276 - mse: 0.5276 - val_loss: 0.6642 - val_mse: 0.6642\n",
      "Epoch 131/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5269 - mse: 0.5269 - val_loss: 0.6471 - val_mse: 0.6471\n",
      "Epoch 132/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5264 - mse: 0.5264 - val_loss: 0.6425 - val_mse: 0.6425\n",
      "Epoch 133/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5260 - mse: 0.5260 - val_loss: 0.6519 - val_mse: 0.6519\n",
      "Epoch 134/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5258 - mse: 0.5258 - val_loss: 0.6605 - val_mse: 0.6605\n",
      "Epoch 135/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5256 - mse: 0.5256 - val_loss: 0.6536 - val_mse: 0.6536\n",
      "Epoch 136/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5250 - mse: 0.5250 - val_loss: 0.6366 - val_mse: 0.6366\n",
      "Epoch 137/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5248 - mse: 0.5248 - val_loss: 0.6305 - val_mse: 0.6305\n",
      "Epoch 138/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5243 - mse: 0.5243 - val_loss: 0.6738 - val_mse: 0.6738\n",
      "Epoch 139/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5240 - mse: 0.5240 - val_loss: 0.6422 - val_mse: 0.6422\n",
      "Epoch 140/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5237 - mse: 0.5237 - val_loss: 0.6481 - val_mse: 0.6481\n",
      "Epoch 141/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5235 - mse: 0.5235 - val_loss: 0.6795 - val_mse: 0.6795\n",
      "Epoch 142/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5237 - mse: 0.5237 - val_loss: 0.6492 - val_mse: 0.6492\n",
      "Epoch 143/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5226 - mse: 0.5226 - val_loss: 0.6435 - val_mse: 0.6435\n",
      "Epoch 144/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5226 - mse: 0.5226 - val_loss: 0.6377 - val_mse: 0.6377\n",
      "Epoch 145/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5221 - mse: 0.5221 - val_loss: 0.6437 - val_mse: 0.6437\n",
      "Epoch 146/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5218 - mse: 0.5218 - val_loss: 0.6410 - val_mse: 0.6410\n",
      "Epoch 147/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5216 - mse: 0.5216 - val_loss: 0.6556 - val_mse: 0.6556\n",
      "Epoch 148/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5216 - mse: 0.5216 - val_loss: 0.6455 - val_mse: 0.6455\n",
      "Epoch 149/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5208 - mse: 0.5208 - val_loss: 0.6357 - val_mse: 0.6357\n",
      "Epoch 150/150\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5208 - mse: 0.5208 - val_loss: 0.6521 - val_mse: 0.6521\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.8591 - mse: 0.8591 - val_loss: 0.8278 - val_mse: 0.8278\n",
      "Epoch 2/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.8062 - mse: 0.8062 - val_loss: 0.8398 - val_mse: 0.8398\n",
      "Epoch 3/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7850 - mse: 0.7850 - val_loss: 0.7968 - val_mse: 0.7968\n",
      "Epoch 4/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7712 - mse: 0.7712 - val_loss: 0.8350 - val_mse: 0.8350\n",
      "Epoch 5/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7606 - mse: 0.7606 - val_loss: 0.7729 - val_mse: 0.7729\n",
      "Epoch 6/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7532 - mse: 0.7532 - val_loss: 0.7628 - val_mse: 0.7628\n",
      "Epoch 7/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7469 - mse: 0.7469 - val_loss: 0.7552 - val_mse: 0.7552\n",
      "Epoch 8/150\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.7408 - mse: 0.7408 - val_loss: 0.7497 - val_mse: 0.7497\n",
      "Epoch 9/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7352 - mse: 0.7352 - val_loss: 0.7542 - val_mse: 0.7542\n",
      "Epoch 10/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7312 - mse: 0.7312 - val_loss: 0.7585 - val_mse: 0.7585\n",
      "Epoch 11/150\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.7266 - mse: 0.7266 - val_loss: 0.7611 - val_mse: 0.7611\n",
      "Epoch 12/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7222 - mse: 0.7222 - val_loss: 0.7438 - val_mse: 0.7438\n",
      "Epoch 13/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7187 - mse: 0.7187 - val_loss: 0.7520 - val_mse: 0.7520\n",
      "Epoch 14/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7158 - mse: 0.7158 - val_loss: 0.7714 - val_mse: 0.7714\n",
      "Epoch 15/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7129 - mse: 0.7129 - val_loss: 0.7484 - val_mse: 0.7484\n",
      "Epoch 16/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7092 - mse: 0.7092 - val_loss: 0.7512 - val_mse: 0.7512\n",
      "Epoch 17/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7066 - mse: 0.7066 - val_loss: 0.7634 - val_mse: 0.7634\n",
      "Epoch 18/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7033 - mse: 0.7033 - val_loss: 0.7609 - val_mse: 0.7609\n",
      "Epoch 19/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7008 - mse: 0.7008 - val_loss: 0.7608 - val_mse: 0.7608\n",
      "Epoch 20/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6978 - mse: 0.6978 - val_loss: 0.7685 - val_mse: 0.7685\n",
      "Epoch 21/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6955 - mse: 0.6955 - val_loss: 0.7595 - val_mse: 0.7595\n",
      "Epoch 22/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6933 - mse: 0.6933 - val_loss: 0.7698 - val_mse: 0.7698\n",
      "Epoch 23/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6913 - mse: 0.6913 - val_loss: 0.7614 - val_mse: 0.7614\n",
      "Epoch 24/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6883 - mse: 0.6883 - val_loss: 0.7631 - val_mse: 0.7631\n",
      "Epoch 25/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6868 - mse: 0.6868 - val_loss: 0.7649 - val_mse: 0.7649\n",
      "Epoch 26/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6842 - mse: 0.6842 - val_loss: 0.7765 - val_mse: 0.7765\n",
      "Epoch 27/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6826 - mse: 0.6826 - val_loss: 0.7775 - val_mse: 0.7775\n",
      "Epoch 28/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6808 - mse: 0.6808 - val_loss: 0.7802 - val_mse: 0.7802\n",
      "Epoch 29/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6787 - mse: 0.6787 - val_loss: 0.7623 - val_mse: 0.7623\n",
      "Epoch 30/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6768 - mse: 0.6768 - val_loss: 0.7683 - val_mse: 0.7683\n",
      "Epoch 31/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6746 - mse: 0.6746 - val_loss: 0.7905 - val_mse: 0.7905\n",
      "Epoch 32/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6731 - mse: 0.6731 - val_loss: 0.7699 - val_mse: 0.7699\n",
      "Epoch 33/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6711 - mse: 0.6711 - val_loss: 0.7737 - val_mse: 0.7737\n",
      "Epoch 34/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6695 - mse: 0.6695 - val_loss: 0.7759 - val_mse: 0.7759\n",
      "Epoch 35/150\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6677 - mse: 0.6677 - val_loss: 0.7730 - val_mse: 0.7730\n",
      "Epoch 36/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6659 - mse: 0.6659 - val_loss: 0.7877 - val_mse: 0.7877\n",
      "Epoch 37/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6644 - mse: 0.6644 - val_loss: 0.7614 - val_mse: 0.7614\n",
      "Epoch 38/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6628 - mse: 0.6628 - val_loss: 0.7638 - val_mse: 0.7638\n",
      "Epoch 39/150\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6612 - mse: 0.6612 - val_loss: 0.7761 - val_mse: 0.7761\n",
      "Epoch 40/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6604 - mse: 0.6604 - val_loss: 0.7923 - val_mse: 0.7923\n",
      "Epoch 41/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6586 - mse: 0.6586 - val_loss: 0.7791 - val_mse: 0.7791\n",
      "Epoch 42/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6573 - mse: 0.6573 - val_loss: 0.7862 - val_mse: 0.7862\n",
      "Epoch 43/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6559 - mse: 0.6559 - val_loss: 0.7632 - val_mse: 0.7632\n",
      "Epoch 44/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6544 - mse: 0.6544 - val_loss: 0.7799 - val_mse: 0.7799\n",
      "Epoch 45/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6532 - mse: 0.6532 - val_loss: 0.7806 - val_mse: 0.7806\n",
      "Epoch 46/150\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6520 - mse: 0.6520 - val_loss: 0.7867 - val_mse: 0.7867\n",
      "Epoch 47/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6507 - mse: 0.6507 - val_loss: 0.7765 - val_mse: 0.7765\n",
      "Epoch 48/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6498 - mse: 0.6498 - val_loss: 0.7719 - val_mse: 0.7719\n",
      "Epoch 49/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6481 - mse: 0.6481 - val_loss: 0.7779 - val_mse: 0.7779\n",
      "Epoch 50/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6470 - mse: 0.6470 - val_loss: 0.7699 - val_mse: 0.7699\n",
      "Epoch 51/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6458 - mse: 0.6458 - val_loss: 0.7730 - val_mse: 0.7730\n",
      "Epoch 52/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6448 - mse: 0.6448 - val_loss: 0.7681 - val_mse: 0.7681\n",
      "Epoch 53/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6439 - mse: 0.6439 - val_loss: 0.7792 - val_mse: 0.7792\n",
      "Epoch 54/150\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6427 - mse: 0.6427 - val_loss: 0.7777 - val_mse: 0.7777\n",
      "Epoch 55/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6418 - mse: 0.6418 - val_loss: 0.7855 - val_mse: 0.7855\n",
      "Epoch 56/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6404 - mse: 0.6404 - val_loss: 0.7848 - val_mse: 0.7848\n",
      "Epoch 57/150\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6395 - mse: 0.6395 - val_loss: 0.7877 - val_mse: 0.7877\n",
      "Epoch 58/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6388 - mse: 0.6388 - val_loss: 0.8004 - val_mse: 0.8004\n",
      "Epoch 59/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6375 - mse: 0.6375 - val_loss: 0.8127 - val_mse: 0.8127\n",
      "Epoch 60/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6366 - mse: 0.6366 - val_loss: 0.7837 - val_mse: 0.7837\n",
      "Epoch 61/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6361 - mse: 0.6361 - val_loss: 0.7912 - val_mse: 0.7912\n",
      "Epoch 62/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6349 - mse: 0.6349 - val_loss: 0.7851 - val_mse: 0.7851\n",
      "Epoch 63/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6342 - mse: 0.6342 - val_loss: 0.8276 - val_mse: 0.8276\n",
      "Epoch 64/150\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6333 - mse: 0.6333 - val_loss: 0.7829 - val_mse: 0.7829\n",
      "Epoch 65/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6326 - mse: 0.6326 - val_loss: 0.8036 - val_mse: 0.8036\n",
      "Epoch 66/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6317 - mse: 0.6317 - val_loss: 0.8134 - val_mse: 0.8134\n",
      "Epoch 67/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6307 - mse: 0.6307 - val_loss: 0.8140 - val_mse: 0.8140\n",
      "Epoch 68/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6298 - mse: 0.6298 - val_loss: 0.7892 - val_mse: 0.7892\n",
      "Epoch 69/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6292 - mse: 0.6292 - val_loss: 0.8813 - val_mse: 0.8813\n",
      "Epoch 70/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6285 - mse: 0.6285 - val_loss: 0.8072 - val_mse: 0.8072\n",
      "Epoch 71/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6274 - mse: 0.6274 - val_loss: 0.8290 - val_mse: 0.8290\n",
      "Epoch 72/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6269 - mse: 0.6269 - val_loss: 0.8579 - val_mse: 0.8579\n",
      "Epoch 73/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6260 - mse: 0.6260 - val_loss: 0.8084 - val_mse: 0.8084\n",
      "Epoch 74/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6251 - mse: 0.6251 - val_loss: 0.8198 - val_mse: 0.8198\n",
      "Epoch 75/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6250 - mse: 0.6250 - val_loss: 0.8732 - val_mse: 0.8732\n",
      "Epoch 76/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6238 - mse: 0.6238 - val_loss: 0.8653 - val_mse: 0.8653\n",
      "Epoch 77/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6234 - mse: 0.6234 - val_loss: 0.8579 - val_mse: 0.8579\n",
      "Epoch 78/150\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6224 - mse: 0.6224 - val_loss: 0.9816 - val_mse: 0.9816\n",
      "Epoch 79/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6221 - mse: 0.6221 - val_loss: 0.8472 - val_mse: 0.8472\n",
      "Epoch 80/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6210 - mse: 0.6210 - val_loss: 0.8699 - val_mse: 0.8699\n",
      "Epoch 81/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6204 - mse: 0.6204 - val_loss: 0.9249 - val_mse: 0.9249\n",
      "Epoch 82/150\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6197 - mse: 0.6197 - val_loss: 1.0335 - val_mse: 1.0335\n",
      "Epoch 83/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6188 - mse: 0.6188 - val_loss: 0.9917 - val_mse: 0.9917\n",
      "Epoch 84/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6180 - mse: 0.6180 - val_loss: 0.9719 - val_mse: 0.9719\n",
      "Epoch 85/150\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6182 - mse: 0.6182 - val_loss: 0.9823 - val_mse: 0.9823\n",
      "Epoch 86/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6174 - mse: 0.6174 - val_loss: 1.0896 - val_mse: 1.0896\n",
      "Epoch 87/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6171 - mse: 0.6170 - val_loss: 0.9574 - val_mse: 0.9574\n",
      "Epoch 88/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6160 - mse: 0.6160 - val_loss: 1.0136 - val_mse: 1.0136\n",
      "Epoch 89/150\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6158 - mse: 0.6158 - val_loss: 0.9866 - val_mse: 0.9866\n",
      "Epoch 90/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6153 - mse: 0.6153 - val_loss: 0.9706 - val_mse: 0.9706\n",
      "Epoch 91/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6143 - mse: 0.6143 - val_loss: 1.0904 - val_mse: 1.0904\n",
      "Epoch 92/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6140 - mse: 0.6140 - val_loss: 1.0762 - val_mse: 1.0762\n",
      "Epoch 93/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6136 - mse: 0.6136 - val_loss: 0.9239 - val_mse: 0.9239\n",
      "Epoch 94/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6130 - mse: 0.6130 - val_loss: 1.0213 - val_mse: 1.0213\n",
      "Epoch 95/150\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6122 - mse: 0.6122 - val_loss: 1.0235 - val_mse: 1.0235\n",
      "Epoch 96/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6116 - mse: 0.6116 - val_loss: 1.0080 - val_mse: 1.0080\n",
      "Epoch 97/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6115 - mse: 0.6115 - val_loss: 1.1352 - val_mse: 1.1352\n",
      "Epoch 98/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6109 - mse: 0.6109 - val_loss: 1.2310 - val_mse: 1.2310\n",
      "Epoch 99/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6100 - mse: 0.6100 - val_loss: 1.1731 - val_mse: 1.1731\n",
      "Epoch 100/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6098 - mse: 0.6098 - val_loss: 1.2343 - val_mse: 1.2343\n",
      "Epoch 101/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6090 - mse: 0.6090 - val_loss: 1.1271 - val_mse: 1.1271\n",
      "Epoch 102/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6090 - mse: 0.6090 - val_loss: 1.4547 - val_mse: 1.4547\n",
      "Epoch 103/150\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6084 - mse: 0.6084 - val_loss: 1.3183 - val_mse: 1.3183\n",
      "Epoch 104/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6081 - mse: 0.6081 - val_loss: 1.3903 - val_mse: 1.3903\n",
      "Epoch 105/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6075 - mse: 0.6075 - val_loss: 1.3912 - val_mse: 1.3912\n",
      "Epoch 106/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6070 - mse: 0.6070 - val_loss: 1.3087 - val_mse: 1.3087\n",
      "Epoch 107/150\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6066 - mse: 0.6066 - val_loss: 1.4146 - val_mse: 1.4146\n",
      "Epoch 108/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6060 - mse: 0.6060 - val_loss: 1.3320 - val_mse: 1.3320\n",
      "Epoch 109/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6060 - mse: 0.6060 - val_loss: 1.4326 - val_mse: 1.4326\n",
      "Epoch 110/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6049 - mse: 0.6049 - val_loss: 1.4594 - val_mse: 1.4594\n",
      "Epoch 111/150\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6049 - mse: 0.6049 - val_loss: 1.7879 - val_mse: 1.7879\n",
      "Epoch 112/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6044 - mse: 0.6044 - val_loss: 1.5600 - val_mse: 1.5600\n",
      "Epoch 113/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6040 - mse: 0.6040 - val_loss: 1.6528 - val_mse: 1.6528\n",
      "Epoch 114/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6040 - mse: 0.6040 - val_loss: 1.5126 - val_mse: 1.5126\n",
      "Epoch 115/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6035 - mse: 0.6035 - val_loss: 1.5874 - val_mse: 1.5874\n",
      "Epoch 116/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6028 - mse: 0.6028 - val_loss: 1.7484 - val_mse: 1.7484\n",
      "Epoch 117/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6027 - mse: 0.6027 - val_loss: 1.3501 - val_mse: 1.3501\n",
      "Epoch 118/150\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.6022 - mse: 0.6022 - val_loss: 1.9412 - val_mse: 1.9412\n",
      "Epoch 119/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6021 - mse: 0.6021 - val_loss: 2.0093 - val_mse: 2.0093\n",
      "Epoch 120/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6017 - mse: 0.6017 - val_loss: 1.8465 - val_mse: 1.8465\n",
      "Epoch 121/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6008 - mse: 0.6008 - val_loss: 1.5659 - val_mse: 1.5659\n",
      "Epoch 122/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6011 - mse: 0.6011 - val_loss: 1.7851 - val_mse: 1.7851\n",
      "Epoch 123/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6007 - mse: 0.6007 - val_loss: 1.5733 - val_mse: 1.5733\n",
      "Epoch 124/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6001 - mse: 0.6001 - val_loss: 1.5681 - val_mse: 1.5681\n",
      "Epoch 125/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6002 - mse: 0.6002 - val_loss: 1.9334 - val_mse: 1.9334\n",
      "Epoch 126/150\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.5991 - mse: 0.5991 - val_loss: 2.3530 - val_mse: 2.3530\n",
      "Epoch 127/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5990 - mse: 0.5990 - val_loss: 1.9102 - val_mse: 1.9102\n",
      "Epoch 128/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5986 - mse: 0.5986 - val_loss: 2.4189 - val_mse: 2.4189\n",
      "Epoch 129/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5981 - mse: 0.5981 - val_loss: 2.2104 - val_mse: 2.2104\n",
      "Epoch 130/150\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.5982 - mse: 0.5982 - val_loss: 2.3637 - val_mse: 2.3637\n",
      "Epoch 131/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5979 - mse: 0.5979 - val_loss: 1.9553 - val_mse: 1.9553\n",
      "Epoch 132/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5976 - mse: 0.5976 - val_loss: 2.1788 - val_mse: 2.1788\n",
      "Epoch 133/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5969 - mse: 0.5969 - val_loss: 2.2919 - val_mse: 2.2919\n",
      "Epoch 134/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5968 - mse: 0.5968 - val_loss: 2.0224 - val_mse: 2.0224\n",
      "Epoch 135/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5965 - mse: 0.5965 - val_loss: 1.8783 - val_mse: 1.8783\n",
      "Epoch 136/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5961 - mse: 0.5961 - val_loss: 2.0233 - val_mse: 2.0233\n",
      "Epoch 137/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5961 - mse: 0.5961 - val_loss: 2.3430 - val_mse: 2.3430\n",
      "Epoch 138/150\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.5957 - mse: 0.5957 - val_loss: 2.3380 - val_mse: 2.3380\n",
      "Epoch 139/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5949 - mse: 0.5949 - val_loss: 1.9993 - val_mse: 1.9993\n",
      "Epoch 140/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5952 - mse: 0.5952 - val_loss: 1.8253 - val_mse: 1.8253\n",
      "Epoch 141/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5947 - mse: 0.5947 - val_loss: 2.1069 - val_mse: 2.1069\n",
      "Epoch 142/150\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.5942 - mse: 0.5942 - val_loss: 2.2016 - val_mse: 2.2016\n",
      "Epoch 143/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5940 - mse: 0.5940 - val_loss: 2.2506 - val_mse: 2.2506\n",
      "Epoch 144/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5938 - mse: 0.5938 - val_loss: 2.2416 - val_mse: 2.2416\n",
      "Epoch 145/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5934 - mse: 0.5934 - val_loss: 2.3310 - val_mse: 2.3310\n",
      "Epoch 146/150\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.5937 - mse: 0.5937 - val_loss: 2.3990 - val_mse: 2.3990\n",
      "Epoch 147/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5932 - mse: 0.5932 - val_loss: 2.5675 - val_mse: 2.5675\n",
      "Epoch 148/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5926 - mse: 0.5926 - val_loss: 2.3437 - val_mse: 2.3437\n",
      "Epoch 149/150\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5928 - mse: 0.5928 - val_loss: 2.6141 - val_mse: 2.6141\n",
      "Epoch 150/150\n",
      "3353318/3353318 [==============================] - 15s 5us/step - loss: 0.5923 - mse: 0.5923 - val_loss: 2.0017 - val_mse: 2.0017\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.8452 - mse: 0.8452 - val_loss: 0.7799 - val_mse: 0.7799\n",
      "Epoch 2/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7897 - mse: 0.7897 - val_loss: 0.7622 - val_mse: 0.7622\n",
      "Epoch 3/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7625 - mse: 0.7625 - val_loss: 0.7358 - val_mse: 0.7358\n",
      "Epoch 4/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7454 - mse: 0.7454 - val_loss: 0.7342 - val_mse: 0.7342\n",
      "Epoch 5/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7334 - mse: 0.7334 - val_loss: 0.7261 - val_mse: 0.7261\n",
      "Epoch 6/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7249 - mse: 0.7249 - val_loss: 0.7188 - val_mse: 0.7188\n",
      "Epoch 7/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7176 - mse: 0.7176 - val_loss: 0.7516 - val_mse: 0.7516\n",
      "Epoch 8/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7119 - mse: 0.7119 - val_loss: 0.7131 - val_mse: 0.7131\n",
      "Epoch 9/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7070 - mse: 0.7070 - val_loss: 0.7080 - val_mse: 0.7080\n",
      "Epoch 10/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7018 - mse: 0.7018 - val_loss: 0.7073 - val_mse: 0.7073\n",
      "Epoch 11/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6980 - mse: 0.6980 - val_loss: 0.7066 - val_mse: 0.7066\n",
      "Epoch 12/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6935 - mse: 0.6935 - val_loss: 0.7084 - val_mse: 0.7084\n",
      "Epoch 13/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6896 - mse: 0.6896 - val_loss: 0.6982 - val_mse: 0.6982\n",
      "Epoch 14/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6857 - mse: 0.6857 - val_loss: 0.7001 - val_mse: 0.7001\n",
      "Epoch 15/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6827 - mse: 0.6827 - val_loss: 0.7221 - val_mse: 0.7221\n",
      "Epoch 16/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6788 - mse: 0.6788 - val_loss: 0.6895 - val_mse: 0.6895\n",
      "Epoch 17/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6759 - mse: 0.6759 - val_loss: 0.6872 - val_mse: 0.6872\n",
      "Epoch 18/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6734 - mse: 0.6734 - val_loss: 0.6876 - val_mse: 0.6876\n",
      "Epoch 19/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6706 - mse: 0.6706 - val_loss: 0.6836 - val_mse: 0.6836\n",
      "Epoch 20/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6672 - mse: 0.6672 - val_loss: 0.6891 - val_mse: 0.6891\n",
      "Epoch 21/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6656 - mse: 0.6656 - val_loss: 0.6905 - val_mse: 0.6905\n",
      "Epoch 22/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6630 - mse: 0.6630 - val_loss: 0.6840 - val_mse: 0.6840\n",
      "Epoch 23/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6605 - mse: 0.6605 - val_loss: 0.6829 - val_mse: 0.6829\n",
      "Epoch 24/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6581 - mse: 0.6581 - val_loss: 0.6910 - val_mse: 0.6910\n",
      "Epoch 25/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6563 - mse: 0.6563 - val_loss: 0.6902 - val_mse: 0.6902\n",
      "Epoch 26/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6546 - mse: 0.6546 - val_loss: 0.6790 - val_mse: 0.6790\n",
      "Epoch 27/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6518 - mse: 0.6518 - val_loss: 0.6841 - val_mse: 0.6841\n",
      "Epoch 28/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6500 - mse: 0.6500 - val_loss: 0.6833 - val_mse: 0.6833\n",
      "Epoch 29/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6486 - mse: 0.6486 - val_loss: 0.6850 - val_mse: 0.6850\n",
      "Epoch 30/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6468 - mse: 0.6468 - val_loss: 0.6778 - val_mse: 0.6778\n",
      "Epoch 31/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6446 - mse: 0.6446 - val_loss: 0.6684 - val_mse: 0.6684\n",
      "Epoch 32/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6433 - mse: 0.6433 - val_loss: 0.7031 - val_mse: 0.7031\n",
      "Epoch 33/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6415 - mse: 0.6415 - val_loss: 0.6726 - val_mse: 0.6726\n",
      "Epoch 34/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6400 - mse: 0.6400 - val_loss: 0.6706 - val_mse: 0.6706\n",
      "Epoch 35/200\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.6378 - mse: 0.6378 - val_loss: 0.6825 - val_mse: 0.6825\n",
      "Epoch 36/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6365 - mse: 0.6365 - val_loss: 0.7144 - val_mse: 0.7144\n",
      "Epoch 37/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6347 - mse: 0.6347 - val_loss: 0.6785 - val_mse: 0.6785\n",
      "Epoch 38/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6332 - mse: 0.6332 - val_loss: 0.7031 - val_mse: 0.7031\n",
      "Epoch 39/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6313 - mse: 0.6313 - val_loss: 0.6617 - val_mse: 0.6617\n",
      "Epoch 40/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6297 - mse: 0.6297 - val_loss: 0.6748 - val_mse: 0.6748\n",
      "Epoch 41/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6281 - mse: 0.6281 - val_loss: 0.6754 - val_mse: 0.6754\n",
      "Epoch 42/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6264 - mse: 0.6264 - val_loss: 0.6731 - val_mse: 0.6731\n",
      "Epoch 43/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6255 - mse: 0.6255 - val_loss: 0.6714 - val_mse: 0.6714\n",
      "Epoch 44/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6242 - mse: 0.6242 - val_loss: 0.6722 - val_mse: 0.6722\n",
      "Epoch 45/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6230 - mse: 0.6230 - val_loss: 0.6893 - val_mse: 0.6893\n",
      "Epoch 46/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6212 - mse: 0.6212 - val_loss: 0.6613 - val_mse: 0.6613\n",
      "Epoch 47/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6200 - mse: 0.6200 - val_loss: 0.6632 - val_mse: 0.6632\n",
      "Epoch 48/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6185 - mse: 0.6185 - val_loss: 0.6771 - val_mse: 0.6771\n",
      "Epoch 49/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6175 - mse: 0.6175 - val_loss: 0.6743 - val_mse: 0.6743\n",
      "Epoch 50/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6161 - mse: 0.6161 - val_loss: 0.6894 - val_mse: 0.6894\n",
      "Epoch 51/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6151 - mse: 0.6151 - val_loss: 0.6666 - val_mse: 0.6666\n",
      "Epoch 52/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6143 - mse: 0.6143 - val_loss: 0.6842 - val_mse: 0.6842\n",
      "Epoch 53/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6129 - mse: 0.6129 - val_loss: 0.6673 - val_mse: 0.6673\n",
      "Epoch 54/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6119 - mse: 0.6119 - val_loss: 0.6750 - val_mse: 0.6750\n",
      "Epoch 55/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6105 - mse: 0.6105 - val_loss: 0.6706 - val_mse: 0.6706\n",
      "Epoch 56/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6096 - mse: 0.6096 - val_loss: 0.6666 - val_mse: 0.6666\n",
      "Epoch 57/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6087 - mse: 0.6087 - val_loss: 0.6618 - val_mse: 0.6618\n",
      "Epoch 58/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6079 - mse: 0.6079 - val_loss: 0.6900 - val_mse: 0.6900\n",
      "Epoch 59/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6069 - mse: 0.6069 - val_loss: 0.6711 - val_mse: 0.6711\n",
      "Epoch 60/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6053 - mse: 0.6053 - val_loss: 0.6837 - val_mse: 0.6837\n",
      "Epoch 61/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6047 - mse: 0.6047 - val_loss: 0.6668 - val_mse: 0.6668\n",
      "Epoch 62/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6040 - mse: 0.6040 - val_loss: 0.6689 - val_mse: 0.6689\n",
      "Epoch 63/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6030 - mse: 0.6030 - val_loss: 0.6706 - val_mse: 0.6706\n",
      "Epoch 64/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6021 - mse: 0.6021 - val_loss: 0.6627 - val_mse: 0.6627\n",
      "Epoch 65/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6010 - mse: 0.6010 - val_loss: 0.6597 - val_mse: 0.6597\n",
      "Epoch 66/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6002 - mse: 0.6002 - val_loss: 0.6719 - val_mse: 0.6719\n",
      "Epoch 67/200\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5998 - mse: 0.5998 - val_loss: 0.6640 - val_mse: 0.6640\n",
      "Epoch 68/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5983 - mse: 0.5983 - val_loss: 0.6894 - val_mse: 0.6894\n",
      "Epoch 69/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5976 - mse: 0.5976 - val_loss: 0.6662 - val_mse: 0.6662\n",
      "Epoch 70/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5972 - mse: 0.5972 - val_loss: 0.6813 - val_mse: 0.6813\n",
      "Epoch 71/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5960 - mse: 0.5960 - val_loss: 0.6685 - val_mse: 0.6685\n",
      "Epoch 72/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5954 - mse: 0.5954 - val_loss: 0.6672 - val_mse: 0.6672\n",
      "Epoch 73/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5944 - mse: 0.5944 - val_loss: 0.6680 - val_mse: 0.6680\n",
      "Epoch 74/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5936 - mse: 0.5936 - val_loss: 0.6613 - val_mse: 0.6613\n",
      "Epoch 75/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5930 - mse: 0.5930 - val_loss: 0.6774 - val_mse: 0.6774\n",
      "Epoch 76/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5923 - mse: 0.5923 - val_loss: 0.6784 - val_mse: 0.6784\n",
      "Epoch 77/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5918 - mse: 0.5918 - val_loss: 0.6797 - val_mse: 0.6797\n",
      "Epoch 78/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5911 - mse: 0.5911 - val_loss: 0.7444 - val_mse: 0.7444\n",
      "Epoch 79/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5909 - mse: 0.5909 - val_loss: 0.7095 - val_mse: 0.7095\n",
      "Epoch 80/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5899 - mse: 0.5899 - val_loss: 0.6930 - val_mse: 0.6930\n",
      "Epoch 81/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5897 - mse: 0.5897 - val_loss: 0.6862 - val_mse: 0.6862\n",
      "Epoch 82/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5889 - mse: 0.5889 - val_loss: 0.7008 - val_mse: 0.7008\n",
      "Epoch 83/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5881 - mse: 0.5881 - val_loss: 0.6698 - val_mse: 0.6698\n",
      "Epoch 84/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5878 - mse: 0.5878 - val_loss: 0.6743 - val_mse: 0.6743\n",
      "Epoch 85/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5870 - mse: 0.5870 - val_loss: 0.6973 - val_mse: 0.6973\n",
      "Epoch 86/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5863 - mse: 0.5863 - val_loss: 0.6825 - val_mse: 0.6825\n",
      "Epoch 87/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5857 - mse: 0.5857 - val_loss: 0.6995 - val_mse: 0.6995\n",
      "Epoch 88/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5852 - mse: 0.5852 - val_loss: 0.6815 - val_mse: 0.6815\n",
      "Epoch 89/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5849 - mse: 0.5849 - val_loss: 0.6623 - val_mse: 0.6623\n",
      "Epoch 90/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5840 - mse: 0.5840 - val_loss: 0.7151 - val_mse: 0.7151\n",
      "Epoch 91/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5843 - mse: 0.5843 - val_loss: 0.6995 - val_mse: 0.6995\n",
      "Epoch 92/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5833 - mse: 0.5833 - val_loss: 0.7089 - val_mse: 0.7089\n",
      "Epoch 93/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5831 - mse: 0.5831 - val_loss: 0.6961 - val_mse: 0.6961\n",
      "Epoch 94/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5821 - mse: 0.5821 - val_loss: 0.7502 - val_mse: 0.7502\n",
      "Epoch 95/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5818 - mse: 0.5818 - val_loss: 0.6640 - val_mse: 0.6640\n",
      "Epoch 96/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5812 - mse: 0.5812 - val_loss: 0.6682 - val_mse: 0.6682\n",
      "Epoch 97/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5809 - mse: 0.5809 - val_loss: 0.6703 - val_mse: 0.6703\n",
      "Epoch 98/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5804 - mse: 0.5804 - val_loss: 0.7103 - val_mse: 0.7103\n",
      "Epoch 99/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5802 - mse: 0.5802 - val_loss: 0.6724 - val_mse: 0.6724\n",
      "Epoch 100/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5795 - mse: 0.5795 - val_loss: 0.6724 - val_mse: 0.6724\n",
      "Epoch 101/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5789 - mse: 0.5789 - val_loss: 0.6885 - val_mse: 0.6885\n",
      "Epoch 102/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5787 - mse: 0.5787 - val_loss: 0.6716 - val_mse: 0.6716\n",
      "Epoch 103/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5779 - mse: 0.5779 - val_loss: 0.6801 - val_mse: 0.6801\n",
      "Epoch 104/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5776 - mse: 0.5776 - val_loss: 0.6953 - val_mse: 0.6953\n",
      "Epoch 105/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5772 - mse: 0.5772 - val_loss: 0.7841 - val_mse: 0.7841\n",
      "Epoch 106/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5768 - mse: 0.5768 - val_loss: 0.6844 - val_mse: 0.6844\n",
      "Epoch 107/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5761 - mse: 0.5761 - val_loss: 0.6892 - val_mse: 0.6892\n",
      "Epoch 108/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5765 - mse: 0.5765 - val_loss: 0.6972 - val_mse: 0.6972\n",
      "Epoch 109/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5754 - mse: 0.5754 - val_loss: 0.6904 - val_mse: 0.6904\n",
      "Epoch 110/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5750 - mse: 0.5750 - val_loss: 0.7070 - val_mse: 0.7070\n",
      "Epoch 111/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5749 - mse: 0.5749 - val_loss: 0.6834 - val_mse: 0.6834\n",
      "Epoch 112/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5744 - mse: 0.5744 - val_loss: 0.6970 - val_mse: 0.6970\n",
      "Epoch 113/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5744 - mse: 0.5744 - val_loss: 0.6916 - val_mse: 0.6916\n",
      "Epoch 114/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5737 - mse: 0.5737 - val_loss: 0.6753 - val_mse: 0.6753\n",
      "Epoch 115/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5730 - mse: 0.5730 - val_loss: 0.7042 - val_mse: 0.7042\n",
      "Epoch 116/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5731 - mse: 0.5731 - val_loss: 0.6856 - val_mse: 0.6856\n",
      "Epoch 117/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5726 - mse: 0.5726 - val_loss: 0.6800 - val_mse: 0.6800\n",
      "Epoch 118/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5723 - mse: 0.5723 - val_loss: 0.7503 - val_mse: 0.7503\n",
      "Epoch 119/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5721 - mse: 0.5721 - val_loss: 0.7020 - val_mse: 0.7020\n",
      "Epoch 120/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5717 - mse: 0.5717 - val_loss: 0.6908 - val_mse: 0.6908\n",
      "Epoch 121/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5711 - mse: 0.5711 - val_loss: 0.6946 - val_mse: 0.6946\n",
      "Epoch 122/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5710 - mse: 0.5710 - val_loss: 0.7290 - val_mse: 0.7290\n",
      "Epoch 123/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5706 - mse: 0.5706 - val_loss: 0.7224 - val_mse: 0.7224\n",
      "Epoch 124/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5701 - mse: 0.5701 - val_loss: 0.6874 - val_mse: 0.6874\n",
      "Epoch 125/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5702 - mse: 0.5702 - val_loss: 0.7320 - val_mse: 0.7320\n",
      "Epoch 126/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5695 - mse: 0.5695 - val_loss: 0.7219 - val_mse: 0.7219\n",
      "Epoch 127/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5691 - mse: 0.5691 - val_loss: 0.7374 - val_mse: 0.7374\n",
      "Epoch 128/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5693 - mse: 0.5693 - val_loss: 0.7641 - val_mse: 0.7641\n",
      "Epoch 129/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5681 - mse: 0.5681 - val_loss: 0.7138 - val_mse: 0.7138\n",
      "Epoch 130/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5684 - mse: 0.5684 - val_loss: 0.7733 - val_mse: 0.7733\n",
      "Epoch 131/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5677 - mse: 0.5677 - val_loss: 0.6774 - val_mse: 0.6774\n",
      "Epoch 132/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5676 - mse: 0.5676 - val_loss: 0.7436 - val_mse: 0.7436\n",
      "Epoch 133/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5674 - mse: 0.5674 - val_loss: 0.7675 - val_mse: 0.7675\n",
      "Epoch 134/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5672 - mse: 0.5672 - val_loss: 0.7200 - val_mse: 0.7200\n",
      "Epoch 135/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5665 - mse: 0.5665 - val_loss: 0.6719 - val_mse: 0.6719\n",
      "Epoch 136/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5663 - mse: 0.5663 - val_loss: 0.6857 - val_mse: 0.6857\n",
      "Epoch 137/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5660 - mse: 0.5660 - val_loss: 0.7677 - val_mse: 0.7677\n",
      "Epoch 138/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5659 - mse: 0.5659 - val_loss: 0.7448 - val_mse: 0.7448\n",
      "Epoch 139/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5655 - mse: 0.5655 - val_loss: 0.7023 - val_mse: 0.7023\n",
      "Epoch 140/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5654 - mse: 0.5654 - val_loss: 0.7559 - val_mse: 0.7559\n",
      "Epoch 141/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5652 - mse: 0.5652 - val_loss: 0.7455 - val_mse: 0.7455\n",
      "Epoch 142/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5645 - mse: 0.5645 - val_loss: 0.7516 - val_mse: 0.7516\n",
      "Epoch 143/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5644 - mse: 0.5644 - val_loss: 0.6782 - val_mse: 0.6782\n",
      "Epoch 144/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5639 - mse: 0.5639 - val_loss: 0.7198 - val_mse: 0.7198\n",
      "Epoch 145/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5635 - mse: 0.5635 - val_loss: 0.7074 - val_mse: 0.7074\n",
      "Epoch 146/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5636 - mse: 0.5636 - val_loss: 0.6926 - val_mse: 0.6926\n",
      "Epoch 147/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5631 - mse: 0.5631 - val_loss: 0.7536 - val_mse: 0.7536\n",
      "Epoch 148/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5630 - mse: 0.5630 - val_loss: 0.7249 - val_mse: 0.7249\n",
      "Epoch 149/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5625 - mse: 0.5625 - val_loss: 0.7237 - val_mse: 0.7237\n",
      "Epoch 150/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5621 - mse: 0.5621 - val_loss: 0.7370 - val_mse: 0.7370\n",
      "Epoch 151/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5621 - mse: 0.5621 - val_loss: 0.7254 - val_mse: 0.7254\n",
      "Epoch 152/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5624 - mse: 0.5624 - val_loss: 0.7262 - val_mse: 0.7262\n",
      "Epoch 153/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5617 - mse: 0.5617 - val_loss: 0.7301 - val_mse: 0.7301\n",
      "Epoch 154/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5612 - mse: 0.5612 - val_loss: 0.7839 - val_mse: 0.7839\n",
      "Epoch 155/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5612 - mse: 0.5612 - val_loss: 0.7570 - val_mse: 0.7570\n",
      "Epoch 156/200\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5607 - mse: 0.5607 - val_loss: 0.7052 - val_mse: 0.7052\n",
      "Epoch 157/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5607 - mse: 0.5607 - val_loss: 0.7306 - val_mse: 0.7306\n",
      "Epoch 158/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5605 - mse: 0.5605 - val_loss: 0.7810 - val_mse: 0.7810\n",
      "Epoch 159/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5604 - mse: 0.5604 - val_loss: 0.7817 - val_mse: 0.7817\n",
      "Epoch 160/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5600 - mse: 0.5600 - val_loss: 0.7549 - val_mse: 0.7549\n",
      "Epoch 161/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5602 - mse: 0.5602 - val_loss: 0.7438 - val_mse: 0.7438\n",
      "Epoch 162/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5593 - mse: 0.5593 - val_loss: 0.7388 - val_mse: 0.7388\n",
      "Epoch 163/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5591 - mse: 0.5591 - val_loss: 0.7598 - val_mse: 0.7598\n",
      "Epoch 164/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5590 - mse: 0.5590 - val_loss: 0.7646 - val_mse: 0.7646\n",
      "Epoch 165/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5587 - mse: 0.5587 - val_loss: 0.8349 - val_mse: 0.8349\n",
      "Epoch 166/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5584 - mse: 0.5584 - val_loss: 0.7324 - val_mse: 0.7324\n",
      "Epoch 167/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5580 - mse: 0.5580 - val_loss: 0.7280 - val_mse: 0.7280\n",
      "Epoch 168/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5581 - mse: 0.5581 - val_loss: 0.7363 - val_mse: 0.7363\n",
      "Epoch 169/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5576 - mse: 0.5576 - val_loss: 0.8205 - val_mse: 0.8205\n",
      "Epoch 170/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5576 - mse: 0.5576 - val_loss: 0.9021 - val_mse: 0.9021\n",
      "Epoch 171/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5577 - mse: 0.5577 - val_loss: 0.7860 - val_mse: 0.7860\n",
      "Epoch 172/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5567 - mse: 0.5567 - val_loss: 0.7557 - val_mse: 0.7557\n",
      "Epoch 173/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5565 - mse: 0.5565 - val_loss: 0.7832 - val_mse: 0.7832\n",
      "Epoch 174/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5563 - mse: 0.5563 - val_loss: 0.7998 - val_mse: 0.7998\n",
      "Epoch 175/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5562 - mse: 0.5562 - val_loss: 0.8392 - val_mse: 0.8392\n",
      "Epoch 176/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5560 - mse: 0.5560 - val_loss: 0.7509 - val_mse: 0.7509\n",
      "Epoch 177/200\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5556 - mse: 0.5556 - val_loss: 0.7742 - val_mse: 0.7742\n",
      "Epoch 178/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5552 - mse: 0.5552 - val_loss: 0.7239 - val_mse: 0.7239\n",
      "Epoch 179/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5550 - mse: 0.5550 - val_loss: 0.7443 - val_mse: 0.7443\n",
      "Epoch 180/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5548 - mse: 0.5548 - val_loss: 0.8803 - val_mse: 0.8803\n",
      "Epoch 181/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5547 - mse: 0.5547 - val_loss: 0.7410 - val_mse: 0.7410\n",
      "Epoch 182/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5544 - mse: 0.5544 - val_loss: 0.7254 - val_mse: 0.7254\n",
      "Epoch 183/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5544 - mse: 0.5544 - val_loss: 0.7897 - val_mse: 0.7897\n",
      "Epoch 184/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5540 - mse: 0.5540 - val_loss: 0.7721 - val_mse: 0.7721\n",
      "Epoch 185/200\n",
      "3353317/3353317 [==============================] - 15s 5us/step - loss: 0.5538 - mse: 0.5538 - val_loss: 0.7677 - val_mse: 0.7677\n",
      "Epoch 186/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5537 - mse: 0.5537 - val_loss: 0.7653 - val_mse: 0.7653\n",
      "Epoch 187/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5534 - mse: 0.5534 - val_loss: 0.8085 - val_mse: 0.8085\n",
      "Epoch 188/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5531 - mse: 0.5531 - val_loss: 0.8816 - val_mse: 0.8816\n",
      "Epoch 189/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5529 - mse: 0.5529 - val_loss: 0.7754 - val_mse: 0.7754\n",
      "Epoch 190/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5530 - mse: 0.5530 - val_loss: 0.7796 - val_mse: 0.7796\n",
      "Epoch 191/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5525 - mse: 0.5525 - val_loss: 0.7583 - val_mse: 0.7583\n",
      "Epoch 192/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5523 - mse: 0.5523 - val_loss: 0.7649 - val_mse: 0.7649\n",
      "Epoch 193/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5519 - mse: 0.5519 - val_loss: 0.8008 - val_mse: 0.8008\n",
      "Epoch 194/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5520 - mse: 0.5520 - val_loss: 0.7725 - val_mse: 0.7725\n",
      "Epoch 195/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5518 - mse: 0.5518 - val_loss: 0.7668 - val_mse: 0.7668\n",
      "Epoch 196/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5516 - mse: 0.5516 - val_loss: 0.8545 - val_mse: 0.8545\n",
      "Epoch 197/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5516 - mse: 0.5516 - val_loss: 0.7804 - val_mse: 0.7804\n",
      "Epoch 198/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5513 - mse: 0.5513 - val_loss: 0.9249 - val_mse: 0.9249\n",
      "Epoch 199/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5506 - mse: 0.5506 - val_loss: 0.8176 - val_mse: 0.8176\n",
      "Epoch 200/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5509 - mse: 0.5509 - val_loss: 0.7920 - val_mse: 0.7920\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7522 - mse: 0.7522 - val_loss: 0.7730 - val_mse: 0.7730\n",
      "Epoch 2/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.7069 - mse: 0.7069 - val_loss: 0.7672 - val_mse: 0.7672\n",
      "Epoch 3/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6888 - mse: 0.6888 - val_loss: 0.7481 - val_mse: 0.7481\n",
      "Epoch 4/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6765 - mse: 0.6765 - val_loss: 0.7508 - val_mse: 0.7508\n",
      "Epoch 5/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6678 - mse: 0.6678 - val_loss: 0.7383 - val_mse: 0.7383\n",
      "Epoch 6/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6615 - mse: 0.6615 - val_loss: 0.7318 - val_mse: 0.7318\n",
      "Epoch 7/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6549 - mse: 0.6549 - val_loss: 0.7264 - val_mse: 0.7264\n",
      "Epoch 8/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6499 - mse: 0.6499 - val_loss: 0.7151 - val_mse: 0.7151\n",
      "Epoch 9/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6456 - mse: 0.6456 - val_loss: 0.7162 - val_mse: 0.7162\n",
      "Epoch 10/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6414 - mse: 0.6414 - val_loss: 0.7195 - val_mse: 0.7195\n",
      "Epoch 11/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6375 - mse: 0.6375 - val_loss: 0.7154 - val_mse: 0.7154\n",
      "Epoch 12/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6340 - mse: 0.6340 - val_loss: 0.7153 - val_mse: 0.7153\n",
      "Epoch 13/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6303 - mse: 0.6303 - val_loss: 0.7125 - val_mse: 0.7125\n",
      "Epoch 14/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6271 - mse: 0.6271 - val_loss: 0.7064 - val_mse: 0.7064\n",
      "Epoch 15/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6243 - mse: 0.6243 - val_loss: 0.7011 - val_mse: 0.7011\n",
      "Epoch 16/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6212 - mse: 0.6212 - val_loss: 0.6997 - val_mse: 0.6997\n",
      "Epoch 17/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6186 - mse: 0.6186 - val_loss: 0.6902 - val_mse: 0.6902\n",
      "Epoch 18/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6163 - mse: 0.6163 - val_loss: 0.6931 - val_mse: 0.6931\n",
      "Epoch 19/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6138 - mse: 0.6138 - val_loss: 0.6977 - val_mse: 0.6977\n",
      "Epoch 20/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6120 - mse: 0.6120 - val_loss: 0.7136 - val_mse: 0.7136\n",
      "Epoch 21/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6100 - mse: 0.6100 - val_loss: 0.6994 - val_mse: 0.6994\n",
      "Epoch 22/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6078 - mse: 0.6078 - val_loss: 0.6913 - val_mse: 0.6913\n",
      "Epoch 23/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6058 - mse: 0.6058 - val_loss: 0.6854 - val_mse: 0.6854\n",
      "Epoch 24/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.6041 - mse: 0.6041 - val_loss: 0.6823 - val_mse: 0.6823\n",
      "Epoch 25/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6026 - mse: 0.6026 - val_loss: 0.7063 - val_mse: 0.7063\n",
      "Epoch 26/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.6008 - mse: 0.6008 - val_loss: 0.6851 - val_mse: 0.6851\n",
      "Epoch 27/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5992 - mse: 0.5992 - val_loss: 0.6801 - val_mse: 0.6801\n",
      "Epoch 28/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5981 - mse: 0.5981 - val_loss: 0.6964 - val_mse: 0.6964\n",
      "Epoch 29/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5969 - mse: 0.5969 - val_loss: 0.6827 - val_mse: 0.6827\n",
      "Epoch 30/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5948 - mse: 0.5948 - val_loss: 0.6869 - val_mse: 0.6869\n",
      "Epoch 31/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5942 - mse: 0.5942 - val_loss: 0.6773 - val_mse: 0.6773\n",
      "Epoch 32/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5926 - mse: 0.5926 - val_loss: 0.6787 - val_mse: 0.6787\n",
      "Epoch 33/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5915 - mse: 0.5915 - val_loss: 0.7085 - val_mse: 0.7085\n",
      "Epoch 34/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5899 - mse: 0.5899 - val_loss: 0.6935 - val_mse: 0.6935\n",
      "Epoch 35/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5883 - mse: 0.5883 - val_loss: 0.6890 - val_mse: 0.6890\n",
      "Epoch 36/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5872 - mse: 0.5872 - val_loss: 0.6792 - val_mse: 0.6792\n",
      "Epoch 37/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5862 - mse: 0.5862 - val_loss: 0.6952 - val_mse: 0.6952\n",
      "Epoch 38/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5849 - mse: 0.5849 - val_loss: 0.6841 - val_mse: 0.6841\n",
      "Epoch 39/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5842 - mse: 0.5842 - val_loss: 0.6878 - val_mse: 0.6878\n",
      "Epoch 40/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5827 - mse: 0.5827 - val_loss: 0.6668 - val_mse: 0.6668\n",
      "Epoch 41/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5820 - mse: 0.5820 - val_loss: 0.6808 - val_mse: 0.6808\n",
      "Epoch 42/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5809 - mse: 0.5809 - val_loss: 0.6813 - val_mse: 0.6813\n",
      "Epoch 43/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5798 - mse: 0.5798 - val_loss: 0.6895 - val_mse: 0.6895\n",
      "Epoch 44/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5789 - mse: 0.5789 - val_loss: 0.6787 - val_mse: 0.6787\n",
      "Epoch 45/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5780 - mse: 0.5780 - val_loss: 0.7009 - val_mse: 0.7009\n",
      "Epoch 46/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5771 - mse: 0.5771 - val_loss: 0.6976 - val_mse: 0.6976\n",
      "Epoch 47/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5760 - mse: 0.5760 - val_loss: 0.6785 - val_mse: 0.6785\n",
      "Epoch 48/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5750 - mse: 0.5750 - val_loss: 0.6832 - val_mse: 0.6832\n",
      "Epoch 49/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5743 - mse: 0.5743 - val_loss: 0.6929 - val_mse: 0.6929\n",
      "Epoch 50/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5732 - mse: 0.5732 - val_loss: 0.6810 - val_mse: 0.6810\n",
      "Epoch 51/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5726 - mse: 0.5726 - val_loss: 0.7017 - val_mse: 0.7017\n",
      "Epoch 52/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5716 - mse: 0.5716 - val_loss: 0.6770 - val_mse: 0.6770\n",
      "Epoch 53/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5707 - mse: 0.5707 - val_loss: 0.6849 - val_mse: 0.6849\n",
      "Epoch 54/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5696 - mse: 0.5696 - val_loss: 0.6811 - val_mse: 0.6811\n",
      "Epoch 55/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5696 - mse: 0.5696 - val_loss: 0.6823 - val_mse: 0.6823\n",
      "Epoch 56/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5684 - mse: 0.5684 - val_loss: 0.6902 - val_mse: 0.6902\n",
      "Epoch 57/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5677 - mse: 0.5677 - val_loss: 0.7021 - val_mse: 0.7021\n",
      "Epoch 58/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5672 - mse: 0.5672 - val_loss: 0.6935 - val_mse: 0.6935\n",
      "Epoch 59/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5663 - mse: 0.5663 - val_loss: 0.7030 - val_mse: 0.7030\n",
      "Epoch 60/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5655 - mse: 0.5655 - val_loss: 0.6888 - val_mse: 0.6888\n",
      "Epoch 61/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5646 - mse: 0.5646 - val_loss: 0.6867 - val_mse: 0.6867\n",
      "Epoch 62/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5640 - mse: 0.5640 - val_loss: 0.6858 - val_mse: 0.6858\n",
      "Epoch 63/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5634 - mse: 0.5634 - val_loss: 0.6968 - val_mse: 0.6968\n",
      "Epoch 64/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5628 - mse: 0.5628 - val_loss: 0.6876 - val_mse: 0.6876\n",
      "Epoch 65/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5620 - mse: 0.5620 - val_loss: 0.6767 - val_mse: 0.6767\n",
      "Epoch 66/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5613 - mse: 0.5613 - val_loss: 0.6964 - val_mse: 0.6964\n",
      "Epoch 67/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5605 - mse: 0.5605 - val_loss: 0.6746 - val_mse: 0.6746\n",
      "Epoch 68/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5597 - mse: 0.5597 - val_loss: 0.6799 - val_mse: 0.6799\n",
      "Epoch 69/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5594 - mse: 0.5594 - val_loss: 0.6891 - val_mse: 0.6891\n",
      "Epoch 70/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5585 - mse: 0.5585 - val_loss: 0.6890 - val_mse: 0.6890\n",
      "Epoch 71/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5578 - mse: 0.5578 - val_loss: 0.6752 - val_mse: 0.6752\n",
      "Epoch 72/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5571 - mse: 0.5571 - val_loss: 0.6938 - val_mse: 0.6938\n",
      "Epoch 73/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5569 - mse: 0.5569 - val_loss: 0.6845 - val_mse: 0.6845\n",
      "Epoch 74/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5560 - mse: 0.5560 - val_loss: 0.6999 - val_mse: 0.6999\n",
      "Epoch 75/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5556 - mse: 0.5556 - val_loss: 0.6970 - val_mse: 0.6970\n",
      "Epoch 76/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5548 - mse: 0.5548 - val_loss: 0.6835 - val_mse: 0.6835\n",
      "Epoch 77/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5541 - mse: 0.5541 - val_loss: 0.6842 - val_mse: 0.6842\n",
      "Epoch 78/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5536 - mse: 0.5536 - val_loss: 0.6773 - val_mse: 0.6773\n",
      "Epoch 79/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5530 - mse: 0.5530 - val_loss: 0.6835 - val_mse: 0.6835\n",
      "Epoch 80/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5523 - mse: 0.5523 - val_loss: 0.6937 - val_mse: 0.6937\n",
      "Epoch 81/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5519 - mse: 0.5519 - val_loss: 0.6805 - val_mse: 0.6805\n",
      "Epoch 82/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5514 - mse: 0.5514 - val_loss: 0.7124 - val_mse: 0.7124\n",
      "Epoch 83/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5507 - mse: 0.5507 - val_loss: 0.6920 - val_mse: 0.6920\n",
      "Epoch 84/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5500 - mse: 0.5500 - val_loss: 0.6877 - val_mse: 0.6877\n",
      "Epoch 85/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5496 - mse: 0.5496 - val_loss: 0.6977 - val_mse: 0.6977\n",
      "Epoch 86/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5492 - mse: 0.5492 - val_loss: 0.6829 - val_mse: 0.6829\n",
      "Epoch 87/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5483 - mse: 0.5483 - val_loss: 0.6972 - val_mse: 0.6972\n",
      "Epoch 88/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5483 - mse: 0.5483 - val_loss: 0.7005 - val_mse: 0.7005\n",
      "Epoch 89/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5475 - mse: 0.5475 - val_loss: 0.6872 - val_mse: 0.6872\n",
      "Epoch 90/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5469 - mse: 0.5469 - val_loss: 0.6989 - val_mse: 0.6989\n",
      "Epoch 91/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5465 - mse: 0.5465 - val_loss: 0.7033 - val_mse: 0.7033\n",
      "Epoch 92/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5458 - mse: 0.5458 - val_loss: 0.6658 - val_mse: 0.6658\n",
      "Epoch 93/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5453 - mse: 0.5453 - val_loss: 0.7168 - val_mse: 0.7168\n",
      "Epoch 94/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5451 - mse: 0.5451 - val_loss: 0.6877 - val_mse: 0.6877\n",
      "Epoch 95/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5443 - mse: 0.5443 - val_loss: 0.7037 - val_mse: 0.7037\n",
      "Epoch 96/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5440 - mse: 0.5440 - val_loss: 0.6977 - val_mse: 0.6977\n",
      "Epoch 97/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5431 - mse: 0.5431 - val_loss: 0.6770 - val_mse: 0.6770\n",
      "Epoch 98/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5430 - mse: 0.5430 - val_loss: 0.7055 - val_mse: 0.7055\n",
      "Epoch 99/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5423 - mse: 0.5423 - val_loss: 0.6905 - val_mse: 0.6905\n",
      "Epoch 100/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5419 - mse: 0.5419 - val_loss: 0.6973 - val_mse: 0.6973\n",
      "Epoch 101/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5411 - mse: 0.5411 - val_loss: 0.6921 - val_mse: 0.6921\n",
      "Epoch 102/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5409 - mse: 0.5409 - val_loss: 0.6893 - val_mse: 0.6893\n",
      "Epoch 103/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5401 - mse: 0.5401 - val_loss: 0.6974 - val_mse: 0.6974\n",
      "Epoch 104/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5399 - mse: 0.5399 - val_loss: 0.6981 - val_mse: 0.6981\n",
      "Epoch 105/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5394 - mse: 0.5394 - val_loss: 0.6902 - val_mse: 0.6902\n",
      "Epoch 106/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5387 - mse: 0.5387 - val_loss: 0.6739 - val_mse: 0.6739\n",
      "Epoch 107/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5383 - mse: 0.5383 - val_loss: 0.6793 - val_mse: 0.6793\n",
      "Epoch 108/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5383 - mse: 0.5383 - val_loss: 0.6650 - val_mse: 0.6650\n",
      "Epoch 109/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5378 - mse: 0.5378 - val_loss: 0.6908 - val_mse: 0.6908\n",
      "Epoch 110/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5370 - mse: 0.5370 - val_loss: 0.6913 - val_mse: 0.6913\n",
      "Epoch 111/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5368 - mse: 0.5368 - val_loss: 0.6876 - val_mse: 0.6876\n",
      "Epoch 112/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5361 - mse: 0.5361 - val_loss: 0.6764 - val_mse: 0.6764\n",
      "Epoch 113/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5356 - mse: 0.5356 - val_loss: 0.6826 - val_mse: 0.6826\n",
      "Epoch 114/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5354 - mse: 0.5354 - val_loss: 0.6942 - val_mse: 0.6942\n",
      "Epoch 115/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5351 - mse: 0.5351 - val_loss: 0.6744 - val_mse: 0.6744\n",
      "Epoch 116/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5343 - mse: 0.5343 - val_loss: 0.6963 - val_mse: 0.6963\n",
      "Epoch 117/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5344 - mse: 0.5344 - val_loss: 0.6870 - val_mse: 0.6870\n",
      "Epoch 118/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5338 - mse: 0.5338 - val_loss: 0.7014 - val_mse: 0.7014\n",
      "Epoch 119/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5336 - mse: 0.5336 - val_loss: 0.6901 - val_mse: 0.6901\n",
      "Epoch 120/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5324 - mse: 0.5324 - val_loss: 0.6905 - val_mse: 0.6905\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5318 - mse: 0.5318 - val_loss: 0.6845 - val_mse: 0.6845\n",
      "Epoch 122/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5319 - mse: 0.5319 - val_loss: 0.6897 - val_mse: 0.6897\n",
      "Epoch 123/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5316 - mse: 0.5316 - val_loss: 0.6689 - val_mse: 0.6689\n",
      "Epoch 124/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5309 - mse: 0.5309 - val_loss: 0.6933 - val_mse: 0.6933\n",
      "Epoch 125/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5308 - mse: 0.5308 - val_loss: 0.6774 - val_mse: 0.6774\n",
      "Epoch 126/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5302 - mse: 0.5302 - val_loss: 0.6804 - val_mse: 0.6804\n",
      "Epoch 127/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5299 - mse: 0.5299 - val_loss: 0.6933 - val_mse: 0.6933\n",
      "Epoch 128/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5298 - mse: 0.5298 - val_loss: 0.6728 - val_mse: 0.6728\n",
      "Epoch 129/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5291 - mse: 0.5291 - val_loss: 0.7018 - val_mse: 0.7018\n",
      "Epoch 130/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5287 - mse: 0.5287 - val_loss: 0.7023 - val_mse: 0.7023\n",
      "Epoch 131/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5282 - mse: 0.5282 - val_loss: 0.6842 - val_mse: 0.6842\n",
      "Epoch 132/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5281 - mse: 0.5281 - val_loss: 0.6794 - val_mse: 0.6794\n",
      "Epoch 133/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5274 - mse: 0.5274 - val_loss: 0.6897 - val_mse: 0.6897\n",
      "Epoch 134/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5272 - mse: 0.5272 - val_loss: 0.6839 - val_mse: 0.6839\n",
      "Epoch 135/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5272 - mse: 0.5272 - val_loss: 0.6906 - val_mse: 0.6906\n",
      "Epoch 136/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5265 - mse: 0.5265 - val_loss: 0.6751 - val_mse: 0.6751\n",
      "Epoch 137/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5261 - mse: 0.5261 - val_loss: 0.6785 - val_mse: 0.6785\n",
      "Epoch 138/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5259 - mse: 0.5259 - val_loss: 0.6683 - val_mse: 0.6683\n",
      "Epoch 139/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5254 - mse: 0.5254 - val_loss: 0.6735 - val_mse: 0.6735\n",
      "Epoch 140/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5248 - mse: 0.5248 - val_loss: 0.6743 - val_mse: 0.6743\n",
      "Epoch 141/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5249 - mse: 0.5249 - val_loss: 0.6842 - val_mse: 0.6842\n",
      "Epoch 142/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5244 - mse: 0.5244 - val_loss: 0.6601 - val_mse: 0.6601\n",
      "Epoch 143/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5241 - mse: 0.5241 - val_loss: 0.7089 - val_mse: 0.7089\n",
      "Epoch 144/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5236 - mse: 0.5236 - val_loss: 0.6868 - val_mse: 0.6868\n",
      "Epoch 145/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5230 - mse: 0.5230 - val_loss: 0.6879 - val_mse: 0.6879\n",
      "Epoch 146/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5227 - mse: 0.5227 - val_loss: 0.6736 - val_mse: 0.6736\n",
      "Epoch 147/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5228 - mse: 0.5228 - val_loss: 0.6700 - val_mse: 0.6700\n",
      "Epoch 148/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5225 - mse: 0.5225 - val_loss: 0.6759 - val_mse: 0.6759\n",
      "Epoch 149/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5219 - mse: 0.5219 - val_loss: 0.6788 - val_mse: 0.6788\n",
      "Epoch 150/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5215 - mse: 0.5215 - val_loss: 0.6994 - val_mse: 0.6994\n",
      "Epoch 151/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5213 - mse: 0.5213 - val_loss: 0.6742 - val_mse: 0.6742\n",
      "Epoch 152/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5211 - mse: 0.5211 - val_loss: 0.6848 - val_mse: 0.6848\n",
      "Epoch 153/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5208 - mse: 0.5208 - val_loss: 0.6799 - val_mse: 0.6799\n",
      "Epoch 154/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5203 - mse: 0.5203 - val_loss: 0.6927 - val_mse: 0.6927\n",
      "Epoch 155/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5202 - mse: 0.5202 - val_loss: 0.6874 - val_mse: 0.6874\n",
      "Epoch 156/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5196 - mse: 0.5196 - val_loss: 0.6811 - val_mse: 0.6811\n",
      "Epoch 157/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5195 - mse: 0.5195 - val_loss: 0.6561 - val_mse: 0.6561\n",
      "Epoch 158/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5190 - mse: 0.5190 - val_loss: 0.6637 - val_mse: 0.6637\n",
      "Epoch 159/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5189 - mse: 0.5189 - val_loss: 0.6619 - val_mse: 0.6619\n",
      "Epoch 160/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5185 - mse: 0.5185 - val_loss: 0.6747 - val_mse: 0.6747\n",
      "Epoch 161/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5184 - mse: 0.5184 - val_loss: 0.6727 - val_mse: 0.6727\n",
      "Epoch 162/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5177 - mse: 0.5177 - val_loss: 0.6932 - val_mse: 0.6932\n",
      "Epoch 163/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5174 - mse: 0.5174 - val_loss: 0.6811 - val_mse: 0.6811\n",
      "Epoch 164/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5173 - mse: 0.5173 - val_loss: 0.6675 - val_mse: 0.6675\n",
      "Epoch 165/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5170 - mse: 0.5170 - val_loss: 0.6615 - val_mse: 0.6615\n",
      "Epoch 166/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5165 - mse: 0.5165 - val_loss: 0.6859 - val_mse: 0.6859\n",
      "Epoch 167/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5164 - mse: 0.5164 - val_loss: 0.6708 - val_mse: 0.6708\n",
      "Epoch 168/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5166 - mse: 0.5166 - val_loss: 0.6948 - val_mse: 0.6948\n",
      "Epoch 169/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5162 - mse: 0.5162 - val_loss: 0.6825 - val_mse: 0.6825\n",
      "Epoch 170/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5158 - mse: 0.5158 - val_loss: 0.6822 - val_mse: 0.6822\n",
      "Epoch 171/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5154 - mse: 0.5154 - val_loss: 0.6547 - val_mse: 0.6547\n",
      "Epoch 172/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5155 - mse: 0.5155 - val_loss: 0.6575 - val_mse: 0.6575\n",
      "Epoch 173/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5150 - mse: 0.5150 - val_loss: 0.6635 - val_mse: 0.6635\n",
      "Epoch 174/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5149 - mse: 0.5149 - val_loss: 0.6779 - val_mse: 0.6779\n",
      "Epoch 175/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5144 - mse: 0.5144 - val_loss: 0.6525 - val_mse: 0.6525\n",
      "Epoch 176/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5143 - mse: 0.5143 - val_loss: 0.6710 - val_mse: 0.6710\n",
      "Epoch 177/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5140 - mse: 0.5140 - val_loss: 0.6858 - val_mse: 0.6858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5138 - mse: 0.5138 - val_loss: 0.6778 - val_mse: 0.6778\n",
      "Epoch 179/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5137 - mse: 0.5137 - val_loss: 0.6565 - val_mse: 0.6565\n",
      "Epoch 180/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5132 - mse: 0.5132 - val_loss: 0.7022 - val_mse: 0.7022\n",
      "Epoch 181/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5129 - mse: 0.5129 - val_loss: 0.6725 - val_mse: 0.6725\n",
      "Epoch 182/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5127 - mse: 0.5127 - val_loss: 0.6642 - val_mse: 0.6642\n",
      "Epoch 183/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5127 - mse: 0.5127 - val_loss: 0.6676 - val_mse: 0.6676\n",
      "Epoch 184/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5123 - mse: 0.5123 - val_loss: 0.6670 - val_mse: 0.6670\n",
      "Epoch 185/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5121 - mse: 0.5121 - val_loss: 0.6606 - val_mse: 0.6606\n",
      "Epoch 186/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5121 - mse: 0.5121 - val_loss: 0.6774 - val_mse: 0.6774\n",
      "Epoch 187/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5115 - mse: 0.5115 - val_loss: 0.6710 - val_mse: 0.6710\n",
      "Epoch 188/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5112 - mse: 0.5112 - val_loss: 0.6722 - val_mse: 0.6722\n",
      "Epoch 189/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5109 - mse: 0.5109 - val_loss: 0.6580 - val_mse: 0.6580\n",
      "Epoch 190/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5110 - mse: 0.5110 - val_loss: 0.6677 - val_mse: 0.6677\n",
      "Epoch 191/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5104 - mse: 0.5104 - val_loss: 0.6710 - val_mse: 0.6710\n",
      "Epoch 192/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5104 - mse: 0.5104 - val_loss: 0.6671 - val_mse: 0.6671\n",
      "Epoch 193/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5102 - mse: 0.5102 - val_loss: 0.6765 - val_mse: 0.6765\n",
      "Epoch 194/200\n",
      "3353317/3353317 [==============================] - 17s 5us/step - loss: 0.5101 - mse: 0.5101 - val_loss: 0.6958 - val_mse: 0.6958\n",
      "Epoch 195/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5098 - mse: 0.5098 - val_loss: 0.6650 - val_mse: 0.6650\n",
      "Epoch 196/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5096 - mse: 0.5096 - val_loss: 0.6625 - val_mse: 0.6625\n",
      "Epoch 197/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5093 - mse: 0.5093 - val_loss: 0.6577 - val_mse: 0.6577\n",
      "Epoch 198/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5092 - mse: 0.5092 - val_loss: 0.6611 - val_mse: 0.6611\n",
      "Epoch 199/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5091 - mse: 0.5091 - val_loss: 0.6688 - val_mse: 0.6688\n",
      "Epoch 200/200\n",
      "3353317/3353317 [==============================] - 16s 5us/step - loss: 0.5092 - mse: 0.5092 - val_loss: 0.6942 - val_mse: 0.6942\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.8598 - mse: 0.8598 - val_loss: 0.8350 - val_mse: 0.8350\n",
      "Epoch 2/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.8030 - mse: 0.8030 - val_loss: 0.8252 - val_mse: 0.8252\n",
      "Epoch 3/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7820 - mse: 0.7820 - val_loss: 0.7891 - val_mse: 0.7891\n",
      "Epoch 4/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7675 - mse: 0.7675 - val_loss: 0.7894 - val_mse: 0.7894\n",
      "Epoch 5/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7586 - mse: 0.7586 - val_loss: 0.7594 - val_mse: 0.7594\n",
      "Epoch 6/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7510 - mse: 0.7510 - val_loss: 0.7529 - val_mse: 0.7529\n",
      "Epoch 7/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7453 - mse: 0.7453 - val_loss: 0.7463 - val_mse: 0.7463\n",
      "Epoch 8/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7403 - mse: 0.7403 - val_loss: 0.7595 - val_mse: 0.7595\n",
      "Epoch 9/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7358 - mse: 0.7358 - val_loss: 0.7572 - val_mse: 0.7572\n",
      "Epoch 10/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7306 - mse: 0.7306 - val_loss: 0.7394 - val_mse: 0.7394\n",
      "Epoch 11/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7266 - mse: 0.7266 - val_loss: 0.7477 - val_mse: 0.7477\n",
      "Epoch 12/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7225 - mse: 0.7225 - val_loss: 0.7454 - val_mse: 0.7454\n",
      "Epoch 13/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7189 - mse: 0.7189 - val_loss: 0.7521 - val_mse: 0.7521\n",
      "Epoch 14/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7155 - mse: 0.7155 - val_loss: 0.7620 - val_mse: 0.7620\n",
      "Epoch 15/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7123 - mse: 0.7123 - val_loss: 0.7341 - val_mse: 0.7341\n",
      "Epoch 16/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7090 - mse: 0.7090 - val_loss: 0.7447 - val_mse: 0.7447\n",
      "Epoch 17/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7060 - mse: 0.7060 - val_loss: 0.7431 - val_mse: 0.7431\n",
      "Epoch 18/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7035 - mse: 0.7035 - val_loss: 0.7513 - val_mse: 0.7513\n",
      "Epoch 19/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.7002 - mse: 0.7002 - val_loss: 0.7354 - val_mse: 0.7354\n",
      "Epoch 20/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6980 - mse: 0.6980 - val_loss: 0.7629 - val_mse: 0.7629\n",
      "Epoch 21/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6957 - mse: 0.6957 - val_loss: 0.7464 - val_mse: 0.7464\n",
      "Epoch 22/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6929 - mse: 0.6929 - val_loss: 0.7386 - val_mse: 0.7386\n",
      "Epoch 23/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6906 - mse: 0.6906 - val_loss: 0.7663 - val_mse: 0.7663\n",
      "Epoch 24/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6886 - mse: 0.6886 - val_loss: 0.7488 - val_mse: 0.7488\n",
      "Epoch 25/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6868 - mse: 0.6868 - val_loss: 0.7567 - val_mse: 0.7567\n",
      "Epoch 26/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6840 - mse: 0.6840 - val_loss: 0.7730 - val_mse: 0.7730\n",
      "Epoch 27/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6822 - mse: 0.6822 - val_loss: 0.7730 - val_mse: 0.7730\n",
      "Epoch 28/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6805 - mse: 0.6805 - val_loss: 0.7606 - val_mse: 0.7606\n",
      "Epoch 29/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6785 - mse: 0.6785 - val_loss: 0.7494 - val_mse: 0.7494\n",
      "Epoch 30/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6768 - mse: 0.6768 - val_loss: 0.7737 - val_mse: 0.7737\n",
      "Epoch 31/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6748 - mse: 0.6748 - val_loss: 0.7535 - val_mse: 0.7535\n",
      "Epoch 32/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6731 - mse: 0.6731 - val_loss: 0.7697 - val_mse: 0.7697\n",
      "Epoch 33/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6715 - mse: 0.6715 - val_loss: 0.7532 - val_mse: 0.7532\n",
      "Epoch 34/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6698 - mse: 0.6698 - val_loss: 0.7743 - val_mse: 0.7743\n",
      "Epoch 35/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6681 - mse: 0.6681 - val_loss: 0.7516 - val_mse: 0.7516\n",
      "Epoch 36/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6664 - mse: 0.6664 - val_loss: 0.7641 - val_mse: 0.7641\n",
      "Epoch 37/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6650 - mse: 0.6650 - val_loss: 0.7701 - val_mse: 0.7701\n",
      "Epoch 38/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6634 - mse: 0.6634 - val_loss: 0.7565 - val_mse: 0.7565\n",
      "Epoch 39/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6615 - mse: 0.6615 - val_loss: 0.7700 - val_mse: 0.7700\n",
      "Epoch 40/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6603 - mse: 0.6603 - val_loss: 0.7574 - val_mse: 0.7574\n",
      "Epoch 41/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6586 - mse: 0.6586 - val_loss: 0.7560 - val_mse: 0.7560\n",
      "Epoch 42/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6573 - mse: 0.6573 - val_loss: 0.7770 - val_mse: 0.7770\n",
      "Epoch 43/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6561 - mse: 0.6561 - val_loss: 0.7830 - val_mse: 0.7830\n",
      "Epoch 44/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6543 - mse: 0.6543 - val_loss: 0.7817 - val_mse: 0.7817\n",
      "Epoch 45/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6528 - mse: 0.6528 - val_loss: 0.7628 - val_mse: 0.7628\n",
      "Epoch 46/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6519 - mse: 0.6519 - val_loss: 0.7832 - val_mse: 0.7832\n",
      "Epoch 47/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6502 - mse: 0.6502 - val_loss: 0.7767 - val_mse: 0.7767\n",
      "Epoch 48/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6492 - mse: 0.6492 - val_loss: 0.7776 - val_mse: 0.7776\n",
      "Epoch 49/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6476 - mse: 0.6476 - val_loss: 0.7714 - val_mse: 0.7714\n",
      "Epoch 50/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6460 - mse: 0.6460 - val_loss: 0.7930 - val_mse: 0.7930\n",
      "Epoch 51/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6453 - mse: 0.6453 - val_loss: 0.7968 - val_mse: 0.7968\n",
      "Epoch 52/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6439 - mse: 0.6439 - val_loss: 0.8219 - val_mse: 0.8219\n",
      "Epoch 53/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6429 - mse: 0.6429 - val_loss: 0.8036 - val_mse: 0.8036\n",
      "Epoch 54/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6422 - mse: 0.6422 - val_loss: 0.7918 - val_mse: 0.7918\n",
      "Epoch 55/200\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 0.6408 - mse: 0.6408 - val_loss: 0.7811 - val_mse: 0.7811\n",
      "Epoch 56/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6398 - mse: 0.6398 - val_loss: 0.8199 - val_mse: 0.8199\n",
      "Epoch 57/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6384 - mse: 0.6384 - val_loss: 0.7753 - val_mse: 0.7753\n",
      "Epoch 58/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6382 - mse: 0.6382 - val_loss: 0.7729 - val_mse: 0.7729\n",
      "Epoch 59/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6366 - mse: 0.6366 - val_loss: 0.7984 - val_mse: 0.7984\n",
      "Epoch 60/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6354 - mse: 0.6354 - val_loss: 0.8104 - val_mse: 0.8104\n",
      "Epoch 61/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6349 - mse: 0.6349 - val_loss: 0.8527 - val_mse: 0.8527\n",
      "Epoch 62/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6337 - mse: 0.6337 - val_loss: 0.8189 - val_mse: 0.8189\n",
      "Epoch 63/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6328 - mse: 0.6328 - val_loss: 0.8119 - val_mse: 0.8119\n",
      "Epoch 64/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6316 - mse: 0.6316 - val_loss: 0.7880 - val_mse: 0.7880\n",
      "Epoch 65/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6311 - mse: 0.6311 - val_loss: 0.8574 - val_mse: 0.8574\n",
      "Epoch 66/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6304 - mse: 0.6304 - val_loss: 0.8113 - val_mse: 0.8113\n",
      "Epoch 67/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6296 - mse: 0.6296 - val_loss: 0.8358 - val_mse: 0.8358\n",
      "Epoch 68/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6288 - mse: 0.6288 - val_loss: 0.8707 - val_mse: 0.8707\n",
      "Epoch 69/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6278 - mse: 0.6278 - val_loss: 0.8063 - val_mse: 0.8063\n",
      "Epoch 70/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6269 - mse: 0.6269 - val_loss: 0.7747 - val_mse: 0.7747\n",
      "Epoch 71/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6265 - mse: 0.6265 - val_loss: 0.8886 - val_mse: 0.8886\n",
      "Epoch 72/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6254 - mse: 0.6254 - val_loss: 0.8223 - val_mse: 0.8223\n",
      "Epoch 73/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6249 - mse: 0.6249 - val_loss: 0.8733 - val_mse: 0.8733\n",
      "Epoch 74/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6242 - mse: 0.6242 - val_loss: 0.8915 - val_mse: 0.8915\n",
      "Epoch 75/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6230 - mse: 0.6230 - val_loss: 0.7994 - val_mse: 0.7994\n",
      "Epoch 76/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6225 - mse: 0.6225 - val_loss: 0.8012 - val_mse: 0.8012\n",
      "Epoch 77/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6218 - mse: 0.6218 - val_loss: 0.8325 - val_mse: 0.8325\n",
      "Epoch 78/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6214 - mse: 0.6214 - val_loss: 0.8464 - val_mse: 0.8464\n",
      "Epoch 79/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6207 - mse: 0.6207 - val_loss: 0.8491 - val_mse: 0.8491\n",
      "Epoch 80/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6200 - mse: 0.6200 - val_loss: 0.9192 - val_mse: 0.9192\n",
      "Epoch 81/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6192 - mse: 0.6192 - val_loss: 0.7999 - val_mse: 0.7999\n",
      "Epoch 82/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6188 - mse: 0.6188 - val_loss: 0.8098 - val_mse: 0.8098\n",
      "Epoch 83/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6179 - mse: 0.6179 - val_loss: 0.7908 - val_mse: 0.7908\n",
      "Epoch 84/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6172 - mse: 0.6172 - val_loss: 0.8275 - val_mse: 0.8275\n",
      "Epoch 85/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6168 - mse: 0.6168 - val_loss: 0.8418 - val_mse: 0.8418\n",
      "Epoch 86/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6163 - mse: 0.6163 - val_loss: 0.8673 - val_mse: 0.8673\n",
      "Epoch 87/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6154 - mse: 0.6154 - val_loss: 0.8639 - val_mse: 0.8639\n",
      "Epoch 88/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6150 - mse: 0.6150 - val_loss: 0.9004 - val_mse: 0.9004\n",
      "Epoch 89/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6146 - mse: 0.6146 - val_loss: 0.8382 - val_mse: 0.8382\n",
      "Epoch 90/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6141 - mse: 0.6141 - val_loss: 0.8722 - val_mse: 0.8722\n",
      "Epoch 91/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6138 - mse: 0.6138 - val_loss: 0.8043 - val_mse: 0.8043\n",
      "Epoch 92/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6130 - mse: 0.6130 - val_loss: 0.8553 - val_mse: 0.8553\n",
      "Epoch 93/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6126 - mse: 0.6126 - val_loss: 0.9539 - val_mse: 0.9539\n",
      "Epoch 94/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6118 - mse: 0.6118 - val_loss: 0.8288 - val_mse: 0.8288\n",
      "Epoch 95/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6113 - mse: 0.6113 - val_loss: 0.9395 - val_mse: 0.9395\n",
      "Epoch 96/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6105 - mse: 0.6105 - val_loss: 0.8659 - val_mse: 0.8659\n",
      "Epoch 97/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6099 - mse: 0.6099 - val_loss: 0.9074 - val_mse: 0.9074\n",
      "Epoch 98/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6097 - mse: 0.6097 - val_loss: 0.8608 - val_mse: 0.8608\n",
      "Epoch 99/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6093 - mse: 0.6093 - val_loss: 1.0009 - val_mse: 1.0009\n",
      "Epoch 100/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6087 - mse: 0.6087 - val_loss: 0.9156 - val_mse: 0.9156\n",
      "Epoch 101/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6084 - mse: 0.6084 - val_loss: 0.8759 - val_mse: 0.8759\n",
      "Epoch 102/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6078 - mse: 0.6078 - val_loss: 0.9808 - val_mse: 0.9808\n",
      "Epoch 103/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6073 - mse: 0.6073 - val_loss: 0.8616 - val_mse: 0.8616\n",
      "Epoch 104/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6067 - mse: 0.6067 - val_loss: 0.8888 - val_mse: 0.8888\n",
      "Epoch 105/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6065 - mse: 0.6065 - val_loss: 0.8966 - val_mse: 0.8966\n",
      "Epoch 106/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6056 - mse: 0.6056 - val_loss: 0.9259 - val_mse: 0.9259\n",
      "Epoch 107/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6049 - mse: 0.6049 - val_loss: 0.9819 - val_mse: 0.9819\n",
      "Epoch 108/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6047 - mse: 0.6047 - val_loss: 0.8955 - val_mse: 0.8955\n",
      "Epoch 109/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6045 - mse: 0.6045 - val_loss: 0.9770 - val_mse: 0.9770\n",
      "Epoch 110/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6040 - mse: 0.6040 - val_loss: 0.8692 - val_mse: 0.8692\n",
      "Epoch 111/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6033 - mse: 0.6033 - val_loss: 0.9800 - val_mse: 0.9800\n",
      "Epoch 112/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6032 - mse: 0.6032 - val_loss: 0.9972 - val_mse: 0.9972\n",
      "Epoch 113/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6027 - mse: 0.6027 - val_loss: 1.0747 - val_mse: 1.0747\n",
      "Epoch 114/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6021 - mse: 0.6021 - val_loss: 0.9271 - val_mse: 0.9271\n",
      "Epoch 115/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6020 - mse: 0.6020 - val_loss: 0.8459 - val_mse: 0.8459\n",
      "Epoch 116/200\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 0.6011 - mse: 0.6011 - val_loss: 0.9001 - val_mse: 0.9001\n",
      "Epoch 117/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6007 - mse: 0.6007 - val_loss: 0.9134 - val_mse: 0.9134\n",
      "Epoch 118/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6004 - mse: 0.6004 - val_loss: 1.0394 - val_mse: 1.0394\n",
      "Epoch 119/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.6000 - mse: 0.6000 - val_loss: 0.9567 - val_mse: 0.9567\n",
      "Epoch 120/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5999 - mse: 0.5999 - val_loss: 0.9045 - val_mse: 0.9045\n",
      "Epoch 121/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5994 - mse: 0.5994 - val_loss: 0.8672 - val_mse: 0.8672\n",
      "Epoch 122/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5988 - mse: 0.5988 - val_loss: 0.8760 - val_mse: 0.8760\n",
      "Epoch 123/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5985 - mse: 0.5985 - val_loss: 1.0263 - val_mse: 1.0263\n",
      "Epoch 124/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5985 - mse: 0.5985 - val_loss: 1.0645 - val_mse: 1.0645\n",
      "Epoch 125/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5978 - mse: 0.5978 - val_loss: 0.9281 - val_mse: 0.9281\n",
      "Epoch 126/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5974 - mse: 0.5974 - val_loss: 0.9610 - val_mse: 0.9610\n",
      "Epoch 127/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5970 - mse: 0.5970 - val_loss: 0.9683 - val_mse: 0.9683\n",
      "Epoch 128/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5968 - mse: 0.5968 - val_loss: 0.9489 - val_mse: 0.9489\n",
      "Epoch 129/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5967 - mse: 0.5967 - val_loss: 0.8925 - val_mse: 0.8925\n",
      "Epoch 130/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5963 - mse: 0.5963 - val_loss: 0.9068 - val_mse: 0.9068\n",
      "Epoch 131/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5959 - mse: 0.5959 - val_loss: 1.0331 - val_mse: 1.0331\n",
      "Epoch 132/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5955 - mse: 0.5955 - val_loss: 0.9131 - val_mse: 0.9131\n",
      "Epoch 133/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5955 - mse: 0.5955 - val_loss: 0.9297 - val_mse: 0.9297\n",
      "Epoch 134/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5947 - mse: 0.5947 - val_loss: 0.8507 - val_mse: 0.8507\n",
      "Epoch 135/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5944 - mse: 0.5944 - val_loss: 0.9780 - val_mse: 0.9780\n",
      "Epoch 136/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5940 - mse: 0.5940 - val_loss: 0.8764 - val_mse: 0.8764\n",
      "Epoch 137/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5940 - mse: 0.5940 - val_loss: 0.8612 - val_mse: 0.8612\n",
      "Epoch 138/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5936 - mse: 0.5936 - val_loss: 0.8873 - val_mse: 0.8873\n",
      "Epoch 139/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5929 - mse: 0.5929 - val_loss: 1.0259 - val_mse: 1.0259\n",
      "Epoch 140/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5927 - mse: 0.5927 - val_loss: 0.9877 - val_mse: 0.9877\n",
      "Epoch 141/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5927 - mse: 0.5927 - val_loss: 0.8830 - val_mse: 0.8830\n",
      "Epoch 142/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5918 - mse: 0.5918 - val_loss: 0.9930 - val_mse: 0.9930\n",
      "Epoch 143/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5916 - mse: 0.5916 - val_loss: 0.9755 - val_mse: 0.9755\n",
      "Epoch 144/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5922 - mse: 0.5922 - val_loss: 0.9069 - val_mse: 0.9069\n",
      "Epoch 145/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5913 - mse: 0.5913 - val_loss: 0.9360 - val_mse: 0.9360\n",
      "Epoch 146/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5911 - mse: 0.5911 - val_loss: 0.9257 - val_mse: 0.9257\n",
      "Epoch 147/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5908 - mse: 0.5908 - val_loss: 0.8695 - val_mse: 0.8695\n",
      "Epoch 148/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5904 - mse: 0.5904 - val_loss: 0.9350 - val_mse: 0.9350\n",
      "Epoch 149/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5902 - mse: 0.5902 - val_loss: 0.9218 - val_mse: 0.9218\n",
      "Epoch 150/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5898 - mse: 0.5898 - val_loss: 1.0588 - val_mse: 1.0588\n",
      "Epoch 151/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5896 - mse: 0.5896 - val_loss: 0.9349 - val_mse: 0.9349\n",
      "Epoch 152/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5896 - mse: 0.5896 - val_loss: 0.9401 - val_mse: 0.9401\n",
      "Epoch 153/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5893 - mse: 0.5893 - val_loss: 0.8895 - val_mse: 0.8895\n",
      "Epoch 154/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5887 - mse: 0.5887 - val_loss: 0.9513 - val_mse: 0.9513\n",
      "Epoch 155/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5885 - mse: 0.5885 - val_loss: 1.0042 - val_mse: 1.0042\n",
      "Epoch 156/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5880 - mse: 0.5880 - val_loss: 0.9147 - val_mse: 0.9147\n",
      "Epoch 157/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5881 - mse: 0.5881 - val_loss: 0.9396 - val_mse: 0.9396\n",
      "Epoch 158/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5880 - mse: 0.5880 - val_loss: 0.9092 - val_mse: 0.9092\n",
      "Epoch 159/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5876 - mse: 0.5876 - val_loss: 0.8941 - val_mse: 0.8941\n",
      "Epoch 160/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5872 - mse: 0.5872 - val_loss: 1.0028 - val_mse: 1.0028\n",
      "Epoch 161/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5872 - mse: 0.5872 - val_loss: 0.9749 - val_mse: 0.9749\n",
      "Epoch 162/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5864 - mse: 0.5864 - val_loss: 0.8004 - val_mse: 0.8004\n",
      "Epoch 163/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5869 - mse: 0.5869 - val_loss: 1.0097 - val_mse: 1.0097\n",
      "Epoch 164/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5863 - mse: 0.5863 - val_loss: 0.9231 - val_mse: 0.9231\n",
      "Epoch 165/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5860 - mse: 0.5860 - val_loss: 0.9473 - val_mse: 0.9473\n",
      "Epoch 166/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5855 - mse: 0.5855 - val_loss: 0.9818 - val_mse: 0.9818\n",
      "Epoch 167/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5852 - mse: 0.5852 - val_loss: 0.9627 - val_mse: 0.9627\n",
      "Epoch 168/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5852 - mse: 0.5852 - val_loss: 0.9202 - val_mse: 0.9202\n",
      "Epoch 169/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5847 - mse: 0.5847 - val_loss: 1.0588 - val_mse: 1.0588\n",
      "Epoch 170/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5845 - mse: 0.5845 - val_loss: 1.0029 - val_mse: 1.0029\n",
      "Epoch 171/200\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 0.5845 - mse: 0.5845 - val_loss: 1.0336 - val_mse: 1.0336\n",
      "Epoch 172/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5843 - mse: 0.5843 - val_loss: 0.9738 - val_mse: 0.9738\n",
      "Epoch 173/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5841 - mse: 0.5841 - val_loss: 0.9599 - val_mse: 0.9599\n",
      "Epoch 174/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5837 - mse: 0.5837 - val_loss: 0.9671 - val_mse: 0.9671\n",
      "Epoch 175/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5836 - mse: 0.5836 - val_loss: 0.9217 - val_mse: 0.9217\n",
      "Epoch 176/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5834 - mse: 0.5834 - val_loss: 1.0157 - val_mse: 1.0157\n",
      "Epoch 177/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5831 - mse: 0.5831 - val_loss: 0.9150 - val_mse: 0.9150\n",
      "Epoch 178/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5831 - mse: 0.5831 - val_loss: 1.0103 - val_mse: 1.0103\n",
      "Epoch 179/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5828 - mse: 0.5828 - val_loss: 1.0547 - val_mse: 1.0547\n",
      "Epoch 180/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5828 - mse: 0.5828 - val_loss: 1.0473 - val_mse: 1.0473\n",
      "Epoch 181/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5823 - mse: 0.5823 - val_loss: 0.9579 - val_mse: 0.9579\n",
      "Epoch 182/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5818 - mse: 0.5818 - val_loss: 1.0381 - val_mse: 1.0381\n",
      "Epoch 183/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5816 - mse: 0.5816 - val_loss: 1.1099 - val_mse: 1.1099\n",
      "Epoch 184/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5817 - mse: 0.5817 - val_loss: 0.9819 - val_mse: 0.9819\n",
      "Epoch 185/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5813 - mse: 0.5813 - val_loss: 0.9634 - val_mse: 0.9634\n",
      "Epoch 186/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5809 - mse: 0.5809 - val_loss: 1.0196 - val_mse: 1.0196\n",
      "Epoch 187/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5814 - mse: 0.5814 - val_loss: 0.9791 - val_mse: 0.9791\n",
      "Epoch 188/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5805 - mse: 0.5805 - val_loss: 1.1193 - val_mse: 1.1193\n",
      "Epoch 189/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5807 - mse: 0.5807 - val_loss: 1.0530 - val_mse: 1.0530\n",
      "Epoch 190/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5805 - mse: 0.5805 - val_loss: 0.9880 - val_mse: 0.9880\n",
      "Epoch 191/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5804 - mse: 0.5804 - val_loss: 1.1044 - val_mse: 1.1044\n",
      "Epoch 192/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5802 - mse: 0.5802 - val_loss: 1.1939 - val_mse: 1.1939\n",
      "Epoch 193/200\n",
      "3353318/3353318 [==============================] - 17s 5us/step - loss: 0.5800 - mse: 0.5800 - val_loss: 0.9939 - val_mse: 0.9939\n",
      "Epoch 194/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5795 - mse: 0.5795 - val_loss: 1.1503 - val_mse: 1.1503\n",
      "Epoch 195/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5792 - mse: 0.5792 - val_loss: 0.9843 - val_mse: 0.9843\n",
      "Epoch 196/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5792 - mse: 0.5792 - val_loss: 1.0435 - val_mse: 1.0435\n",
      "Epoch 197/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5790 - mse: 0.5790 - val_loss: 1.0578 - val_mse: 1.0578\n",
      "Epoch 198/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5785 - mse: 0.5785 - val_loss: 1.0657 - val_mse: 1.0657\n",
      "Epoch 199/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5788 - mse: 0.5788 - val_loss: 1.0224 - val_mse: 1.0224\n",
      "Epoch 200/200\n",
      "3353318/3353318 [==============================] - 16s 5us/step - loss: 0.5782 - mse: 0.5782 - val_loss: 1.0513 - val_mse: 1.0513\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.9038 - mse: 0.9038 - val_loss: 0.7977 - val_mse: 0.7977\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8260 - mse: 0.8260 - val_loss: 0.7688 - val_mse: 0.7688\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8068 - mse: 0.8068 - val_loss: 0.7686 - val_mse: 0.7686\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7926 - mse: 0.7926 - val_loss: 0.7501 - val_mse: 0.7501\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7794 - mse: 0.7794 - val_loss: 0.7596 - val_mse: 0.7596\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8255 - mse: 0.8255 - val_loss: 0.7929 - val_mse: 0.7929\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7470 - mse: 0.7470 - val_loss: 0.7881 - val_mse: 0.7881\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7356 - mse: 0.7356 - val_loss: 0.7806 - val_mse: 0.7806\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7255 - mse: 0.7255 - val_loss: 0.7745 - val_mse: 0.7745\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7171 - mse: 0.7171 - val_loss: 0.7737 - val_mse: 0.7737\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.9373 - mse: 0.9373 - val_loss: 0.8129 - val_mse: 0.8129\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8388 - mse: 0.8388 - val_loss: 0.8333 - val_mse: 0.8333\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8200 - mse: 0.8200 - val_loss: 0.8302 - val_mse: 0.8302\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8070 - mse: 0.8070 - val_loss: 0.8271 - val_mse: 0.8271\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7956 - mse: 0.7956 - val_loss: 0.8014 - val_mse: 0.8014\n",
      "1676658/1676658 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.9047 - mse: 0.9047 - val_loss: 0.7844 - val_mse: 0.7844\n",
      "Epoch 2/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8244 - mse: 0.8244 - val_loss: 0.7636 - val_mse: 0.7636\n",
      "Epoch 3/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8050 - mse: 0.8050 - val_loss: 0.8098 - val_mse: 0.8098\n",
      "Epoch 4/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7906 - mse: 0.7906 - val_loss: 0.7684 - val_mse: 0.7684\n",
      "Epoch 5/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7783 - mse: 0.7783 - val_loss: 0.7581 - val_mse: 0.7581\n",
      "Epoch 6/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7668 - mse: 0.7668 - val_loss: 0.7475 - val_mse: 0.7475\n",
      "Epoch 7/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7578 - mse: 0.7578 - val_loss: 0.7418 - val_mse: 0.7418\n",
      "Epoch 8/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7500 - mse: 0.7500 - val_loss: 0.7415 - val_mse: 0.7415\n",
      "Epoch 9/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7434 - mse: 0.7434 - val_loss: 0.7338 - val_mse: 0.7338\n",
      "Epoch 10/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7385 - mse: 0.7385 - val_loss: 0.7664 - val_mse: 0.7664\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7955 - mse: 0.7955 - val_loss: 0.7786 - val_mse: 0.7786\n",
      "Epoch 2/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7291 - mse: 0.7291 - val_loss: 0.7663 - val_mse: 0.7663\n",
      "Epoch 3/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7155 - mse: 0.7155 - val_loss: 0.7658 - val_mse: 0.7658\n",
      "Epoch 4/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7046 - mse: 0.7046 - val_loss: 0.7537 - val_mse: 0.7537\n",
      "Epoch 5/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6953 - mse: 0.6953 - val_loss: 0.7515 - val_mse: 0.7515\n",
      "Epoch 6/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6880 - mse: 0.6880 - val_loss: 0.7469 - val_mse: 0.7469\n",
      "Epoch 7/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6813 - mse: 0.6813 - val_loss: 0.7425 - val_mse: 0.7425\n",
      "Epoch 8/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6762 - mse: 0.6762 - val_loss: 0.7441 - val_mse: 0.7441\n",
      "Epoch 9/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6715 - mse: 0.6715 - val_loss: 0.7381 - val_mse: 0.7381\n",
      "Epoch 10/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6675 - mse: 0.6675 - val_loss: 0.7371 - val_mse: 0.7371\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/10\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.9314 - mse: 0.9314 - val_loss: 0.8226 - val_mse: 0.8226\n",
      "Epoch 2/10\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8389 - mse: 0.8389 - val_loss: 0.8391 - val_mse: 0.8391\n",
      "Epoch 3/10\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8195 - mse: 0.8195 - val_loss: 0.8324 - val_mse: 0.8324\n",
      "Epoch 4/10\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8060 - mse: 0.8060 - val_loss: 0.8126 - val_mse: 0.8126\n",
      "Epoch 5/10\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7943 - mse: 0.7943 - val_loss: 0.8044 - val_mse: 0.8044\n",
      "Epoch 6/10\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7856 - mse: 0.7856 - val_loss: 0.7924 - val_mse: 0.7924\n",
      "Epoch 7/10\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7780 - mse: 0.7780 - val_loss: 0.7825 - val_mse: 0.7825\n",
      "Epoch 8/10\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7722 - mse: 0.7722 - val_loss: 0.7835 - val_mse: 0.7835\n",
      "Epoch 9/10\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7674 - mse: 0.7674 - val_loss: 0.7739 - val_mse: 0.7739\n",
      "Epoch 10/10\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7628 - mse: 0.7628 - val_loss: 0.7829 - val_mse: 0.7829\n",
      "1676658/1676658 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.9169 - mse: 0.9169 - val_loss: 0.7718 - val_mse: 0.7718\n",
      "Epoch 2/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8264 - mse: 0.8264 - val_loss: 0.7682 - val_mse: 0.7682\n",
      "Epoch 3/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8065 - mse: 0.8065 - val_loss: 0.7632 - val_mse: 0.7632\n",
      "Epoch 4/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7917 - mse: 0.7917 - val_loss: 0.7612 - val_mse: 0.7612\n",
      "Epoch 5/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7788 - mse: 0.7788 - val_loss: 0.7504 - val_mse: 0.7504\n",
      "Epoch 6/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7675 - mse: 0.7675 - val_loss: 0.7452 - val_mse: 0.7452\n",
      "Epoch 7/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7574 - mse: 0.7574 - val_loss: 0.7429 - val_mse: 0.7429\n",
      "Epoch 8/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7490 - mse: 0.7490 - val_loss: 0.7394 - val_mse: 0.7394\n",
      "Epoch 9/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7424 - mse: 0.7424 - val_loss: 0.7456 - val_mse: 0.7456\n",
      "Epoch 10/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7363 - mse: 0.7363 - val_loss: 0.7366 - val_mse: 0.7366\n",
      "Epoch 11/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7318 - mse: 0.7318 - val_loss: 0.7299 - val_mse: 0.7299\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7273 - mse: 0.7273 - val_loss: 0.7226 - val_mse: 0.7226\n",
      "Epoch 13/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7238 - mse: 0.7238 - val_loss: 0.7237 - val_mse: 0.7237\n",
      "Epoch 14/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7204 - mse: 0.7204 - val_loss: 0.7158 - val_mse: 0.7158\n",
      "Epoch 15/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7185 - mse: 0.7185 - val_loss: 0.7148 - val_mse: 0.7148\n",
      "Epoch 16/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7152 - mse: 0.7152 - val_loss: 0.7126 - val_mse: 0.7126\n",
      "Epoch 17/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7128 - mse: 0.7128 - val_loss: 0.7106 - val_mse: 0.7106\n",
      "Epoch 18/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7104 - mse: 0.7104 - val_loss: 0.7096 - val_mse: 0.7096\n",
      "Epoch 19/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7085 - mse: 0.7085 - val_loss: 0.7127 - val_mse: 0.7127\n",
      "Epoch 20/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7062 - mse: 0.7062 - val_loss: 0.7050 - val_mse: 0.7050\n",
      "Epoch 21/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7037 - mse: 0.7037 - val_loss: 0.7188 - val_mse: 0.7188\n",
      "Epoch 22/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7016 - mse: 0.7016 - val_loss: 0.7008 - val_mse: 0.7008\n",
      "Epoch 23/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7001 - mse: 0.7001 - val_loss: 0.7004 - val_mse: 0.7004\n",
      "Epoch 24/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6983 - mse: 0.6983 - val_loss: 0.7126 - val_mse: 0.7126\n",
      "Epoch 25/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6967 - mse: 0.6967 - val_loss: 0.7059 - val_mse: 0.7059\n",
      "Epoch 26/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6951 - mse: 0.6951 - val_loss: 0.7025 - val_mse: 0.7025\n",
      "Epoch 27/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6930 - mse: 0.6930 - val_loss: 0.7001 - val_mse: 0.7001\n",
      "Epoch 28/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6917 - mse: 0.6917 - val_loss: 0.7016 - val_mse: 0.7016\n",
      "Epoch 29/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6895 - mse: 0.6895 - val_loss: 0.7035 - val_mse: 0.7035\n",
      "Epoch 30/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6883 - mse: 0.6883 - val_loss: 0.6918 - val_mse: 0.6918\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8074 - mse: 0.8074 - val_loss: 0.8025 - val_mse: 0.8025\n",
      "Epoch 2/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7435 - mse: 0.7435 - val_loss: 0.7864 - val_mse: 0.7864\n",
      "Epoch 3/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7332 - mse: 0.7332 - val_loss: 0.7840 - val_mse: 0.7840\n",
      "Epoch 4/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7231 - mse: 0.7231 - val_loss: 0.7782 - val_mse: 0.7782\n",
      "Epoch 5/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7146 - mse: 0.7146 - val_loss: 0.7732 - val_mse: 0.7732\n",
      "Epoch 6/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7072 - mse: 0.7072 - val_loss: 0.7640 - val_mse: 0.7640\n",
      "Epoch 7/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7011 - mse: 0.7011 - val_loss: 0.7712 - val_mse: 0.7712\n",
      "Epoch 8/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6956 - mse: 0.6956 - val_loss: 0.7598 - val_mse: 0.7598\n",
      "Epoch 9/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6912 - mse: 0.6912 - val_loss: 0.7609 - val_mse: 0.7609\n",
      "Epoch 10/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6880 - mse: 0.6880 - val_loss: 0.7534 - val_mse: 0.7534\n",
      "Epoch 11/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6851 - mse: 0.6851 - val_loss: 0.7579 - val_mse: 0.7579\n",
      "Epoch 12/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6825 - mse: 0.6825 - val_loss: 0.7565 - val_mse: 0.7565\n",
      "Epoch 13/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6805 - mse: 0.6805 - val_loss: 0.7543 - val_mse: 0.7543\n",
      "Epoch 14/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6778 - mse: 0.6778 - val_loss: 0.7503 - val_mse: 0.7503\n",
      "Epoch 15/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6765 - mse: 0.6765 - val_loss: 0.7522 - val_mse: 0.7522\n",
      "Epoch 16/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6742 - mse: 0.6742 - val_loss: 0.7509 - val_mse: 0.7509\n",
      "Epoch 17/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6731 - mse: 0.6731 - val_loss: 0.7465 - val_mse: 0.7465\n",
      "Epoch 18/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6709 - mse: 0.6709 - val_loss: 0.7427 - val_mse: 0.7427\n",
      "Epoch 19/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6701 - mse: 0.6701 - val_loss: 0.7424 - val_mse: 0.7424\n",
      "Epoch 20/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6687 - mse: 0.6687 - val_loss: 0.7495 - val_mse: 0.7495\n",
      "Epoch 21/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6669 - mse: 0.6669 - val_loss: 0.7525 - val_mse: 0.7525\n",
      "Epoch 22/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6661 - mse: 0.6661 - val_loss: 0.7355 - val_mse: 0.7355\n",
      "Epoch 23/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6640 - mse: 0.6640 - val_loss: 0.7659 - val_mse: 0.7659\n",
      "Epoch 24/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6637 - mse: 0.6637 - val_loss: 0.7347 - val_mse: 0.7347\n",
      "Epoch 25/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6628 - mse: 0.6628 - val_loss: 0.7385 - val_mse: 0.7385\n",
      "Epoch 26/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6608 - mse: 0.6608 - val_loss: 0.7327 - val_mse: 0.7327\n",
      "Epoch 27/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6602 - mse: 0.6602 - val_loss: 0.7327 - val_mse: 0.7327\n",
      "Epoch 28/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6592 - mse: 0.6592 - val_loss: 0.7373 - val_mse: 0.7373\n",
      "Epoch 29/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6578 - mse: 0.6578 - val_loss: 0.7272 - val_mse: 0.7272\n",
      "Epoch 30/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6570 - mse: 0.6570 - val_loss: 0.7313 - val_mse: 0.7313\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.9297 - mse: 0.9297 - val_loss: 0.8215 - val_mse: 0.8215\n",
      "Epoch 2/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8368 - mse: 0.8368 - val_loss: 0.8341 - val_mse: 0.8341\n",
      "Epoch 3/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8191 - mse: 0.8191 - val_loss: 0.8279 - val_mse: 0.8279\n",
      "Epoch 4/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8059 - mse: 0.8059 - val_loss: 0.8183 - val_mse: 0.8183\n",
      "Epoch 5/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7944 - mse: 0.7944 - val_loss: 0.8018 - val_mse: 0.8018\n",
      "Epoch 6/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7849 - mse: 0.7849 - val_loss: 0.8064 - val_mse: 0.8064\n",
      "Epoch 7/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7781 - mse: 0.7781 - val_loss: 0.7962 - val_mse: 0.7962\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7711 - mse: 0.7711 - val_loss: 0.7846 - val_mse: 0.7846\n",
      "Epoch 9/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7664 - mse: 0.7664 - val_loss: 0.7768 - val_mse: 0.7768\n",
      "Epoch 10/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7618 - mse: 0.7618 - val_loss: 0.7763 - val_mse: 0.7763\n",
      "Epoch 11/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7573 - mse: 0.7573 - val_loss: 0.7672 - val_mse: 0.7672\n",
      "Epoch 12/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7545 - mse: 0.7545 - val_loss: 0.7672 - val_mse: 0.7672\n",
      "Epoch 13/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7507 - mse: 0.7507 - val_loss: 0.7671 - val_mse: 0.7671\n",
      "Epoch 14/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7482 - mse: 0.7482 - val_loss: 0.7604 - val_mse: 0.7604\n",
      "Epoch 15/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7459 - mse: 0.7459 - val_loss: 0.7646 - val_mse: 0.7646\n",
      "Epoch 16/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7430 - mse: 0.7430 - val_loss: 0.7574 - val_mse: 0.7574\n",
      "Epoch 17/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7411 - mse: 0.7411 - val_loss: 0.7571 - val_mse: 0.7571\n",
      "Epoch 18/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7393 - mse: 0.7393 - val_loss: 0.7583 - val_mse: 0.7583\n",
      "Epoch 19/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7367 - mse: 0.7367 - val_loss: 0.7520 - val_mse: 0.7520\n",
      "Epoch 20/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7358 - mse: 0.7358 - val_loss: 0.7641 - val_mse: 0.7641\n",
      "Epoch 21/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7329 - mse: 0.7329 - val_loss: 0.7494 - val_mse: 0.7494\n",
      "Epoch 22/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7319 - mse: 0.7319 - val_loss: 0.7509 - val_mse: 0.7509\n",
      "Epoch 23/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7297 - mse: 0.7297 - val_loss: 0.7493 - val_mse: 0.7493\n",
      "Epoch 24/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7283 - mse: 0.7283 - val_loss: 0.7467 - val_mse: 0.7467\n",
      "Epoch 25/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7264 - mse: 0.7264 - val_loss: 0.7464 - val_mse: 0.7464\n",
      "Epoch 26/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7254 - mse: 0.7254 - val_loss: 0.7427 - val_mse: 0.7427\n",
      "Epoch 27/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7237 - mse: 0.7237 - val_loss: 0.7435 - val_mse: 0.7435\n",
      "Epoch 28/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7223 - mse: 0.7223 - val_loss: 0.7514 - val_mse: 0.7514\n",
      "Epoch 29/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7209 - mse: 0.7209 - val_loss: 0.7487 - val_mse: 0.7487\n",
      "Epoch 30/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7195 - mse: 0.7195 - val_loss: 0.7500 - val_mse: 0.7500\n",
      "1676658/1676658 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.9123 - mse: 0.9123 - val_loss: 0.7773 - val_mse: 0.7773\n",
      "Epoch 2/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8281 - mse: 0.8281 - val_loss: 0.7695 - val_mse: 0.7695\n",
      "Epoch 3/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8076 - mse: 0.8076 - val_loss: 0.7574 - val_mse: 0.7574\n",
      "Epoch 4/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7917 - mse: 0.7917 - val_loss: 0.7554 - val_mse: 0.7554\n",
      "Epoch 5/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7776 - mse: 0.7776 - val_loss: 0.7692 - val_mse: 0.7692\n",
      "Epoch 6/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7660 - mse: 0.7660 - val_loss: 0.7891 - val_mse: 0.7891\n",
      "Epoch 7/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7561 - mse: 0.7561 - val_loss: 0.7393 - val_mse: 0.7393\n",
      "Epoch 8/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7479 - mse: 0.7479 - val_loss: 0.7335 - val_mse: 0.7335\n",
      "Epoch 9/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7413 - mse: 0.7413 - val_loss: 0.7397 - val_mse: 0.7397\n",
      "Epoch 10/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7364 - mse: 0.7364 - val_loss: 0.7376 - val_mse: 0.7376\n",
      "Epoch 11/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7312 - mse: 0.7312 - val_loss: 0.7263 - val_mse: 0.7263\n",
      "Epoch 12/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7275 - mse: 0.7275 - val_loss: 0.7538 - val_mse: 0.7538\n",
      "Epoch 13/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7242 - mse: 0.7242 - val_loss: 0.7240 - val_mse: 0.7240\n",
      "Epoch 14/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7209 - mse: 0.7209 - val_loss: 0.7404 - val_mse: 0.7404\n",
      "Epoch 15/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7181 - mse: 0.7181 - val_loss: 0.7157 - val_mse: 0.7157\n",
      "Epoch 16/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7162 - mse: 0.7162 - val_loss: 0.7221 - val_mse: 0.7221\n",
      "Epoch 17/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7135 - mse: 0.7135 - val_loss: 0.7227 - val_mse: 0.7227\n",
      "Epoch 18/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7108 - mse: 0.7108 - val_loss: 0.7173 - val_mse: 0.7173\n",
      "Epoch 19/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7091 - mse: 0.7091 - val_loss: 0.7308 - val_mse: 0.7308\n",
      "Epoch 20/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7067 - mse: 0.7067 - val_loss: 0.7133 - val_mse: 0.7133\n",
      "Epoch 21/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7050 - mse: 0.7050 - val_loss: 0.7275 - val_mse: 0.7275\n",
      "Epoch 22/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7027 - mse: 0.7027 - val_loss: 0.7089 - val_mse: 0.7089\n",
      "Epoch 23/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7011 - mse: 0.7011 - val_loss: 0.7181 - val_mse: 0.7181\n",
      "Epoch 24/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6993 - mse: 0.6993 - val_loss: 0.7044 - val_mse: 0.7044\n",
      "Epoch 25/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6982 - mse: 0.6982 - val_loss: 0.6994 - val_mse: 0.6994\n",
      "Epoch 26/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6960 - mse: 0.6960 - val_loss: 0.7094 - val_mse: 0.7094\n",
      "Epoch 27/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6946 - mse: 0.6946 - val_loss: 0.7195 - val_mse: 0.7195\n",
      "Epoch 28/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6926 - mse: 0.6926 - val_loss: 0.7151 - val_mse: 0.7151\n",
      "Epoch 29/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6914 - mse: 0.6914 - val_loss: 0.7066 - val_mse: 0.7066\n",
      "Epoch 30/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6900 - mse: 0.6900 - val_loss: 0.7036 - val_mse: 0.7036\n",
      "Epoch 31/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6881 - mse: 0.6881 - val_loss: 0.7023 - val_mse: 0.7023\n",
      "Epoch 32/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6870 - mse: 0.6870 - val_loss: 0.6946 - val_mse: 0.6946\n",
      "Epoch 33/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6851 - mse: 0.6851 - val_loss: 0.6926 - val_mse: 0.6926\n",
      "Epoch 34/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6841 - mse: 0.6841 - val_loss: 0.6972 - val_mse: 0.6972\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6826 - mse: 0.6826 - val_loss: 0.7020 - val_mse: 0.7020\n",
      "Epoch 36/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6814 - mse: 0.6814 - val_loss: 0.6947 - val_mse: 0.6947\n",
      "Epoch 37/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6802 - mse: 0.6802 - val_loss: 0.6928 - val_mse: 0.6928\n",
      "Epoch 38/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6789 - mse: 0.6789 - val_loss: 0.6880 - val_mse: 0.6880\n",
      "Epoch 39/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6780 - mse: 0.6780 - val_loss: 0.6888 - val_mse: 0.6888\n",
      "Epoch 40/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6764 - mse: 0.6764 - val_loss: 0.6895 - val_mse: 0.6895\n",
      "Epoch 41/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6755 - mse: 0.6755 - val_loss: 0.6919 - val_mse: 0.6919\n",
      "Epoch 42/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6746 - mse: 0.6746 - val_loss: 0.6915 - val_mse: 0.6915\n",
      "Epoch 43/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6730 - mse: 0.6730 - val_loss: 0.6906 - val_mse: 0.6906\n",
      "Epoch 44/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6717 - mse: 0.6717 - val_loss: 0.7073 - val_mse: 0.7073\n",
      "Epoch 45/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6712 - mse: 0.6712 - val_loss: 0.6861 - val_mse: 0.6861\n",
      "Epoch 46/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6700 - mse: 0.6700 - val_loss: 0.6869 - val_mse: 0.6869\n",
      "Epoch 47/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6688 - mse: 0.6688 - val_loss: 0.6850 - val_mse: 0.6850\n",
      "Epoch 48/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6672 - mse: 0.6672 - val_loss: 0.6836 - val_mse: 0.6836\n",
      "Epoch 49/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6667 - mse: 0.6667 - val_loss: 0.7075 - val_mse: 0.7075\n",
      "Epoch 50/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6659 - mse: 0.6659 - val_loss: 0.6920 - val_mse: 0.6920\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7943 - mse: 0.7943 - val_loss: 0.7707 - val_mse: 0.7707\n",
      "Epoch 2/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7295 - mse: 0.7295 - val_loss: 0.7653 - val_mse: 0.7653\n",
      "Epoch 3/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7163 - mse: 0.7163 - val_loss: 0.7617 - val_mse: 0.7617\n",
      "Epoch 4/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7059 - mse: 0.7059 - val_loss: 0.7558 - val_mse: 0.7558\n",
      "Epoch 5/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6971 - mse: 0.6971 - val_loss: 0.7498 - val_mse: 0.7498\n",
      "Epoch 6/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6896 - mse: 0.6896 - val_loss: 0.7487 - val_mse: 0.7487\n",
      "Epoch 7/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6839 - mse: 0.6839 - val_loss: 0.7488 - val_mse: 0.7488\n",
      "Epoch 8/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6780 - mse: 0.6780 - val_loss: 0.7478 - val_mse: 0.7478\n",
      "Epoch 9/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6738 - mse: 0.6738 - val_loss: 0.7442 - val_mse: 0.7442\n",
      "Epoch 10/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6698 - mse: 0.6698 - val_loss: 0.7361 - val_mse: 0.7361\n",
      "Epoch 11/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6659 - mse: 0.6659 - val_loss: 0.7411 - val_mse: 0.7411\n",
      "Epoch 12/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6634 - mse: 0.6634 - val_loss: 0.7299 - val_mse: 0.7299\n",
      "Epoch 13/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6603 - mse: 0.6603 - val_loss: 0.7349 - val_mse: 0.7349\n",
      "Epoch 14/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6576 - mse: 0.6576 - val_loss: 0.7331 - val_mse: 0.7331\n",
      "Epoch 15/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6553 - mse: 0.6553 - val_loss: 0.7254 - val_mse: 0.7254\n",
      "Epoch 16/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6534 - mse: 0.6534 - val_loss: 0.7210 - val_mse: 0.7210\n",
      "Epoch 17/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6512 - mse: 0.6512 - val_loss: 0.7207 - val_mse: 0.7207\n",
      "Epoch 18/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6493 - mse: 0.6493 - val_loss: 0.7240 - val_mse: 0.7240\n",
      "Epoch 19/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6473 - mse: 0.6473 - val_loss: 0.7192 - val_mse: 0.7192\n",
      "Epoch 20/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6456 - mse: 0.6456 - val_loss: 0.7196 - val_mse: 0.7196\n",
      "Epoch 21/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6441 - mse: 0.6441 - val_loss: 0.7167 - val_mse: 0.7167\n",
      "Epoch 22/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6418 - mse: 0.6418 - val_loss: 0.7160 - val_mse: 0.7160\n",
      "Epoch 23/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6415 - mse: 0.6415 - val_loss: 0.7130 - val_mse: 0.7130\n",
      "Epoch 24/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6396 - mse: 0.6396 - val_loss: 0.7138 - val_mse: 0.7138\n",
      "Epoch 25/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6384 - mse: 0.6384 - val_loss: 0.7115 - val_mse: 0.7115\n",
      "Epoch 26/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6368 - mse: 0.6368 - val_loss: 0.7043 - val_mse: 0.7043\n",
      "Epoch 27/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6350 - mse: 0.6350 - val_loss: 0.7081 - val_mse: 0.7081\n",
      "Epoch 28/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6344 - mse: 0.6344 - val_loss: 0.7107 - val_mse: 0.7107\n",
      "Epoch 29/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6328 - mse: 0.6328 - val_loss: 0.7058 - val_mse: 0.7058\n",
      "Epoch 30/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6312 - mse: 0.6312 - val_loss: 0.7048 - val_mse: 0.7048\n",
      "Epoch 31/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6304 - mse: 0.6304 - val_loss: 0.7048 - val_mse: 0.7048\n",
      "Epoch 32/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6289 - mse: 0.6289 - val_loss: 0.7034 - val_mse: 0.7034\n",
      "Epoch 33/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6283 - mse: 0.6283 - val_loss: 0.7066 - val_mse: 0.7066\n",
      "Epoch 34/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6263 - mse: 0.6263 - val_loss: 0.7012 - val_mse: 0.7012\n",
      "Epoch 35/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6255 - mse: 0.6255 - val_loss: 0.7041 - val_mse: 0.7041\n",
      "Epoch 36/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6245 - mse: 0.6245 - val_loss: 0.7030 - val_mse: 0.7030\n",
      "Epoch 37/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6233 - mse: 0.6233 - val_loss: 0.6984 - val_mse: 0.6984\n",
      "Epoch 38/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6223 - mse: 0.6223 - val_loss: 0.7043 - val_mse: 0.7043\n",
      "Epoch 39/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6211 - mse: 0.6211 - val_loss: 0.7003 - val_mse: 0.7003\n",
      "Epoch 40/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6201 - mse: 0.6201 - val_loss: 0.6988 - val_mse: 0.6988\n",
      "Epoch 41/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6196 - mse: 0.6196 - val_loss: 0.7023 - val_mse: 0.7023\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6181 - mse: 0.6181 - val_loss: 0.6968 - val_mse: 0.6968\n",
      "Epoch 43/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6175 - mse: 0.6175 - val_loss: 0.7028 - val_mse: 0.7028\n",
      "Epoch 44/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6164 - mse: 0.6164 - val_loss: 0.6934 - val_mse: 0.6934\n",
      "Epoch 45/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6158 - mse: 0.6158 - val_loss: 0.6963 - val_mse: 0.6963\n",
      "Epoch 46/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6148 - mse: 0.6148 - val_loss: 0.6915 - val_mse: 0.6915\n",
      "Epoch 47/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6137 - mse: 0.6137 - val_loss: 0.6927 - val_mse: 0.6927\n",
      "Epoch 48/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6128 - mse: 0.6128 - val_loss: 0.6952 - val_mse: 0.6952\n",
      "Epoch 49/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6119 - mse: 0.6119 - val_loss: 0.6952 - val_mse: 0.6952\n",
      "Epoch 50/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6110 - mse: 0.6110 - val_loss: 0.6966 - val_mse: 0.6966\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.9340 - mse: 0.9340 - val_loss: 0.8238 - val_mse: 0.8238\n",
      "Epoch 2/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8368 - mse: 0.8368 - val_loss: 0.8261 - val_mse: 0.8261\n",
      "Epoch 3/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8176 - mse: 0.8176 - val_loss: 0.8320 - val_mse: 0.8320\n",
      "Epoch 4/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8040 - mse: 0.8040 - val_loss: 0.8132 - val_mse: 0.8132\n",
      "Epoch 5/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7924 - mse: 0.7924 - val_loss: 0.8070 - val_mse: 0.8070\n",
      "Epoch 6/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7837 - mse: 0.7837 - val_loss: 0.8105 - val_mse: 0.8105\n",
      "Epoch 7/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7756 - mse: 0.7756 - val_loss: 0.7999 - val_mse: 0.7999\n",
      "Epoch 8/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7695 - mse: 0.7695 - val_loss: 0.7911 - val_mse: 0.7911\n",
      "Epoch 9/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7641 - mse: 0.7641 - val_loss: 0.7856 - val_mse: 0.7856\n",
      "Epoch 10/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7600 - mse: 0.7600 - val_loss: 0.7726 - val_mse: 0.7726\n",
      "Epoch 11/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7565 - mse: 0.7565 - val_loss: 0.7683 - val_mse: 0.7683\n",
      "Epoch 12/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7523 - mse: 0.7523 - val_loss: 0.7760 - val_mse: 0.7760\n",
      "Epoch 13/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7497 - mse: 0.7497 - val_loss: 0.7569 - val_mse: 0.7569\n",
      "Epoch 14/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7467 - mse: 0.7467 - val_loss: 0.7549 - val_mse: 0.7549\n",
      "Epoch 15/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7447 - mse: 0.7447 - val_loss: 0.7595 - val_mse: 0.7595\n",
      "Epoch 16/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7421 - mse: 0.7421 - val_loss: 0.7610 - val_mse: 0.7610\n",
      "Epoch 17/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7393 - mse: 0.7393 - val_loss: 0.7439 - val_mse: 0.7439\n",
      "Epoch 18/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7380 - mse: 0.7380 - val_loss: 0.7444 - val_mse: 0.7444\n",
      "Epoch 19/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7354 - mse: 0.7354 - val_loss: 0.7416 - val_mse: 0.7416\n",
      "Epoch 20/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7337 - mse: 0.7337 - val_loss: 0.7420 - val_mse: 0.7420\n",
      "Epoch 21/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7320 - mse: 0.7320 - val_loss: 0.7407 - val_mse: 0.7407\n",
      "Epoch 22/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7302 - mse: 0.7302 - val_loss: 0.7423 - val_mse: 0.7423\n",
      "Epoch 23/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7288 - mse: 0.7288 - val_loss: 0.7471 - val_mse: 0.7471\n",
      "Epoch 24/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7270 - mse: 0.7270 - val_loss: 0.7441 - val_mse: 0.7441\n",
      "Epoch 25/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7251 - mse: 0.7251 - val_loss: 0.7399 - val_mse: 0.7399\n",
      "Epoch 26/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7235 - mse: 0.7235 - val_loss: 0.7552 - val_mse: 0.7552\n",
      "Epoch 27/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7223 - mse: 0.7223 - val_loss: 0.7367 - val_mse: 0.7367\n",
      "Epoch 28/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7205 - mse: 0.7205 - val_loss: 0.7350 - val_mse: 0.7350\n",
      "Epoch 29/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7188 - mse: 0.7188 - val_loss: 0.7404 - val_mse: 0.7404\n",
      "Epoch 30/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7176 - mse: 0.7176 - val_loss: 0.7400 - val_mse: 0.7400\n",
      "Epoch 31/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7163 - mse: 0.7163 - val_loss: 0.7353 - val_mse: 0.7353\n",
      "Epoch 32/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7153 - mse: 0.7153 - val_loss: 0.7455 - val_mse: 0.7455\n",
      "Epoch 33/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7138 - mse: 0.7138 - val_loss: 0.7343 - val_mse: 0.7343\n",
      "Epoch 34/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7124 - mse: 0.7124 - val_loss: 0.7291 - val_mse: 0.7291\n",
      "Epoch 35/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7109 - mse: 0.7109 - val_loss: 0.7308 - val_mse: 0.7308\n",
      "Epoch 36/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7098 - mse: 0.7098 - val_loss: 0.7291 - val_mse: 0.7291\n",
      "Epoch 37/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7089 - mse: 0.7089 - val_loss: 0.7340 - val_mse: 0.7340\n",
      "Epoch 38/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7078 - mse: 0.7078 - val_loss: 0.7385 - val_mse: 0.7385\n",
      "Epoch 39/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7067 - mse: 0.7067 - val_loss: 0.7251 - val_mse: 0.7251\n",
      "Epoch 40/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7049 - mse: 0.7049 - val_loss: 0.7295 - val_mse: 0.7295\n",
      "Epoch 41/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7045 - mse: 0.7045 - val_loss: 0.7360 - val_mse: 0.7360\n",
      "Epoch 42/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7027 - mse: 0.7027 - val_loss: 0.7334 - val_mse: 0.7334\n",
      "Epoch 43/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7025 - mse: 0.7025 - val_loss: 0.7275 - val_mse: 0.7275\n",
      "Epoch 44/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7011 - mse: 0.7011 - val_loss: 0.7365 - val_mse: 0.7365\n",
      "Epoch 45/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6999 - mse: 0.6999 - val_loss: 0.7237 - val_mse: 0.7237\n",
      "Epoch 46/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6986 - mse: 0.6986 - val_loss: 0.7257 - val_mse: 0.7257\n",
      "Epoch 47/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6977 - mse: 0.6977 - val_loss: 0.7281 - val_mse: 0.7281\n",
      "Epoch 48/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6969 - mse: 0.6969 - val_loss: 0.7309 - val_mse: 0.7309\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6957 - mse: 0.6957 - val_loss: 0.7332 - val_mse: 0.7332\n",
      "Epoch 50/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6942 - mse: 0.6942 - val_loss: 0.7333 - val_mse: 0.7333\n",
      "1676658/1676658 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.9211 - mse: 0.9211 - val_loss: 0.7726 - val_mse: 0.7726\n",
      "Epoch 2/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8277 - mse: 0.8277 - val_loss: 0.7769 - val_mse: 0.7769\n",
      "Epoch 3/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8076 - mse: 0.8076 - val_loss: 0.7755 - val_mse: 0.7755\n",
      "Epoch 4/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7911 - mse: 0.7911 - val_loss: 0.7562 - val_mse: 0.7562\n",
      "Epoch 5/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7779 - mse: 0.7779 - val_loss: 0.7546 - val_mse: 0.7546\n",
      "Epoch 6/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7659 - mse: 0.7659 - val_loss: 0.7530 - val_mse: 0.7530\n",
      "Epoch 7/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7566 - mse: 0.7566 - val_loss: 0.7496 - val_mse: 0.7496\n",
      "Epoch 8/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7486 - mse: 0.7486 - val_loss: 0.7397 - val_mse: 0.7397\n",
      "Epoch 9/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7412 - mse: 0.7412 - val_loss: 0.7464 - val_mse: 0.7464\n",
      "Epoch 10/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7357 - mse: 0.7357 - val_loss: 0.7390 - val_mse: 0.7390\n",
      "Epoch 11/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7310 - mse: 0.7310 - val_loss: 0.7244 - val_mse: 0.7244\n",
      "Epoch 12/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7265 - mse: 0.7265 - val_loss: 0.7286 - val_mse: 0.7286\n",
      "Epoch 13/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7230 - mse: 0.7230 - val_loss: 0.7244 - val_mse: 0.7244\n",
      "Epoch 14/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7201 - mse: 0.7201 - val_loss: 0.7226 - val_mse: 0.7226\n",
      "Epoch 15/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7174 - mse: 0.7174 - val_loss: 0.7117 - val_mse: 0.7117\n",
      "Epoch 16/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7144 - mse: 0.7144 - val_loss: 0.7179 - val_mse: 0.7179\n",
      "Epoch 17/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7121 - mse: 0.7121 - val_loss: 0.7265 - val_mse: 0.7265\n",
      "Epoch 18/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7096 - mse: 0.7096 - val_loss: 0.7238 - val_mse: 0.7238\n",
      "Epoch 19/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7083 - mse: 0.7083 - val_loss: 0.7103 - val_mse: 0.7103\n",
      "Epoch 20/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7061 - mse: 0.7061 - val_loss: 0.7311 - val_mse: 0.7311\n",
      "Epoch 21/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7036 - mse: 0.7036 - val_loss: 0.7192 - val_mse: 0.7192\n",
      "Epoch 22/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7023 - mse: 0.7023 - val_loss: 0.7039 - val_mse: 0.7039\n",
      "Epoch 23/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6993 - mse: 0.6993 - val_loss: 0.7134 - val_mse: 0.7134\n",
      "Epoch 24/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6978 - mse: 0.6978 - val_loss: 0.7108 - val_mse: 0.7108\n",
      "Epoch 25/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6963 - mse: 0.6963 - val_loss: 0.6982 - val_mse: 0.6982\n",
      "Epoch 26/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6944 - mse: 0.6944 - val_loss: 0.7018 - val_mse: 0.7018\n",
      "Epoch 27/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6929 - mse: 0.6929 - val_loss: 0.6987 - val_mse: 0.6987\n",
      "Epoch 28/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6907 - mse: 0.6907 - val_loss: 0.6995 - val_mse: 0.6995\n",
      "Epoch 29/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6893 - mse: 0.6893 - val_loss: 0.7078 - val_mse: 0.7078\n",
      "Epoch 30/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6880 - mse: 0.6880 - val_loss: 0.7014 - val_mse: 0.7014\n",
      "Epoch 31/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6860 - mse: 0.6860 - val_loss: 0.7126 - val_mse: 0.7126\n",
      "Epoch 32/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6847 - mse: 0.6847 - val_loss: 0.7108 - val_mse: 0.7108\n",
      "Epoch 33/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6834 - mse: 0.6834 - val_loss: 0.6930 - val_mse: 0.6930\n",
      "Epoch 34/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6820 - mse: 0.6820 - val_loss: 0.6895 - val_mse: 0.6895\n",
      "Epoch 35/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6806 - mse: 0.6806 - val_loss: 0.6956 - val_mse: 0.6956\n",
      "Epoch 36/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6793 - mse: 0.6793 - val_loss: 0.6929 - val_mse: 0.6929\n",
      "Epoch 37/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6783 - mse: 0.6783 - val_loss: 0.6922 - val_mse: 0.6922\n",
      "Epoch 38/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6771 - mse: 0.6771 - val_loss: 0.7295 - val_mse: 0.7295\n",
      "Epoch 39/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6757 - mse: 0.6757 - val_loss: 0.7092 - val_mse: 0.7092\n",
      "Epoch 40/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6746 - mse: 0.6746 - val_loss: 0.7066 - val_mse: 0.7066\n",
      "Epoch 41/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6736 - mse: 0.6736 - val_loss: 0.6903 - val_mse: 0.6903\n",
      "Epoch 42/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6724 - mse: 0.6724 - val_loss: 0.6903 - val_mse: 0.6903\n",
      "Epoch 43/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6719 - mse: 0.6719 - val_loss: 0.6981 - val_mse: 0.6981\n",
      "Epoch 44/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6704 - mse: 0.6704 - val_loss: 0.6993 - val_mse: 0.6993\n",
      "Epoch 45/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6691 - mse: 0.6691 - val_loss: 0.6881 - val_mse: 0.6881\n",
      "Epoch 46/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6685 - mse: 0.6685 - val_loss: 0.6911 - val_mse: 0.6911\n",
      "Epoch 47/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6672 - mse: 0.6672 - val_loss: 0.6949 - val_mse: 0.6949\n",
      "Epoch 48/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6663 - mse: 0.6663 - val_loss: 0.6951 - val_mse: 0.6951\n",
      "Epoch 49/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6652 - mse: 0.6652 - val_loss: 0.6813 - val_mse: 0.6813\n",
      "Epoch 50/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6644 - mse: 0.6644 - val_loss: 0.7024 - val_mse: 0.7024\n",
      "Epoch 51/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6636 - mse: 0.6636 - val_loss: 0.6877 - val_mse: 0.6877\n",
      "Epoch 52/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6623 - mse: 0.6623 - val_loss: 0.6840 - val_mse: 0.6840\n",
      "Epoch 53/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6614 - mse: 0.6614 - val_loss: 0.6936 - val_mse: 0.6936\n",
      "Epoch 54/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6608 - mse: 0.6608 - val_loss: 0.6809 - val_mse: 0.6809\n",
      "Epoch 55/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6595 - mse: 0.6595 - val_loss: 0.6787 - val_mse: 0.6787\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6592 - mse: 0.6592 - val_loss: 0.6904 - val_mse: 0.6904\n",
      "Epoch 57/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6577 - mse: 0.6577 - val_loss: 0.6830 - val_mse: 0.6830\n",
      "Epoch 58/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6569 - mse: 0.6569 - val_loss: 0.6879 - val_mse: 0.6879\n",
      "Epoch 59/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6562 - mse: 0.6562 - val_loss: 0.6816 - val_mse: 0.6816\n",
      "Epoch 60/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6553 - mse: 0.6553 - val_loss: 0.6894 - val_mse: 0.6894\n",
      "Epoch 61/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6544 - mse: 0.6544 - val_loss: 0.7008 - val_mse: 0.7008\n",
      "Epoch 62/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6533 - mse: 0.6533 - val_loss: 0.6951 - val_mse: 0.6951\n",
      "Epoch 63/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6528 - mse: 0.6528 - val_loss: 0.6806 - val_mse: 0.6806\n",
      "Epoch 64/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6516 - mse: 0.6516 - val_loss: 0.6797 - val_mse: 0.6797\n",
      "Epoch 65/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6518 - mse: 0.6518 - val_loss: 0.6871 - val_mse: 0.6871\n",
      "Epoch 66/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6505 - mse: 0.6505 - val_loss: 0.6808 - val_mse: 0.6808\n",
      "Epoch 67/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6497 - mse: 0.6497 - val_loss: 0.6900 - val_mse: 0.6900\n",
      "Epoch 68/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6490 - mse: 0.6490 - val_loss: 0.6794 - val_mse: 0.6794\n",
      "Epoch 69/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6482 - mse: 0.6482 - val_loss: 0.6858 - val_mse: 0.6858\n",
      "Epoch 70/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6470 - mse: 0.6470 - val_loss: 0.6791 - val_mse: 0.6791\n",
      "Epoch 71/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6467 - mse: 0.6467 - val_loss: 0.6809 - val_mse: 0.6809\n",
      "Epoch 72/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6462 - mse: 0.6462 - val_loss: 0.7064 - val_mse: 0.7064\n",
      "Epoch 73/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6456 - mse: 0.6456 - val_loss: 0.6813 - val_mse: 0.6813\n",
      "Epoch 74/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6446 - mse: 0.6446 - val_loss: 0.6897 - val_mse: 0.6897\n",
      "Epoch 75/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6433 - mse: 0.6433 - val_loss: 0.6992 - val_mse: 0.6992\n",
      "Epoch 76/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6430 - mse: 0.6430 - val_loss: 0.6904 - val_mse: 0.6904\n",
      "Epoch 77/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6430 - mse: 0.6430 - val_loss: 0.6845 - val_mse: 0.6845\n",
      "Epoch 78/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6417 - mse: 0.6417 - val_loss: 0.6808 - val_mse: 0.6808\n",
      "Epoch 79/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6409 - mse: 0.6409 - val_loss: 0.6937 - val_mse: 0.6937\n",
      "Epoch 80/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6401 - mse: 0.6401 - val_loss: 0.6927 - val_mse: 0.6927\n",
      "Epoch 81/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6397 - mse: 0.6397 - val_loss: 0.6765 - val_mse: 0.6765\n",
      "Epoch 82/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6390 - mse: 0.6390 - val_loss: 0.6736 - val_mse: 0.6736\n",
      "Epoch 83/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6381 - mse: 0.6381 - val_loss: 0.6896 - val_mse: 0.6896\n",
      "Epoch 84/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6376 - mse: 0.6376 - val_loss: 0.6800 - val_mse: 0.6800\n",
      "Epoch 85/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6369 - mse: 0.6369 - val_loss: 0.6920 - val_mse: 0.6920\n",
      "Epoch 86/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6357 - mse: 0.6357 - val_loss: 0.6757 - val_mse: 0.6757\n",
      "Epoch 87/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6354 - mse: 0.6354 - val_loss: 0.6927 - val_mse: 0.6927\n",
      "Epoch 88/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6347 - mse: 0.6347 - val_loss: 0.6759 - val_mse: 0.6759\n",
      "Epoch 89/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6346 - mse: 0.6346 - val_loss: 0.6781 - val_mse: 0.6781\n",
      "Epoch 90/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6334 - mse: 0.6334 - val_loss: 0.6765 - val_mse: 0.6765\n",
      "Epoch 91/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6331 - mse: 0.6331 - val_loss: 0.6798 - val_mse: 0.6798\n",
      "Epoch 92/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6327 - mse: 0.6327 - val_loss: 0.6824 - val_mse: 0.6824\n",
      "Epoch 93/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6318 - mse: 0.6318 - val_loss: 0.6855 - val_mse: 0.6855\n",
      "Epoch 94/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6312 - mse: 0.6312 - val_loss: 0.6732 - val_mse: 0.6732\n",
      "Epoch 95/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6302 - mse: 0.6302 - val_loss: 0.6749 - val_mse: 0.6749\n",
      "Epoch 96/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6303 - mse: 0.6303 - val_loss: 0.6958 - val_mse: 0.6958\n",
      "Epoch 97/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6294 - mse: 0.6294 - val_loss: 0.6931 - val_mse: 0.6931\n",
      "Epoch 98/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6288 - mse: 0.6288 - val_loss: 0.6772 - val_mse: 0.6772\n",
      "Epoch 99/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6282 - mse: 0.6282 - val_loss: 0.6865 - val_mse: 0.6865\n",
      "Epoch 100/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6274 - mse: 0.6274 - val_loss: 0.6987 - val_mse: 0.6987\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7969 - mse: 0.7969 - val_loss: 0.7710 - val_mse: 0.7710\n",
      "Epoch 2/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7285 - mse: 0.7285 - val_loss: 0.7637 - val_mse: 0.7637\n",
      "Epoch 3/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7158 - mse: 0.7158 - val_loss: 0.7610 - val_mse: 0.7610\n",
      "Epoch 4/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7062 - mse: 0.7062 - val_loss: 0.7527 - val_mse: 0.7527\n",
      "Epoch 5/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6983 - mse: 0.6983 - val_loss: 0.7538 - val_mse: 0.7538\n",
      "Epoch 6/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6910 - mse: 0.6910 - val_loss: 0.7513 - val_mse: 0.7513\n",
      "Epoch 7/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6851 - mse: 0.6851 - val_loss: 0.7456 - val_mse: 0.7456\n",
      "Epoch 8/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6805 - mse: 0.6805 - val_loss: 0.7442 - val_mse: 0.7442\n",
      "Epoch 9/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6751 - mse: 0.6751 - val_loss: 0.7408 - val_mse: 0.7408\n",
      "Epoch 10/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6713 - mse: 0.6713 - val_loss: 0.7436 - val_mse: 0.7436\n",
      "Epoch 11/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6682 - mse: 0.6682 - val_loss: 0.7457 - val_mse: 0.7457\n",
      "Epoch 12/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6650 - mse: 0.6650 - val_loss: 0.7360 - val_mse: 0.7360\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6622 - mse: 0.6622 - val_loss: 0.7338 - val_mse: 0.7338\n",
      "Epoch 14/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6600 - mse: 0.6600 - val_loss: 0.7369 - val_mse: 0.7369\n",
      "Epoch 15/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6574 - mse: 0.6574 - val_loss: 0.7540 - val_mse: 0.7540\n",
      "Epoch 16/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6555 - mse: 0.6555 - val_loss: 0.7332 - val_mse: 0.7332\n",
      "Epoch 17/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6531 - mse: 0.6531 - val_loss: 0.7283 - val_mse: 0.7283\n",
      "Epoch 18/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6517 - mse: 0.6517 - val_loss: 0.7297 - val_mse: 0.7297\n",
      "Epoch 19/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6499 - mse: 0.6499 - val_loss: 0.7274 - val_mse: 0.7274\n",
      "Epoch 20/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6477 - mse: 0.6477 - val_loss: 0.7238 - val_mse: 0.7238\n",
      "Epoch 21/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6466 - mse: 0.6466 - val_loss: 0.7357 - val_mse: 0.7357\n",
      "Epoch 22/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6447 - mse: 0.6447 - val_loss: 0.7170 - val_mse: 0.7170\n",
      "Epoch 23/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6434 - mse: 0.6434 - val_loss: 0.7163 - val_mse: 0.7163\n",
      "Epoch 24/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6414 - mse: 0.6414 - val_loss: 0.7093 - val_mse: 0.7093\n",
      "Epoch 25/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6406 - mse: 0.6406 - val_loss: 0.7189 - val_mse: 0.7189\n",
      "Epoch 26/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6384 - mse: 0.6384 - val_loss: 0.7106 - val_mse: 0.7106\n",
      "Epoch 27/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6376 - mse: 0.6376 - val_loss: 0.7125 - val_mse: 0.7125\n",
      "Epoch 28/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6355 - mse: 0.6355 - val_loss: 0.7054 - val_mse: 0.7054\n",
      "Epoch 29/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6349 - mse: 0.6349 - val_loss: 0.7153 - val_mse: 0.7153\n",
      "Epoch 30/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6332 - mse: 0.6332 - val_loss: 0.7037 - val_mse: 0.7037\n",
      "Epoch 31/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6315 - mse: 0.6315 - val_loss: 0.7047 - val_mse: 0.7047\n",
      "Epoch 32/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6306 - mse: 0.6306 - val_loss: 0.7043 - val_mse: 0.7043\n",
      "Epoch 33/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6292 - mse: 0.6292 - val_loss: 0.7044 - val_mse: 0.7044\n",
      "Epoch 34/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6283 - mse: 0.6283 - val_loss: 0.7045 - val_mse: 0.7045\n",
      "Epoch 35/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6274 - mse: 0.6274 - val_loss: 0.7178 - val_mse: 0.7178\n",
      "Epoch 36/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6261 - mse: 0.6261 - val_loss: 0.7041 - val_mse: 0.7041\n",
      "Epoch 37/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6246 - mse: 0.6246 - val_loss: 0.7107 - val_mse: 0.7107\n",
      "Epoch 38/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6235 - mse: 0.6235 - val_loss: 0.7021 - val_mse: 0.7021\n",
      "Epoch 39/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6227 - mse: 0.6227 - val_loss: 0.7018 - val_mse: 0.7018\n",
      "Epoch 40/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6221 - mse: 0.6221 - val_loss: 0.6998 - val_mse: 0.6998\n",
      "Epoch 41/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6203 - mse: 0.6203 - val_loss: 0.7051 - val_mse: 0.7051\n",
      "Epoch 42/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6192 - mse: 0.6192 - val_loss: 0.7060 - val_mse: 0.7060\n",
      "Epoch 43/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6182 - mse: 0.6182 - val_loss: 0.6966 - val_mse: 0.6966\n",
      "Epoch 44/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6177 - mse: 0.6177 - val_loss: 0.6942 - val_mse: 0.6942\n",
      "Epoch 45/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6164 - mse: 0.6164 - val_loss: 0.7094 - val_mse: 0.7094\n",
      "Epoch 46/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6161 - mse: 0.6161 - val_loss: 0.7114 - val_mse: 0.7114\n",
      "Epoch 47/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6143 - mse: 0.6143 - val_loss: 0.6970 - val_mse: 0.6970\n",
      "Epoch 48/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6136 - mse: 0.6136 - val_loss: 0.7028 - val_mse: 0.7028\n",
      "Epoch 49/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6133 - mse: 0.6133 - val_loss: 0.6990 - val_mse: 0.6990\n",
      "Epoch 50/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6115 - mse: 0.6115 - val_loss: 0.6968 - val_mse: 0.6968\n",
      "Epoch 51/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6113 - mse: 0.6113 - val_loss: 0.6984 - val_mse: 0.6984\n",
      "Epoch 52/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6101 - mse: 0.6101 - val_loss: 0.6970 - val_mse: 0.6970\n",
      "Epoch 53/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6095 - mse: 0.6095 - val_loss: 0.6898 - val_mse: 0.6898\n",
      "Epoch 54/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6089 - mse: 0.6089 - val_loss: 0.6942 - val_mse: 0.6942\n",
      "Epoch 55/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6081 - mse: 0.6081 - val_loss: 0.6941 - val_mse: 0.6941\n",
      "Epoch 56/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6072 - mse: 0.6072 - val_loss: 0.6992 - val_mse: 0.6992\n",
      "Epoch 57/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6065 - mse: 0.6065 - val_loss: 0.6942 - val_mse: 0.6942\n",
      "Epoch 58/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6058 - mse: 0.6058 - val_loss: 0.6871 - val_mse: 0.6871\n",
      "Epoch 59/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6047 - mse: 0.6047 - val_loss: 0.6960 - val_mse: 0.6960\n",
      "Epoch 60/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6039 - mse: 0.6039 - val_loss: 0.6988 - val_mse: 0.6988\n",
      "Epoch 61/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6034 - mse: 0.6034 - val_loss: 0.6983 - val_mse: 0.6983\n",
      "Epoch 62/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6031 - mse: 0.6031 - val_loss: 0.6877 - val_mse: 0.6877\n",
      "Epoch 63/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6024 - mse: 0.6024 - val_loss: 0.6888 - val_mse: 0.6888\n",
      "Epoch 64/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6013 - mse: 0.6013 - val_loss: 0.6913 - val_mse: 0.6913\n",
      "Epoch 65/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6005 - mse: 0.6005 - val_loss: 0.6838 - val_mse: 0.6838\n",
      "Epoch 66/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6001 - mse: 0.6001 - val_loss: 0.6885 - val_mse: 0.6885\n",
      "Epoch 67/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5998 - mse: 0.5998 - val_loss: 0.7007 - val_mse: 0.7007\n",
      "Epoch 68/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5990 - mse: 0.5990 - val_loss: 0.6995 - val_mse: 0.6995\n",
      "Epoch 69/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5984 - mse: 0.5984 - val_loss: 0.6984 - val_mse: 0.6984\n",
      "Epoch 70/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5979 - mse: 0.5979 - val_loss: 0.6876 - val_mse: 0.6876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5974 - mse: 0.5974 - val_loss: 0.6829 - val_mse: 0.6829\n",
      "Epoch 72/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5967 - mse: 0.5967 - val_loss: 0.6959 - val_mse: 0.6959\n",
      "Epoch 73/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5958 - mse: 0.5958 - val_loss: 0.6791 - val_mse: 0.6791\n",
      "Epoch 74/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5954 - mse: 0.5954 - val_loss: 0.6891 - val_mse: 0.6891\n",
      "Epoch 75/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5947 - mse: 0.5947 - val_loss: 0.6787 - val_mse: 0.6787\n",
      "Epoch 76/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5944 - mse: 0.5944 - val_loss: 0.6825 - val_mse: 0.6825\n",
      "Epoch 77/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5934 - mse: 0.5934 - val_loss: 0.6818 - val_mse: 0.6818\n",
      "Epoch 78/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5930 - mse: 0.5930 - val_loss: 0.6841 - val_mse: 0.6841\n",
      "Epoch 79/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5928 - mse: 0.5928 - val_loss: 0.6917 - val_mse: 0.6917\n",
      "Epoch 80/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5919 - mse: 0.5919 - val_loss: 0.6907 - val_mse: 0.6907\n",
      "Epoch 81/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5918 - mse: 0.5918 - val_loss: 0.6918 - val_mse: 0.6918\n",
      "Epoch 82/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5910 - mse: 0.5910 - val_loss: 0.6847 - val_mse: 0.6847\n",
      "Epoch 83/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5910 - mse: 0.5910 - val_loss: 0.6819 - val_mse: 0.6819\n",
      "Epoch 84/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5900 - mse: 0.5900 - val_loss: 0.6825 - val_mse: 0.6825\n",
      "Epoch 85/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5897 - mse: 0.5897 - val_loss: 0.6878 - val_mse: 0.6878\n",
      "Epoch 86/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5888 - mse: 0.5888 - val_loss: 0.6843 - val_mse: 0.6843\n",
      "Epoch 87/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5891 - mse: 0.5891 - val_loss: 0.6858 - val_mse: 0.6858\n",
      "Epoch 88/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5883 - mse: 0.5883 - val_loss: 0.6890 - val_mse: 0.6890\n",
      "Epoch 89/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5875 - mse: 0.5875 - val_loss: 0.6843 - val_mse: 0.6843\n",
      "Epoch 90/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5873 - mse: 0.5873 - val_loss: 0.6909 - val_mse: 0.6909\n",
      "Epoch 91/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5870 - mse: 0.5870 - val_loss: 0.6809 - val_mse: 0.6809\n",
      "Epoch 92/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5862 - mse: 0.5862 - val_loss: 0.6902 - val_mse: 0.6902\n",
      "Epoch 93/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5861 - mse: 0.5861 - val_loss: 0.6829 - val_mse: 0.6829\n",
      "Epoch 94/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5855 - mse: 0.5855 - val_loss: 0.6901 - val_mse: 0.6901\n",
      "Epoch 95/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5849 - mse: 0.5849 - val_loss: 0.6813 - val_mse: 0.6813\n",
      "Epoch 96/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5843 - mse: 0.5843 - val_loss: 0.6809 - val_mse: 0.6809\n",
      "Epoch 97/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5844 - mse: 0.5844 - val_loss: 0.6891 - val_mse: 0.6891\n",
      "Epoch 98/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5839 - mse: 0.5839 - val_loss: 0.6855 - val_mse: 0.6855\n",
      "Epoch 99/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5834 - mse: 0.5834 - val_loss: 0.6861 - val_mse: 0.6861\n",
      "Epoch 100/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5830 - mse: 0.5830 - val_loss: 0.6937 - val_mse: 0.6937\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.9243 - mse: 0.9243 - val_loss: 0.8163 - val_mse: 0.8163\n",
      "Epoch 2/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8368 - mse: 0.8368 - val_loss: 0.8155 - val_mse: 0.8155\n",
      "Epoch 3/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8183 - mse: 0.8183 - val_loss: 0.8325 - val_mse: 0.8325\n",
      "Epoch 4/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8047 - mse: 0.8047 - val_loss: 0.8285 - val_mse: 0.8285\n",
      "Epoch 5/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7933 - mse: 0.7934 - val_loss: 0.8172 - val_mse: 0.8172\n",
      "Epoch 6/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7844 - mse: 0.7844 - val_loss: 0.7982 - val_mse: 0.7982\n",
      "Epoch 7/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7760 - mse: 0.7760 - val_loss: 0.8021 - val_mse: 0.8021\n",
      "Epoch 8/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7703 - mse: 0.7703 - val_loss: 0.7832 - val_mse: 0.7832\n",
      "Epoch 9/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7654 - mse: 0.7654 - val_loss: 0.7799 - val_mse: 0.7799\n",
      "Epoch 10/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7608 - mse: 0.7608 - val_loss: 0.7751 - val_mse: 0.7751\n",
      "Epoch 11/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7571 - mse: 0.7571 - val_loss: 0.7739 - val_mse: 0.7739\n",
      "Epoch 12/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7539 - mse: 0.7539 - val_loss: 0.7621 - val_mse: 0.7621\n",
      "Epoch 13/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7508 - mse: 0.7508 - val_loss: 0.7636 - val_mse: 0.7636\n",
      "Epoch 14/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7487 - mse: 0.7487 - val_loss: 0.7550 - val_mse: 0.7550\n",
      "Epoch 15/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7456 - mse: 0.7456 - val_loss: 0.7591 - val_mse: 0.7591\n",
      "Epoch 16/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7437 - mse: 0.7437 - val_loss: 0.7583 - val_mse: 0.7583\n",
      "Epoch 17/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7419 - mse: 0.7419 - val_loss: 0.7537 - val_mse: 0.7537\n",
      "Epoch 18/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7390 - mse: 0.7390 - val_loss: 0.7519 - val_mse: 0.7519\n",
      "Epoch 19/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7373 - mse: 0.7373 - val_loss: 0.7516 - val_mse: 0.7516\n",
      "Epoch 20/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7355 - mse: 0.7355 - val_loss: 0.7659 - val_mse: 0.7659\n",
      "Epoch 21/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7336 - mse: 0.7336 - val_loss: 0.7643 - val_mse: 0.7643\n",
      "Epoch 22/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7323 - mse: 0.7323 - val_loss: 0.7482 - val_mse: 0.7482\n",
      "Epoch 23/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7302 - mse: 0.7302 - val_loss: 0.7547 - val_mse: 0.7547\n",
      "Epoch 24/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7295 - mse: 0.7295 - val_loss: 0.7470 - val_mse: 0.7470\n",
      "Epoch 25/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7273 - mse: 0.7273 - val_loss: 0.7473 - val_mse: 0.7473\n",
      "Epoch 26/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7260 - mse: 0.7260 - val_loss: 0.7493 - val_mse: 0.7493\n",
      "Epoch 27/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7250 - mse: 0.7250 - val_loss: 0.7488 - val_mse: 0.7488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7228 - mse: 0.7228 - val_loss: 0.7521 - val_mse: 0.7521\n",
      "Epoch 29/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7215 - mse: 0.7215 - val_loss: 0.7470 - val_mse: 0.7470\n",
      "Epoch 30/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7198 - mse: 0.7198 - val_loss: 0.7444 - val_mse: 0.7444\n",
      "Epoch 31/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7189 - mse: 0.7189 - val_loss: 0.7437 - val_mse: 0.7437\n",
      "Epoch 32/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7174 - mse: 0.7174 - val_loss: 0.7479 - val_mse: 0.7479\n",
      "Epoch 33/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7157 - mse: 0.7157 - val_loss: 0.7483 - val_mse: 0.7483\n",
      "Epoch 34/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7149 - mse: 0.7149 - val_loss: 0.7421 - val_mse: 0.7421\n",
      "Epoch 35/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7128 - mse: 0.7128 - val_loss: 0.7421 - val_mse: 0.7421\n",
      "Epoch 36/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7120 - mse: 0.7120 - val_loss: 0.7459 - val_mse: 0.7459\n",
      "Epoch 37/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7107 - mse: 0.7107 - val_loss: 0.7503 - val_mse: 0.7503\n",
      "Epoch 38/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7101 - mse: 0.7101 - val_loss: 0.7469 - val_mse: 0.7469\n",
      "Epoch 39/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7082 - mse: 0.7082 - val_loss: 0.7531 - val_mse: 0.7531\n",
      "Epoch 40/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7073 - mse: 0.7073 - val_loss: 0.7487 - val_mse: 0.7487\n",
      "Epoch 41/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7062 - mse: 0.7062 - val_loss: 0.7460 - val_mse: 0.7460\n",
      "Epoch 42/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7050 - mse: 0.7050 - val_loss: 0.7555 - val_mse: 0.7555\n",
      "Epoch 43/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7038 - mse: 0.7038 - val_loss: 0.7420 - val_mse: 0.7420\n",
      "Epoch 44/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7028 - mse: 0.7028 - val_loss: 0.7490 - val_mse: 0.7490\n",
      "Epoch 45/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7024 - mse: 0.7024 - val_loss: 0.7533 - val_mse: 0.7533\n",
      "Epoch 46/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7009 - mse: 0.7009 - val_loss: 0.7581 - val_mse: 0.7581\n",
      "Epoch 47/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7001 - mse: 0.7001 - val_loss: 0.7503 - val_mse: 0.7503\n",
      "Epoch 48/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6988 - mse: 0.6988 - val_loss: 0.7581 - val_mse: 0.7581\n",
      "Epoch 49/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6978 - mse: 0.6978 - val_loss: 0.7501 - val_mse: 0.7501\n",
      "Epoch 50/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6975 - mse: 0.6975 - val_loss: 0.7534 - val_mse: 0.7534\n",
      "Epoch 51/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6961 - mse: 0.6961 - val_loss: 0.7505 - val_mse: 0.7505\n",
      "Epoch 52/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6952 - mse: 0.6952 - val_loss: 0.7547 - val_mse: 0.7547\n",
      "Epoch 53/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6947 - mse: 0.6947 - val_loss: 0.7567 - val_mse: 0.7567\n",
      "Epoch 54/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6938 - mse: 0.6938 - val_loss: 0.7575 - val_mse: 0.7575\n",
      "Epoch 55/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6926 - mse: 0.6926 - val_loss: 0.7580 - val_mse: 0.7580\n",
      "Epoch 56/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6924 - mse: 0.6924 - val_loss: 0.7578 - val_mse: 0.7578\n",
      "Epoch 57/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6916 - mse: 0.6916 - val_loss: 0.7623 - val_mse: 0.7623\n",
      "Epoch 58/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6896 - mse: 0.6896 - val_loss: 0.7649 - val_mse: 0.7649\n",
      "Epoch 59/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6890 - mse: 0.6890 - val_loss: 0.7530 - val_mse: 0.7530\n",
      "Epoch 60/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6887 - mse: 0.6887 - val_loss: 0.7683 - val_mse: 0.7683\n",
      "Epoch 61/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6875 - mse: 0.6875 - val_loss: 0.7612 - val_mse: 0.7612\n",
      "Epoch 62/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6868 - mse: 0.6868 - val_loss: 0.7597 - val_mse: 0.7597\n",
      "Epoch 63/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6859 - mse: 0.6859 - val_loss: 0.7567 - val_mse: 0.7567\n",
      "Epoch 64/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6850 - mse: 0.6850 - val_loss: 0.7684 - val_mse: 0.7684\n",
      "Epoch 65/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6845 - mse: 0.6845 - val_loss: 0.7595 - val_mse: 0.7595\n",
      "Epoch 66/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6835 - mse: 0.6835 - val_loss: 0.7550 - val_mse: 0.7550\n",
      "Epoch 67/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6829 - mse: 0.6829 - val_loss: 0.7652 - val_mse: 0.7652\n",
      "Epoch 68/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6817 - mse: 0.6817 - val_loss: 0.7689 - val_mse: 0.7689\n",
      "Epoch 69/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6816 - mse: 0.6816 - val_loss: 0.7672 - val_mse: 0.7672\n",
      "Epoch 70/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6805 - mse: 0.6805 - val_loss: 0.7602 - val_mse: 0.7602\n",
      "Epoch 71/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6793 - mse: 0.6793 - val_loss: 0.7618 - val_mse: 0.7618\n",
      "Epoch 72/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6792 - mse: 0.6792 - val_loss: 0.7727 - val_mse: 0.7727\n",
      "Epoch 73/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6781 - mse: 0.6781 - val_loss: 0.7618 - val_mse: 0.7618\n",
      "Epoch 74/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6772 - mse: 0.6772 - val_loss: 0.7694 - val_mse: 0.7694\n",
      "Epoch 75/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6769 - mse: 0.6769 - val_loss: 0.7678 - val_mse: 0.7678\n",
      "Epoch 76/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6760 - mse: 0.6760 - val_loss: 0.7667 - val_mse: 0.7667\n",
      "Epoch 77/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6749 - mse: 0.6749 - val_loss: 0.7748 - val_mse: 0.7748\n",
      "Epoch 78/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6742 - mse: 0.6742 - val_loss: 0.7798 - val_mse: 0.7798\n",
      "Epoch 79/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6736 - mse: 0.6736 - val_loss: 0.7811 - val_mse: 0.7811\n",
      "Epoch 80/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6733 - mse: 0.6733 - val_loss: 0.7677 - val_mse: 0.7677\n",
      "Epoch 81/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6719 - mse: 0.6719 - val_loss: 0.7719 - val_mse: 0.7719\n",
      "Epoch 82/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6719 - mse: 0.6719 - val_loss: 0.7730 - val_mse: 0.7730\n",
      "Epoch 83/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6707 - mse: 0.6707 - val_loss: 0.7787 - val_mse: 0.7787\n",
      "Epoch 84/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6703 - mse: 0.6703 - val_loss: 0.7689 - val_mse: 0.7689\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6694 - mse: 0.6694 - val_loss: 0.7770 - val_mse: 0.7770\n",
      "Epoch 86/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6688 - mse: 0.6688 - val_loss: 0.7649 - val_mse: 0.7649\n",
      "Epoch 87/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6679 - mse: 0.6679 - val_loss: 0.7708 - val_mse: 0.7708\n",
      "Epoch 88/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6676 - mse: 0.6676 - val_loss: 0.7692 - val_mse: 0.7692\n",
      "Epoch 89/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6665 - mse: 0.6665 - val_loss: 0.7688 - val_mse: 0.7688\n",
      "Epoch 90/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6657 - mse: 0.6657 - val_loss: 0.7700 - val_mse: 0.7700\n",
      "Epoch 91/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6651 - mse: 0.6651 - val_loss: 0.7858 - val_mse: 0.7858\n",
      "Epoch 92/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6650 - mse: 0.6650 - val_loss: 0.7775 - val_mse: 0.7775\n",
      "Epoch 93/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6645 - mse: 0.6645 - val_loss: 0.7717 - val_mse: 0.7717\n",
      "Epoch 94/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6636 - mse: 0.6636 - val_loss: 0.7893 - val_mse: 0.7893\n",
      "Epoch 95/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6630 - mse: 0.6630 - val_loss: 0.7736 - val_mse: 0.7736\n",
      "Epoch 96/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6621 - mse: 0.6621 - val_loss: 0.7791 - val_mse: 0.7791\n",
      "Epoch 97/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6613 - mse: 0.6613 - val_loss: 0.7924 - val_mse: 0.7924\n",
      "Epoch 98/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6611 - mse: 0.6611 - val_loss: 0.7887 - val_mse: 0.7887\n",
      "Epoch 99/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6603 - mse: 0.6603 - val_loss: 0.7850 - val_mse: 0.7850\n",
      "Epoch 100/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6595 - mse: 0.6595 - val_loss: 0.7779 - val_mse: 0.7779\n",
      "1676658/1676658 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.9059 - mse: 0.9059 - val_loss: 0.7783 - val_mse: 0.7783\n",
      "Epoch 2/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8255 - mse: 0.8255 - val_loss: 0.7617 - val_mse: 0.7617\n",
      "Epoch 3/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8060 - mse: 0.8060 - val_loss: 0.7742 - val_mse: 0.7742\n",
      "Epoch 4/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7914 - mse: 0.7914 - val_loss: 0.7606 - val_mse: 0.7606\n",
      "Epoch 5/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7797 - mse: 0.7797 - val_loss: 0.7567 - val_mse: 0.7567\n",
      "Epoch 6/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7687 - mse: 0.7687 - val_loss: 0.7498 - val_mse: 0.7498\n",
      "Epoch 7/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7598 - mse: 0.7598 - val_loss: 0.7566 - val_mse: 0.7566\n",
      "Epoch 8/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7511 - mse: 0.7511 - val_loss: 0.7514 - val_mse: 0.7514\n",
      "Epoch 9/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7449 - mse: 0.7449 - val_loss: 0.7474 - val_mse: 0.7474\n",
      "Epoch 10/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7392 - mse: 0.7392 - val_loss: 0.7331 - val_mse: 0.7331\n",
      "Epoch 11/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7340 - mse: 0.7340 - val_loss: 0.7309 - val_mse: 0.7309\n",
      "Epoch 12/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7298 - mse: 0.7298 - val_loss: 0.7281 - val_mse: 0.7281\n",
      "Epoch 13/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7262 - mse: 0.7262 - val_loss: 0.7259 - val_mse: 0.7259\n",
      "Epoch 14/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7229 - mse: 0.7229 - val_loss: 0.7608 - val_mse: 0.7608\n",
      "Epoch 15/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7197 - mse: 0.7197 - val_loss: 0.7172 - val_mse: 0.7172\n",
      "Epoch 16/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7175 - mse: 0.7175 - val_loss: 0.7200 - val_mse: 0.7200\n",
      "Epoch 17/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7146 - mse: 0.7146 - val_loss: 0.7143 - val_mse: 0.7143\n",
      "Epoch 18/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7119 - mse: 0.7119 - val_loss: 0.7094 - val_mse: 0.7094\n",
      "Epoch 19/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7101 - mse: 0.7101 - val_loss: 0.7111 - val_mse: 0.7111\n",
      "Epoch 20/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7079 - mse: 0.7079 - val_loss: 0.7046 - val_mse: 0.7046\n",
      "Epoch 21/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7052 - mse: 0.7052 - val_loss: 0.7149 - val_mse: 0.7149\n",
      "Epoch 22/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7039 - mse: 0.7039 - val_loss: 0.7058 - val_mse: 0.7058\n",
      "Epoch 23/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7012 - mse: 0.7012 - val_loss: 0.7033 - val_mse: 0.7033\n",
      "Epoch 24/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7003 - mse: 0.7003 - val_loss: 0.7105 - val_mse: 0.7105\n",
      "Epoch 25/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6981 - mse: 0.6981 - val_loss: 0.6989 - val_mse: 0.6989\n",
      "Epoch 26/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6964 - mse: 0.6964 - val_loss: 0.7011 - val_mse: 0.7011\n",
      "Epoch 27/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6951 - mse: 0.6951 - val_loss: 0.6976 - val_mse: 0.6976\n",
      "Epoch 28/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6925 - mse: 0.6925 - val_loss: 0.6996 - val_mse: 0.6996\n",
      "Epoch 29/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6920 - mse: 0.6920 - val_loss: 0.7123 - val_mse: 0.7123\n",
      "Epoch 30/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6899 - mse: 0.6899 - val_loss: 0.6954 - val_mse: 0.6954\n",
      "Epoch 31/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6884 - mse: 0.6884 - val_loss: 0.6953 - val_mse: 0.6953\n",
      "Epoch 32/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6867 - mse: 0.6867 - val_loss: 0.7100 - val_mse: 0.7100\n",
      "Epoch 33/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6860 - mse: 0.6860 - val_loss: 0.7132 - val_mse: 0.7132\n",
      "Epoch 34/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6840 - mse: 0.6840 - val_loss: 0.6942 - val_mse: 0.6942\n",
      "Epoch 35/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6825 - mse: 0.6825 - val_loss: 0.7046 - val_mse: 0.7046\n",
      "Epoch 36/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6814 - mse: 0.6814 - val_loss: 0.6921 - val_mse: 0.6921\n",
      "Epoch 37/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6800 - mse: 0.6800 - val_loss: 0.7030 - val_mse: 0.7030\n",
      "Epoch 38/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6784 - mse: 0.6784 - val_loss: 0.6886 - val_mse: 0.6886\n",
      "Epoch 39/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6773 - mse: 0.6773 - val_loss: 0.7057 - val_mse: 0.7057\n",
      "Epoch 40/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6763 - mse: 0.6763 - val_loss: 0.6866 - val_mse: 0.6866\n",
      "Epoch 41/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6756 - mse: 0.6756 - val_loss: 0.7004 - val_mse: 0.7004\n",
      "Epoch 42/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6735 - mse: 0.6735 - val_loss: 0.6921 - val_mse: 0.6921\n",
      "Epoch 43/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6723 - mse: 0.6723 - val_loss: 0.6903 - val_mse: 0.6903\n",
      "Epoch 44/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6718 - mse: 0.6718 - val_loss: 0.6862 - val_mse: 0.6862\n",
      "Epoch 45/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6704 - mse: 0.6704 - val_loss: 0.6855 - val_mse: 0.6855\n",
      "Epoch 46/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6692 - mse: 0.6692 - val_loss: 0.6938 - val_mse: 0.6938\n",
      "Epoch 47/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6683 - mse: 0.6683 - val_loss: 0.6820 - val_mse: 0.6820\n",
      "Epoch 48/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6673 - mse: 0.6673 - val_loss: 0.6838 - val_mse: 0.6838\n",
      "Epoch 49/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6657 - mse: 0.6657 - val_loss: 0.6825 - val_mse: 0.6825\n",
      "Epoch 50/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6649 - mse: 0.6649 - val_loss: 0.6848 - val_mse: 0.6848\n",
      "Epoch 51/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6639 - mse: 0.6639 - val_loss: 0.6852 - val_mse: 0.6852\n",
      "Epoch 52/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6631 - mse: 0.6631 - val_loss: 0.6925 - val_mse: 0.6925\n",
      "Epoch 53/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6621 - mse: 0.6621 - val_loss: 0.6810 - val_mse: 0.6810\n",
      "Epoch 54/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6606 - mse: 0.6606 - val_loss: 0.6782 - val_mse: 0.6782\n",
      "Epoch 55/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6598 - mse: 0.6598 - val_loss: 0.6937 - val_mse: 0.6937\n",
      "Epoch 56/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6592 - mse: 0.6592 - val_loss: 0.7346 - val_mse: 0.7346\n",
      "Epoch 57/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6582 - mse: 0.6582 - val_loss: 0.6845 - val_mse: 0.6845\n",
      "Epoch 58/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6573 - mse: 0.6573 - val_loss: 0.6802 - val_mse: 0.6802\n",
      "Epoch 59/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6558 - mse: 0.6558 - val_loss: 0.6873 - val_mse: 0.6873\n",
      "Epoch 60/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6553 - mse: 0.6553 - val_loss: 0.6838 - val_mse: 0.6838\n",
      "Epoch 61/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6547 - mse: 0.6547 - val_loss: 0.6816 - val_mse: 0.6816\n",
      "Epoch 62/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6536 - mse: 0.6536 - val_loss: 0.6853 - val_mse: 0.6853\n",
      "Epoch 63/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6530 - mse: 0.6530 - val_loss: 0.6847 - val_mse: 0.6847\n",
      "Epoch 64/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6519 - mse: 0.6519 - val_loss: 0.6759 - val_mse: 0.6759\n",
      "Epoch 65/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6517 - mse: 0.6517 - val_loss: 0.6760 - val_mse: 0.6760\n",
      "Epoch 66/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6502 - mse: 0.6502 - val_loss: 0.6835 - val_mse: 0.6835\n",
      "Epoch 67/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6496 - mse: 0.6496 - val_loss: 0.6745 - val_mse: 0.6745\n",
      "Epoch 68/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6483 - mse: 0.6483 - val_loss: 0.6747 - val_mse: 0.6747\n",
      "Epoch 69/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6475 - mse: 0.6475 - val_loss: 0.6753 - val_mse: 0.6753\n",
      "Epoch 70/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6471 - mse: 0.6471 - val_loss: 0.7129 - val_mse: 0.7129\n",
      "Epoch 71/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6464 - mse: 0.6464 - val_loss: 0.6773 - val_mse: 0.6773\n",
      "Epoch 72/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6454 - mse: 0.6454 - val_loss: 0.6715 - val_mse: 0.6715\n",
      "Epoch 73/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6442 - mse: 0.6442 - val_loss: 0.6791 - val_mse: 0.6791\n",
      "Epoch 74/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6447 - mse: 0.6447 - val_loss: 0.6763 - val_mse: 0.6763\n",
      "Epoch 75/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6426 - mse: 0.6426 - val_loss: 0.6703 - val_mse: 0.6703\n",
      "Epoch 76/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6426 - mse: 0.6426 - val_loss: 0.6899 - val_mse: 0.6899\n",
      "Epoch 77/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6417 - mse: 0.6417 - val_loss: 0.6782 - val_mse: 0.6782\n",
      "Epoch 78/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6411 - mse: 0.6411 - val_loss: 0.6816 - val_mse: 0.6816\n",
      "Epoch 79/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6409 - mse: 0.6409 - val_loss: 0.6744 - val_mse: 0.6744\n",
      "Epoch 80/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6396 - mse: 0.6396 - val_loss: 0.6815 - val_mse: 0.6815\n",
      "Epoch 81/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6389 - mse: 0.6389 - val_loss: 0.6822 - val_mse: 0.6822\n",
      "Epoch 82/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6382 - mse: 0.6382 - val_loss: 0.6745 - val_mse: 0.6745\n",
      "Epoch 83/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6380 - mse: 0.6380 - val_loss: 0.6898 - val_mse: 0.6898\n",
      "Epoch 84/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6377 - mse: 0.6377 - val_loss: 0.6801 - val_mse: 0.6801\n",
      "Epoch 85/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6364 - mse: 0.6364 - val_loss: 0.6722 - val_mse: 0.6722\n",
      "Epoch 86/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6357 - mse: 0.6357 - val_loss: 0.6743 - val_mse: 0.6743\n",
      "Epoch 87/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6355 - mse: 0.6355 - val_loss: 0.6656 - val_mse: 0.6656\n",
      "Epoch 88/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6346 - mse: 0.6346 - val_loss: 0.6725 - val_mse: 0.6725\n",
      "Epoch 89/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6340 - mse: 0.6340 - val_loss: 0.6779 - val_mse: 0.6779\n",
      "Epoch 90/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6330 - mse: 0.6330 - val_loss: 0.6736 - val_mse: 0.6736\n",
      "Epoch 91/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6325 - mse: 0.6325 - val_loss: 0.6686 - val_mse: 0.6686\n",
      "Epoch 92/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6320 - mse: 0.6320 - val_loss: 0.6730 - val_mse: 0.6730\n",
      "Epoch 93/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6314 - mse: 0.6314 - val_loss: 0.6725 - val_mse: 0.6725\n",
      "Epoch 94/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6306 - mse: 0.6306 - val_loss: 0.6720 - val_mse: 0.6720\n",
      "Epoch 95/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6302 - mse: 0.6302 - val_loss: 0.6748 - val_mse: 0.6748\n",
      "Epoch 96/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6299 - mse: 0.6299 - val_loss: 0.6758 - val_mse: 0.6758\n",
      "Epoch 97/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6293 - mse: 0.6293 - val_loss: 0.6708 - val_mse: 0.6708\n",
      "Epoch 98/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6286 - mse: 0.6286 - val_loss: 0.6713 - val_mse: 0.6713\n",
      "Epoch 99/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6278 - mse: 0.6278 - val_loss: 0.6690 - val_mse: 0.6690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6272 - mse: 0.6272 - val_loss: 0.6821 - val_mse: 0.6821\n",
      "Epoch 101/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6269 - mse: 0.6269 - val_loss: 0.6574 - val_mse: 0.6574\n",
      "Epoch 102/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6262 - mse: 0.6262 - val_loss: 0.6780 - val_mse: 0.6780\n",
      "Epoch 103/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6256 - mse: 0.6256 - val_loss: 0.6736 - val_mse: 0.6736\n",
      "Epoch 104/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6257 - mse: 0.6257 - val_loss: 0.6826 - val_mse: 0.6826\n",
      "Epoch 105/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6245 - mse: 0.6245 - val_loss: 0.6772 - val_mse: 0.6772\n",
      "Epoch 106/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6240 - mse: 0.6240 - val_loss: 0.6706 - val_mse: 0.6706\n",
      "Epoch 107/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6237 - mse: 0.6237 - val_loss: 0.6738 - val_mse: 0.6738\n",
      "Epoch 108/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6231 - mse: 0.6231 - val_loss: 0.6557 - val_mse: 0.6557\n",
      "Epoch 109/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6226 - mse: 0.6226 - val_loss: 0.6700 - val_mse: 0.6700\n",
      "Epoch 110/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6224 - mse: 0.6224 - val_loss: 0.6592 - val_mse: 0.6592\n",
      "Epoch 111/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6216 - mse: 0.6216 - val_loss: 0.6576 - val_mse: 0.6576\n",
      "Epoch 112/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6213 - mse: 0.6213 - val_loss: 0.6655 - val_mse: 0.6655\n",
      "Epoch 113/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6208 - mse: 0.6208 - val_loss: 0.6716 - val_mse: 0.6716\n",
      "Epoch 114/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6205 - mse: 0.6205 - val_loss: 0.6615 - val_mse: 0.6615\n",
      "Epoch 115/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6203 - mse: 0.6203 - val_loss: 0.6662 - val_mse: 0.6662\n",
      "Epoch 116/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6196 - mse: 0.6196 - val_loss: 0.6687 - val_mse: 0.6687\n",
      "Epoch 117/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6186 - mse: 0.6186 - val_loss: 0.6705 - val_mse: 0.6705\n",
      "Epoch 118/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6183 - mse: 0.6183 - val_loss: 0.6640 - val_mse: 0.6640\n",
      "Epoch 119/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6181 - mse: 0.6181 - val_loss: 0.6652 - val_mse: 0.6652\n",
      "Epoch 120/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6171 - mse: 0.6171 - val_loss: 0.6571 - val_mse: 0.6571\n",
      "Epoch 121/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6169 - mse: 0.6169 - val_loss: 0.6630 - val_mse: 0.6630\n",
      "Epoch 122/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6166 - mse: 0.6166 - val_loss: 0.6705 - val_mse: 0.6705\n",
      "Epoch 123/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6158 - mse: 0.6158 - val_loss: 0.6685 - val_mse: 0.6685\n",
      "Epoch 124/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6159 - mse: 0.6159 - val_loss: 0.6534 - val_mse: 0.6534\n",
      "Epoch 125/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6152 - mse: 0.6152 - val_loss: 0.6651 - val_mse: 0.6651\n",
      "Epoch 126/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6150 - mse: 0.6150 - val_loss: 0.6679 - val_mse: 0.6679\n",
      "Epoch 127/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6146 - mse: 0.6146 - val_loss: 0.6541 - val_mse: 0.6541\n",
      "Epoch 128/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6143 - mse: 0.6143 - val_loss: 0.6817 - val_mse: 0.6817\n",
      "Epoch 129/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6142 - mse: 0.6142 - val_loss: 0.6943 - val_mse: 0.6943\n",
      "Epoch 130/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6127 - mse: 0.6127 - val_loss: 0.6710 - val_mse: 0.6710\n",
      "Epoch 131/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6129 - mse: 0.6129 - val_loss: 0.6529 - val_mse: 0.6529\n",
      "Epoch 132/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6123 - mse: 0.6123 - val_loss: 0.6773 - val_mse: 0.6773\n",
      "Epoch 133/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6118 - mse: 0.6118 - val_loss: 0.6666 - val_mse: 0.6666\n",
      "Epoch 134/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6113 - mse: 0.6113 - val_loss: 0.6580 - val_mse: 0.6580\n",
      "Epoch 135/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6114 - mse: 0.6114 - val_loss: 0.6689 - val_mse: 0.6689\n",
      "Epoch 136/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6104 - mse: 0.6104 - val_loss: 0.6585 - val_mse: 0.6585\n",
      "Epoch 137/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6101 - mse: 0.6101 - val_loss: 0.6651 - val_mse: 0.6651\n",
      "Epoch 138/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6103 - mse: 0.6103 - val_loss: 0.6589 - val_mse: 0.6589\n",
      "Epoch 139/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6094 - mse: 0.6094 - val_loss: 0.6621 - val_mse: 0.6621\n",
      "Epoch 140/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6091 - mse: 0.6091 - val_loss: 0.6663 - val_mse: 0.6663\n",
      "Epoch 141/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6088 - mse: 0.6088 - val_loss: 0.6774 - val_mse: 0.6774\n",
      "Epoch 142/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6080 - mse: 0.6080 - val_loss: 0.6566 - val_mse: 0.6566\n",
      "Epoch 143/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6080 - mse: 0.6080 - val_loss: 0.6581 - val_mse: 0.6581\n",
      "Epoch 144/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6074 - mse: 0.6074 - val_loss: 0.6557 - val_mse: 0.6557\n",
      "Epoch 145/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6076 - mse: 0.6076 - val_loss: 0.6547 - val_mse: 0.6547\n",
      "Epoch 146/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6063 - mse: 0.6063 - val_loss: 0.6624 - val_mse: 0.6624\n",
      "Epoch 147/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6064 - mse: 0.6064 - val_loss: 0.6558 - val_mse: 0.6558\n",
      "Epoch 148/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6062 - mse: 0.6062 - val_loss: 0.6965 - val_mse: 0.6965\n",
      "Epoch 149/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6059 - mse: 0.6059 - val_loss: 0.6642 - val_mse: 0.6642\n",
      "Epoch 150/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6054 - mse: 0.6054 - val_loss: 0.6581 - val_mse: 0.6581\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7929 - mse: 0.7929 - val_loss: 0.7731 - val_mse: 0.7731\n",
      "Epoch 2/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7282 - mse: 0.7282 - val_loss: 0.7677 - val_mse: 0.7677\n",
      "Epoch 3/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7153 - mse: 0.7153 - val_loss: 0.7575 - val_mse: 0.7575\n",
      "Epoch 4/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7056 - mse: 0.7056 - val_loss: 0.7542 - val_mse: 0.7542\n",
      "Epoch 5/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6972 - mse: 0.6972 - val_loss: 0.7513 - val_mse: 0.7513\n",
      "Epoch 6/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6896 - mse: 0.6896 - val_loss: 0.7469 - val_mse: 0.7469\n",
      "Epoch 7/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6833 - mse: 0.6833 - val_loss: 0.7470 - val_mse: 0.7470\n",
      "Epoch 8/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6779 - mse: 0.6779 - val_loss: 0.7418 - val_mse: 0.7418\n",
      "Epoch 9/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6729 - mse: 0.6729 - val_loss: 0.7444 - val_mse: 0.7444\n",
      "Epoch 10/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6689 - mse: 0.6689 - val_loss: 0.7386 - val_mse: 0.7386\n",
      "Epoch 11/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6651 - mse: 0.6651 - val_loss: 0.7364 - val_mse: 0.7364\n",
      "Epoch 12/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6619 - mse: 0.6619 - val_loss: 0.7334 - val_mse: 0.7334\n",
      "Epoch 13/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6588 - mse: 0.6588 - val_loss: 0.7322 - val_mse: 0.7322\n",
      "Epoch 14/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6565 - mse: 0.6565 - val_loss: 0.7271 - val_mse: 0.7271\n",
      "Epoch 15/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6544 - mse: 0.6544 - val_loss: 0.7284 - val_mse: 0.7284\n",
      "Epoch 16/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6514 - mse: 0.6514 - val_loss: 0.7248 - val_mse: 0.7248\n",
      "Epoch 17/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6501 - mse: 0.6501 - val_loss: 0.7222 - val_mse: 0.7222\n",
      "Epoch 18/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6479 - mse: 0.6479 - val_loss: 0.7191 - val_mse: 0.7191\n",
      "Epoch 19/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6466 - mse: 0.6466 - val_loss: 0.7231 - val_mse: 0.7231\n",
      "Epoch 20/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6442 - mse: 0.6442 - val_loss: 0.7182 - val_mse: 0.7182\n",
      "Epoch 21/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6432 - mse: 0.6432 - val_loss: 0.7117 - val_mse: 0.7117\n",
      "Epoch 22/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6414 - mse: 0.6414 - val_loss: 0.7124 - val_mse: 0.7124\n",
      "Epoch 23/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6397 - mse: 0.6397 - val_loss: 0.7082 - val_mse: 0.7082\n",
      "Epoch 24/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6381 - mse: 0.6381 - val_loss: 0.7071 - val_mse: 0.7071\n",
      "Epoch 25/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6369 - mse: 0.6369 - val_loss: 0.7079 - val_mse: 0.7079\n",
      "Epoch 26/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6353 - mse: 0.6353 - val_loss: 0.7113 - val_mse: 0.7113\n",
      "Epoch 27/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6342 - mse: 0.6342 - val_loss: 0.7066 - val_mse: 0.7066\n",
      "Epoch 28/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6330 - mse: 0.6330 - val_loss: 0.7036 - val_mse: 0.7036\n",
      "Epoch 29/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6318 - mse: 0.6318 - val_loss: 0.7081 - val_mse: 0.7081\n",
      "Epoch 30/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6304 - mse: 0.6304 - val_loss: 0.7090 - val_mse: 0.7090\n",
      "Epoch 31/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6290 - mse: 0.6290 - val_loss: 0.7034 - val_mse: 0.7034\n",
      "Epoch 32/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6277 - mse: 0.6277 - val_loss: 0.6983 - val_mse: 0.6983\n",
      "Epoch 33/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6263 - mse: 0.6263 - val_loss: 0.7120 - val_mse: 0.7120\n",
      "Epoch 34/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6255 - mse: 0.6255 - val_loss: 0.6974 - val_mse: 0.6974\n",
      "Epoch 35/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6236 - mse: 0.6236 - val_loss: 0.7036 - val_mse: 0.7036\n",
      "Epoch 36/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6235 - mse: 0.6235 - val_loss: 0.6994 - val_mse: 0.6994\n",
      "Epoch 37/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6222 - mse: 0.6222 - val_loss: 0.6988 - val_mse: 0.6988\n",
      "Epoch 38/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6206 - mse: 0.6206 - val_loss: 0.6957 - val_mse: 0.6957\n",
      "Epoch 39/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6200 - mse: 0.6200 - val_loss: 0.7059 - val_mse: 0.7059\n",
      "Epoch 40/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6189 - mse: 0.6189 - val_loss: 0.6950 - val_mse: 0.6950\n",
      "Epoch 41/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6180 - mse: 0.6180 - val_loss: 0.6979 - val_mse: 0.6979\n",
      "Epoch 42/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6165 - mse: 0.6165 - val_loss: 0.7134 - val_mse: 0.7134\n",
      "Epoch 43/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6161 - mse: 0.6161 - val_loss: 0.7046 - val_mse: 0.7046\n",
      "Epoch 44/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6152 - mse: 0.6152 - val_loss: 0.6922 - val_mse: 0.6922\n",
      "Epoch 45/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6143 - mse: 0.6143 - val_loss: 0.6946 - val_mse: 0.6946\n",
      "Epoch 46/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6131 - mse: 0.6131 - val_loss: 0.6963 - val_mse: 0.6963\n",
      "Epoch 47/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6123 - mse: 0.6123 - val_loss: 0.6943 - val_mse: 0.6943\n",
      "Epoch 48/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6115 - mse: 0.6115 - val_loss: 0.6935 - val_mse: 0.6935\n",
      "Epoch 49/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6103 - mse: 0.6103 - val_loss: 0.6874 - val_mse: 0.6874\n",
      "Epoch 50/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6100 - mse: 0.6100 - val_loss: 0.6973 - val_mse: 0.6973\n",
      "Epoch 51/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6086 - mse: 0.6086 - val_loss: 0.6951 - val_mse: 0.6951\n",
      "Epoch 52/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6082 - mse: 0.6082 - val_loss: 0.6891 - val_mse: 0.6891\n",
      "Epoch 53/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6077 - mse: 0.6077 - val_loss: 0.6895 - val_mse: 0.6895\n",
      "Epoch 54/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6068 - mse: 0.6068 - val_loss: 0.6891 - val_mse: 0.6891\n",
      "Epoch 55/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6058 - mse: 0.6058 - val_loss: 0.6910 - val_mse: 0.6910\n",
      "Epoch 56/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6049 - mse: 0.6049 - val_loss: 0.6898 - val_mse: 0.6898\n",
      "Epoch 57/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6045 - mse: 0.6045 - val_loss: 0.6951 - val_mse: 0.6951\n",
      "Epoch 58/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6036 - mse: 0.6036 - val_loss: 0.6869 - val_mse: 0.6869\n",
      "Epoch 59/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6031 - mse: 0.6031 - val_loss: 0.6799 - val_mse: 0.6799\n",
      "Epoch 60/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6020 - mse: 0.6020 - val_loss: 0.6846 - val_mse: 0.6846\n",
      "Epoch 61/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6022 - mse: 0.6022 - val_loss: 0.6853 - val_mse: 0.6853\n",
      "Epoch 62/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6007 - mse: 0.6007 - val_loss: 0.6851 - val_mse: 0.6851\n",
      "Epoch 63/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6007 - mse: 0.6007 - val_loss: 0.6897 - val_mse: 0.6897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5992 - mse: 0.5992 - val_loss: 0.6865 - val_mse: 0.6865\n",
      "Epoch 65/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5991 - mse: 0.5991 - val_loss: 0.6860 - val_mse: 0.6860\n",
      "Epoch 66/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5991 - mse: 0.5991 - val_loss: 0.6806 - val_mse: 0.6806\n",
      "Epoch 67/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5976 - mse: 0.5976 - val_loss: 0.6938 - val_mse: 0.6938\n",
      "Epoch 68/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5969 - mse: 0.5969 - val_loss: 0.6820 - val_mse: 0.6820\n",
      "Epoch 69/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5962 - mse: 0.5962 - val_loss: 0.6820 - val_mse: 0.6820\n",
      "Epoch 70/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5957 - mse: 0.5957 - val_loss: 0.6792 - val_mse: 0.6792\n",
      "Epoch 71/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5953 - mse: 0.5953 - val_loss: 0.6802 - val_mse: 0.6802\n",
      "Epoch 72/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5948 - mse: 0.5948 - val_loss: 0.6748 - val_mse: 0.6748\n",
      "Epoch 73/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5942 - mse: 0.5942 - val_loss: 0.6813 - val_mse: 0.6813\n",
      "Epoch 74/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5935 - mse: 0.5935 - val_loss: 0.7012 - val_mse: 0.7012\n",
      "Epoch 75/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5930 - mse: 0.5930 - val_loss: 0.6764 - val_mse: 0.6764\n",
      "Epoch 76/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5924 - mse: 0.5924 - val_loss: 0.6813 - val_mse: 0.6813\n",
      "Epoch 77/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5921 - mse: 0.5921 - val_loss: 0.6779 - val_mse: 0.6779\n",
      "Epoch 78/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5914 - mse: 0.5914 - val_loss: 0.6795 - val_mse: 0.6795\n",
      "Epoch 79/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5909 - mse: 0.5909 - val_loss: 0.6749 - val_mse: 0.6749\n",
      "Epoch 80/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5902 - mse: 0.5902 - val_loss: 0.6806 - val_mse: 0.6806\n",
      "Epoch 81/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5897 - mse: 0.5897 - val_loss: 0.6782 - val_mse: 0.6782\n",
      "Epoch 82/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5892 - mse: 0.5892 - val_loss: 0.6750 - val_mse: 0.6750\n",
      "Epoch 83/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5885 - mse: 0.5885 - val_loss: 0.6763 - val_mse: 0.6763\n",
      "Epoch 84/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5886 - mse: 0.5886 - val_loss: 0.6822 - val_mse: 0.6822\n",
      "Epoch 85/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5884 - mse: 0.5884 - val_loss: 0.6741 - val_mse: 0.6741\n",
      "Epoch 86/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5870 - mse: 0.5870 - val_loss: 0.6814 - val_mse: 0.6814\n",
      "Epoch 87/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5869 - mse: 0.5869 - val_loss: 0.6818 - val_mse: 0.6818\n",
      "Epoch 88/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5865 - mse: 0.5865 - val_loss: 0.6713 - val_mse: 0.6713\n",
      "Epoch 89/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5855 - mse: 0.5855 - val_loss: 0.6794 - val_mse: 0.6794\n",
      "Epoch 90/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5854 - mse: 0.5854 - val_loss: 0.6795 - val_mse: 0.6795\n",
      "Epoch 91/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5854 - mse: 0.5854 - val_loss: 0.6809 - val_mse: 0.6809\n",
      "Epoch 92/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5849 - mse: 0.5849 - val_loss: 0.6803 - val_mse: 0.6803\n",
      "Epoch 93/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5836 - mse: 0.5836 - val_loss: 0.6768 - val_mse: 0.6768\n",
      "Epoch 94/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5837 - mse: 0.5837 - val_loss: 0.6736 - val_mse: 0.6736\n",
      "Epoch 95/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5830 - mse: 0.5830 - val_loss: 0.6769 - val_mse: 0.6769\n",
      "Epoch 96/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5830 - mse: 0.5830 - val_loss: 0.6776 - val_mse: 0.6776\n",
      "Epoch 97/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5821 - mse: 0.5821 - val_loss: 0.6728 - val_mse: 0.6728\n",
      "Epoch 98/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5818 - mse: 0.5818 - val_loss: 0.6768 - val_mse: 0.6768\n",
      "Epoch 99/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5815 - mse: 0.5815 - val_loss: 0.6759 - val_mse: 0.6759\n",
      "Epoch 100/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5812 - mse: 0.5812 - val_loss: 0.6852 - val_mse: 0.6852\n",
      "Epoch 101/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5806 - mse: 0.5806 - val_loss: 0.6830 - val_mse: 0.6830\n",
      "Epoch 102/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5799 - mse: 0.5799 - val_loss: 0.6784 - val_mse: 0.6784\n",
      "Epoch 103/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5794 - mse: 0.5794 - val_loss: 0.6689 - val_mse: 0.6689\n",
      "Epoch 104/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5788 - mse: 0.5788 - val_loss: 0.6752 - val_mse: 0.6752\n",
      "Epoch 105/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5788 - mse: 0.5788 - val_loss: 0.6773 - val_mse: 0.6773\n",
      "Epoch 106/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5784 - mse: 0.5784 - val_loss: 0.6816 - val_mse: 0.6816\n",
      "Epoch 107/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5775 - mse: 0.5775 - val_loss: 0.6784 - val_mse: 0.6784\n",
      "Epoch 108/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5782 - mse: 0.5782 - val_loss: 0.6717 - val_mse: 0.6717\n",
      "Epoch 109/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5772 - mse: 0.5772 - val_loss: 0.6836 - val_mse: 0.6836\n",
      "Epoch 110/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5768 - mse: 0.5768 - val_loss: 0.6758 - val_mse: 0.6758\n",
      "Epoch 111/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5767 - mse: 0.5767 - val_loss: 0.6740 - val_mse: 0.6740\n",
      "Epoch 112/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5760 - mse: 0.5760 - val_loss: 0.6760 - val_mse: 0.6760\n",
      "Epoch 113/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5754 - mse: 0.5754 - val_loss: 0.6695 - val_mse: 0.6695\n",
      "Epoch 114/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5749 - mse: 0.5749 - val_loss: 0.6703 - val_mse: 0.6703\n",
      "Epoch 115/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5744 - mse: 0.5744 - val_loss: 0.6739 - val_mse: 0.6739\n",
      "Epoch 116/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5741 - mse: 0.5741 - val_loss: 0.6715 - val_mse: 0.6715\n",
      "Epoch 117/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5736 - mse: 0.5736 - val_loss: 0.6724 - val_mse: 0.6724\n",
      "Epoch 118/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5733 - mse: 0.5733 - val_loss: 0.6680 - val_mse: 0.6680\n",
      "Epoch 119/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5730 - mse: 0.5730 - val_loss: 0.6709 - val_mse: 0.6709\n",
      "Epoch 120/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5724 - mse: 0.5724 - val_loss: 0.6650 - val_mse: 0.6650\n",
      "Epoch 121/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5722 - mse: 0.5722 - val_loss: 0.6766 - val_mse: 0.6766\n",
      "Epoch 122/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5723 - mse: 0.5723 - val_loss: 0.6774 - val_mse: 0.6774\n",
      "Epoch 123/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5713 - mse: 0.5713 - val_loss: 0.6726 - val_mse: 0.6726\n",
      "Epoch 124/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5709 - mse: 0.5709 - val_loss: 0.6707 - val_mse: 0.6707\n",
      "Epoch 125/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5705 - mse: 0.5705 - val_loss: 0.6717 - val_mse: 0.6717\n",
      "Epoch 126/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5704 - mse: 0.5704 - val_loss: 0.6706 - val_mse: 0.6706\n",
      "Epoch 127/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5696 - mse: 0.5696 - val_loss: 0.6692 - val_mse: 0.6692\n",
      "Epoch 128/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5696 - mse: 0.5696 - val_loss: 0.6732 - val_mse: 0.6732\n",
      "Epoch 129/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5692 - mse: 0.5692 - val_loss: 0.6686 - val_mse: 0.6686\n",
      "Epoch 130/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5684 - mse: 0.5684 - val_loss: 0.6744 - val_mse: 0.6744\n",
      "Epoch 131/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5680 - mse: 0.5680 - val_loss: 0.6663 - val_mse: 0.6663\n",
      "Epoch 132/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5678 - mse: 0.5678 - val_loss: 0.6781 - val_mse: 0.6781\n",
      "Epoch 133/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5677 - mse: 0.5677 - val_loss: 0.6754 - val_mse: 0.6754\n",
      "Epoch 134/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5669 - mse: 0.5669 - val_loss: 0.6655 - val_mse: 0.6655\n",
      "Epoch 135/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5668 - mse: 0.5668 - val_loss: 0.6686 - val_mse: 0.6686\n",
      "Epoch 136/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5661 - mse: 0.5661 - val_loss: 0.6732 - val_mse: 0.6732\n",
      "Epoch 137/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5661 - mse: 0.5661 - val_loss: 0.6730 - val_mse: 0.6730\n",
      "Epoch 138/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5653 - mse: 0.5653 - val_loss: 0.6744 - val_mse: 0.6744\n",
      "Epoch 139/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5650 - mse: 0.5650 - val_loss: 0.6663 - val_mse: 0.6663\n",
      "Epoch 140/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5645 - mse: 0.5645 - val_loss: 0.6686 - val_mse: 0.6686\n",
      "Epoch 141/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5648 - mse: 0.5648 - val_loss: 0.6765 - val_mse: 0.6765\n",
      "Epoch 142/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5641 - mse: 0.5641 - val_loss: 0.6873 - val_mse: 0.6873\n",
      "Epoch 143/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5636 - mse: 0.5636 - val_loss: 0.6677 - val_mse: 0.6677\n",
      "Epoch 144/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5633 - mse: 0.5633 - val_loss: 0.6720 - val_mse: 0.6720\n",
      "Epoch 145/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5628 - mse: 0.5628 - val_loss: 0.6686 - val_mse: 0.6686\n",
      "Epoch 146/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5625 - mse: 0.5625 - val_loss: 0.6665 - val_mse: 0.6665\n",
      "Epoch 147/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5624 - mse: 0.5624 - val_loss: 0.6630 - val_mse: 0.6630\n",
      "Epoch 148/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5624 - mse: 0.5624 - val_loss: 0.6608 - val_mse: 0.6608\n",
      "Epoch 149/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5617 - mse: 0.5617 - val_loss: 0.6700 - val_mse: 0.6700\n",
      "Epoch 150/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5612 - mse: 0.5612 - val_loss: 0.6687 - val_mse: 0.6687\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.9310 - mse: 0.9310 - val_loss: 0.8140 - val_mse: 0.8140\n",
      "Epoch 2/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8372 - mse: 0.8372 - val_loss: 0.8349 - val_mse: 0.8349\n",
      "Epoch 3/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8181 - mse: 0.8181 - val_loss: 0.8316 - val_mse: 0.8316\n",
      "Epoch 4/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8045 - mse: 0.8045 - val_loss: 0.8743 - val_mse: 0.8743\n",
      "Epoch 5/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7936 - mse: 0.7936 - val_loss: 0.8116 - val_mse: 0.8116\n",
      "Epoch 6/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7843 - mse: 0.7843 - val_loss: 0.8001 - val_mse: 0.8001\n",
      "Epoch 7/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7773 - mse: 0.7773 - val_loss: 0.7916 - val_mse: 0.7916\n",
      "Epoch 8/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7712 - mse: 0.7712 - val_loss: 0.7920 - val_mse: 0.7920\n",
      "Epoch 9/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7660 - mse: 0.7660 - val_loss: 0.7826 - val_mse: 0.7826\n",
      "Epoch 10/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7618 - mse: 0.7618 - val_loss: 0.7844 - val_mse: 0.7844\n",
      "Epoch 11/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7576 - mse: 0.7576 - val_loss: 0.7828 - val_mse: 0.7828\n",
      "Epoch 12/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7541 - mse: 0.7541 - val_loss: 0.7627 - val_mse: 0.7627\n",
      "Epoch 13/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7513 - mse: 0.7513 - val_loss: 0.7739 - val_mse: 0.7739\n",
      "Epoch 14/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7483 - mse: 0.7483 - val_loss: 0.8007 - val_mse: 0.8007\n",
      "Epoch 15/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7458 - mse: 0.7458 - val_loss: 0.7585 - val_mse: 0.7585\n",
      "Epoch 16/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7425 - mse: 0.7425 - val_loss: 0.7661 - val_mse: 0.7661\n",
      "Epoch 17/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7405 - mse: 0.7405 - val_loss: 0.7483 - val_mse: 0.7483\n",
      "Epoch 18/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7390 - mse: 0.7390 - val_loss: 0.7601 - val_mse: 0.7601\n",
      "Epoch 19/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7363 - mse: 0.7363 - val_loss: 0.7473 - val_mse: 0.7473\n",
      "Epoch 20/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7348 - mse: 0.7348 - val_loss: 0.7670 - val_mse: 0.7670\n",
      "Epoch 21/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7324 - mse: 0.7324 - val_loss: 0.7649 - val_mse: 0.7649\n",
      "Epoch 22/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7311 - mse: 0.7311 - val_loss: 0.7474 - val_mse: 0.7474\n",
      "Epoch 23/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7292 - mse: 0.7292 - val_loss: 0.7432 - val_mse: 0.7432\n",
      "Epoch 24/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7270 - mse: 0.7270 - val_loss: 0.7451 - val_mse: 0.7451\n",
      "Epoch 25/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7256 - mse: 0.7256 - val_loss: 0.7476 - val_mse: 0.7476\n",
      "Epoch 26/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7238 - mse: 0.7238 - val_loss: 0.7463 - val_mse: 0.7463\n",
      "Epoch 27/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7224 - mse: 0.7224 - val_loss: 0.7380 - val_mse: 0.7380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7206 - mse: 0.7206 - val_loss: 0.7351 - val_mse: 0.7351\n",
      "Epoch 29/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7192 - mse: 0.7192 - val_loss: 0.7412 - val_mse: 0.7412\n",
      "Epoch 30/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7176 - mse: 0.7176 - val_loss: 0.7417 - val_mse: 0.7417\n",
      "Epoch 31/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7167 - mse: 0.7167 - val_loss: 0.7331 - val_mse: 0.7331\n",
      "Epoch 32/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7152 - mse: 0.7152 - val_loss: 0.7337 - val_mse: 0.7337\n",
      "Epoch 33/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7138 - mse: 0.7138 - val_loss: 0.7368 - val_mse: 0.7368\n",
      "Epoch 34/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7125 - mse: 0.7125 - val_loss: 0.7419 - val_mse: 0.7419\n",
      "Epoch 35/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7107 - mse: 0.7107 - val_loss: 0.7341 - val_mse: 0.7341\n",
      "Epoch 36/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7096 - mse: 0.7096 - val_loss: 0.7315 - val_mse: 0.7315\n",
      "Epoch 37/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7083 - mse: 0.7083 - val_loss: 0.7330 - val_mse: 0.7330\n",
      "Epoch 38/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7067 - mse: 0.7067 - val_loss: 0.7389 - val_mse: 0.7389\n",
      "Epoch 39/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7062 - mse: 0.7062 - val_loss: 0.7331 - val_mse: 0.7331\n",
      "Epoch 40/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7044 - mse: 0.7044 - val_loss: 0.7329 - val_mse: 0.7329\n",
      "Epoch 41/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7033 - mse: 0.7033 - val_loss: 0.7311 - val_mse: 0.7311\n",
      "Epoch 42/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7020 - mse: 0.7020 - val_loss: 0.7316 - val_mse: 0.7316\n",
      "Epoch 43/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7007 - mse: 0.7007 - val_loss: 0.7292 - val_mse: 0.7292\n",
      "Epoch 44/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6995 - mse: 0.6995 - val_loss: 0.7364 - val_mse: 0.7364\n",
      "Epoch 45/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6983 - mse: 0.6983 - val_loss: 0.7330 - val_mse: 0.7330\n",
      "Epoch 46/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6973 - mse: 0.6973 - val_loss: 0.7337 - val_mse: 0.7337\n",
      "Epoch 47/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6963 - mse: 0.6963 - val_loss: 0.7306 - val_mse: 0.7306\n",
      "Epoch 48/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6949 - mse: 0.6949 - val_loss: 0.7373 - val_mse: 0.7373\n",
      "Epoch 49/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6943 - mse: 0.6943 - val_loss: 0.7298 - val_mse: 0.7298\n",
      "Epoch 50/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6928 - mse: 0.6928 - val_loss: 0.7316 - val_mse: 0.7316\n",
      "Epoch 51/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6925 - mse: 0.6925 - val_loss: 0.7323 - val_mse: 0.7323\n",
      "Epoch 52/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6909 - mse: 0.6909 - val_loss: 0.7285 - val_mse: 0.7285\n",
      "Epoch 53/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6897 - mse: 0.6897 - val_loss: 0.7330 - val_mse: 0.7330\n",
      "Epoch 54/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6892 - mse: 0.6892 - val_loss: 0.7362 - val_mse: 0.7362\n",
      "Epoch 55/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6876 - mse: 0.6876 - val_loss: 0.7270 - val_mse: 0.7270\n",
      "Epoch 56/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6877 - mse: 0.6877 - val_loss: 0.7330 - val_mse: 0.7330\n",
      "Epoch 57/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6857 - mse: 0.6857 - val_loss: 0.7265 - val_mse: 0.7265\n",
      "Epoch 58/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6850 - mse: 0.6850 - val_loss: 0.7326 - val_mse: 0.7326\n",
      "Epoch 59/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6844 - mse: 0.6844 - val_loss: 0.7355 - val_mse: 0.7355\n",
      "Epoch 60/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6831 - mse: 0.6831 - val_loss: 0.7339 - val_mse: 0.7339\n",
      "Epoch 61/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6824 - mse: 0.6824 - val_loss: 0.7320 - val_mse: 0.7320\n",
      "Epoch 62/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6815 - mse: 0.6815 - val_loss: 0.7337 - val_mse: 0.7337\n",
      "Epoch 63/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6805 - mse: 0.6805 - val_loss: 0.7328 - val_mse: 0.7328\n",
      "Epoch 64/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6798 - mse: 0.6798 - val_loss: 0.7376 - val_mse: 0.7376\n",
      "Epoch 65/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6791 - mse: 0.6791 - val_loss: 0.7341 - val_mse: 0.7341\n",
      "Epoch 66/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6783 - mse: 0.6783 - val_loss: 0.7349 - val_mse: 0.7349\n",
      "Epoch 67/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6770 - mse: 0.6770 - val_loss: 0.7341 - val_mse: 0.7341\n",
      "Epoch 68/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6763 - mse: 0.6763 - val_loss: 0.7277 - val_mse: 0.7277\n",
      "Epoch 69/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6760 - mse: 0.6760 - val_loss: 0.7342 - val_mse: 0.7342\n",
      "Epoch 70/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6758 - mse: 0.6758 - val_loss: 0.7351 - val_mse: 0.7351\n",
      "Epoch 71/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6743 - mse: 0.6743 - val_loss: 0.7348 - val_mse: 0.7348\n",
      "Epoch 72/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6738 - mse: 0.6738 - val_loss: 0.7356 - val_mse: 0.7356\n",
      "Epoch 73/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6724 - mse: 0.6724 - val_loss: 0.7321 - val_mse: 0.7321\n",
      "Epoch 74/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6718 - mse: 0.6718 - val_loss: 0.7325 - val_mse: 0.7325\n",
      "Epoch 75/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6714 - mse: 0.6714 - val_loss: 0.7389 - val_mse: 0.7389\n",
      "Epoch 76/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6700 - mse: 0.6700 - val_loss: 0.7367 - val_mse: 0.7367\n",
      "Epoch 77/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6696 - mse: 0.6696 - val_loss: 0.7348 - val_mse: 0.7348\n",
      "Epoch 78/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6687 - mse: 0.6687 - val_loss: 0.7357 - val_mse: 0.7357\n",
      "Epoch 79/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6683 - mse: 0.6683 - val_loss: 0.7296 - val_mse: 0.7296\n",
      "Epoch 80/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6676 - mse: 0.6676 - val_loss: 0.7353 - val_mse: 0.7353\n",
      "Epoch 81/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6666 - mse: 0.6666 - val_loss: 0.7419 - val_mse: 0.7419\n",
      "Epoch 82/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6658 - mse: 0.6658 - val_loss: 0.7367 - val_mse: 0.7367\n",
      "Epoch 83/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6659 - mse: 0.6659 - val_loss: 0.7398 - val_mse: 0.7398\n",
      "Epoch 84/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6647 - mse: 0.6647 - val_loss: 0.7401 - val_mse: 0.7401\n",
      "Epoch 85/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6645 - mse: 0.6645 - val_loss: 0.7371 - val_mse: 0.7371\n",
      "Epoch 86/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6634 - mse: 0.6634 - val_loss: 0.7349 - val_mse: 0.7349\n",
      "Epoch 87/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6629 - mse: 0.6629 - val_loss: 0.7490 - val_mse: 0.7490\n",
      "Epoch 88/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6620 - mse: 0.6620 - val_loss: 0.7379 - val_mse: 0.7379\n",
      "Epoch 89/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6615 - mse: 0.6615 - val_loss: 0.7388 - val_mse: 0.7388\n",
      "Epoch 90/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6606 - mse: 0.6606 - val_loss: 0.7265 - val_mse: 0.7265\n",
      "Epoch 91/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6601 - mse: 0.6601 - val_loss: 0.7531 - val_mse: 0.7531\n",
      "Epoch 92/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6598 - mse: 0.6598 - val_loss: 0.7365 - val_mse: 0.7365\n",
      "Epoch 93/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6590 - mse: 0.6590 - val_loss: 0.7340 - val_mse: 0.7340\n",
      "Epoch 94/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6579 - mse: 0.6579 - val_loss: 0.7362 - val_mse: 0.7362\n",
      "Epoch 95/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6577 - mse: 0.6577 - val_loss: 0.7430 - val_mse: 0.7430\n",
      "Epoch 96/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6567 - mse: 0.6567 - val_loss: 0.7442 - val_mse: 0.7442\n",
      "Epoch 97/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6565 - mse: 0.6565 - val_loss: 0.7491 - val_mse: 0.7491\n",
      "Epoch 98/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6561 - mse: 0.6561 - val_loss: 0.7468 - val_mse: 0.7468\n",
      "Epoch 99/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6552 - mse: 0.6552 - val_loss: 0.7612 - val_mse: 0.7612\n",
      "Epoch 100/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6550 - mse: 0.6550 - val_loss: 0.7450 - val_mse: 0.7450\n",
      "Epoch 101/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6539 - mse: 0.6539 - val_loss: 0.7696 - val_mse: 0.7696\n",
      "Epoch 102/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6534 - mse: 0.6534 - val_loss: 0.7487 - val_mse: 0.7487\n",
      "Epoch 103/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6529 - mse: 0.6529 - val_loss: 0.7339 - val_mse: 0.7339\n",
      "Epoch 104/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6520 - mse: 0.6520 - val_loss: 0.7560 - val_mse: 0.7560\n",
      "Epoch 105/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6522 - mse: 0.6522 - val_loss: 0.7418 - val_mse: 0.7418\n",
      "Epoch 106/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6511 - mse: 0.6511 - val_loss: 0.7650 - val_mse: 0.7650\n",
      "Epoch 107/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6511 - mse: 0.6511 - val_loss: 0.7394 - val_mse: 0.7394\n",
      "Epoch 108/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6502 - mse: 0.6502 - val_loss: 0.7366 - val_mse: 0.7366\n",
      "Epoch 109/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6496 - mse: 0.6496 - val_loss: 0.7584 - val_mse: 0.7584\n",
      "Epoch 110/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6486 - mse: 0.6486 - val_loss: 0.7553 - val_mse: 0.7553\n",
      "Epoch 111/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6488 - mse: 0.6488 - val_loss: 0.7933 - val_mse: 0.7933\n",
      "Epoch 112/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6479 - mse: 0.6479 - val_loss: 0.7420 - val_mse: 0.7420\n",
      "Epoch 113/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6475 - mse: 0.6475 - val_loss: 0.7576 - val_mse: 0.7576\n",
      "Epoch 114/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6477 - mse: 0.6477 - val_loss: 0.7574 - val_mse: 0.7574\n",
      "Epoch 115/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6470 - mse: 0.6470 - val_loss: 0.7730 - val_mse: 0.7730\n",
      "Epoch 116/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6458 - mse: 0.6458 - val_loss: 0.7362 - val_mse: 0.7362\n",
      "Epoch 117/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6455 - mse: 0.6455 - val_loss: 0.7796 - val_mse: 0.7796\n",
      "Epoch 118/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6451 - mse: 0.6451 - val_loss: 0.7658 - val_mse: 0.7658\n",
      "Epoch 119/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6444 - mse: 0.6444 - val_loss: 0.7456 - val_mse: 0.7456\n",
      "Epoch 120/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6447 - mse: 0.6447 - val_loss: 0.7876 - val_mse: 0.7876\n",
      "Epoch 121/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6435 - mse: 0.6435 - val_loss: 0.8176 - val_mse: 0.8176\n",
      "Epoch 122/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6428 - mse: 0.6428 - val_loss: 0.8108 - val_mse: 0.8108\n",
      "Epoch 123/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6421 - mse: 0.6421 - val_loss: 0.8320 - val_mse: 0.8320\n",
      "Epoch 124/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6421 - mse: 0.6421 - val_loss: 0.8434 - val_mse: 0.8434\n",
      "Epoch 125/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6416 - mse: 0.6416 - val_loss: 0.8091 - val_mse: 0.8091\n",
      "Epoch 126/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6410 - mse: 0.6410 - val_loss: 0.8827 - val_mse: 0.8827\n",
      "Epoch 127/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6408 - mse: 0.6408 - val_loss: 0.8028 - val_mse: 0.8028\n",
      "Epoch 128/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6401 - mse: 0.6401 - val_loss: 0.7954 - val_mse: 0.7954\n",
      "Epoch 129/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6401 - mse: 0.6401 - val_loss: 0.8271 - val_mse: 0.8271\n",
      "Epoch 130/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6398 - mse: 0.6398 - val_loss: 0.8914 - val_mse: 0.8914\n",
      "Epoch 131/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6386 - mse: 0.6386 - val_loss: 0.8500 - val_mse: 0.8500\n",
      "Epoch 132/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6387 - mse: 0.6387 - val_loss: 0.8789 - val_mse: 0.8789\n",
      "Epoch 133/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6384 - mse: 0.6384 - val_loss: 0.7978 - val_mse: 0.7978\n",
      "Epoch 134/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6372 - mse: 0.6372 - val_loss: 0.7995 - val_mse: 0.7995\n",
      "Epoch 135/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6372 - mse: 0.6372 - val_loss: 0.8462 - val_mse: 0.8462\n",
      "Epoch 136/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6364 - mse: 0.6364 - val_loss: 0.7984 - val_mse: 0.7984\n",
      "Epoch 137/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6356 - mse: 0.6356 - val_loss: 0.8506 - val_mse: 0.8506\n",
      "Epoch 138/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6353 - mse: 0.6353 - val_loss: 0.9568 - val_mse: 0.9568\n",
      "Epoch 139/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6356 - mse: 0.6356 - val_loss: 0.8579 - val_mse: 0.8579\n",
      "Epoch 140/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6347 - mse: 0.6347 - val_loss: 0.8988 - val_mse: 0.8988\n",
      "Epoch 141/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6341 - mse: 0.6341 - val_loss: 1.0043 - val_mse: 1.0043\n",
      "Epoch 142/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6342 - mse: 0.6342 - val_loss: 0.9113 - val_mse: 0.9113\n",
      "Epoch 143/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6332 - mse: 0.6332 - val_loss: 0.9696 - val_mse: 0.9696\n",
      "Epoch 144/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6332 - mse: 0.6332 - val_loss: 0.8809 - val_mse: 0.8809\n",
      "Epoch 145/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6322 - mse: 0.6322 - val_loss: 1.0698 - val_mse: 1.0698\n",
      "Epoch 146/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6319 - mse: 0.6319 - val_loss: 1.0112 - val_mse: 1.0112\n",
      "Epoch 147/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6320 - mse: 0.6320 - val_loss: 0.9231 - val_mse: 0.9231\n",
      "Epoch 148/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6314 - mse: 0.6314 - val_loss: 1.0451 - val_mse: 1.0451\n",
      "Epoch 149/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6315 - mse: 0.6315 - val_loss: 0.9380 - val_mse: 0.9380\n",
      "Epoch 150/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6301 - mse: 0.6301 - val_loss: 1.0189 - val_mse: 1.0189\n",
      "1676658/1676658 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.9030 - mse: 0.9030 - val_loss: 0.7815 - val_mse: 0.7815\n",
      "Epoch 2/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8251 - mse: 0.8251 - val_loss: 0.7702 - val_mse: 0.7702\n",
      "Epoch 3/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8056 - mse: 0.8056 - val_loss: 0.7619 - val_mse: 0.7619\n",
      "Epoch 4/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7906 - mse: 0.7906 - val_loss: 0.7700 - val_mse: 0.7700\n",
      "Epoch 5/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7772 - mse: 0.7772 - val_loss: 0.7578 - val_mse: 0.7578\n",
      "Epoch 6/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7658 - mse: 0.7658 - val_loss: 0.7607 - val_mse: 0.7607\n",
      "Epoch 7/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7563 - mse: 0.7563 - val_loss: 0.7414 - val_mse: 0.7414\n",
      "Epoch 8/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7495 - mse: 0.7495 - val_loss: 0.7414 - val_mse: 0.7414\n",
      "Epoch 9/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7429 - mse: 0.7429 - val_loss: 0.7310 - val_mse: 0.7310\n",
      "Epoch 10/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7379 - mse: 0.7379 - val_loss: 0.7524 - val_mse: 0.7524\n",
      "Epoch 11/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7342 - mse: 0.7342 - val_loss: 0.7411 - val_mse: 0.7411\n",
      "Epoch 12/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7301 - mse: 0.7301 - val_loss: 0.7284 - val_mse: 0.7284\n",
      "Epoch 13/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7267 - mse: 0.7267 - val_loss: 0.7217 - val_mse: 0.7217\n",
      "Epoch 14/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7234 - mse: 0.7234 - val_loss: 0.7229 - val_mse: 0.7229\n",
      "Epoch 15/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7207 - mse: 0.7207 - val_loss: 0.7174 - val_mse: 0.7174\n",
      "Epoch 16/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7182 - mse: 0.7182 - val_loss: 0.7148 - val_mse: 0.7148\n",
      "Epoch 17/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7154 - mse: 0.7154 - val_loss: 0.7167 - val_mse: 0.7167\n",
      "Epoch 18/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7130 - mse: 0.7130 - val_loss: 0.7172 - val_mse: 0.7172\n",
      "Epoch 19/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7109 - mse: 0.7109 - val_loss: 0.7150 - val_mse: 0.7150\n",
      "Epoch 20/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7092 - mse: 0.7092 - val_loss: 0.7116 - val_mse: 0.7116\n",
      "Epoch 21/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7070 - mse: 0.7070 - val_loss: 0.7112 - val_mse: 0.7112\n",
      "Epoch 22/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7053 - mse: 0.7053 - val_loss: 0.7132 - val_mse: 0.7132\n",
      "Epoch 23/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7036 - mse: 0.7036 - val_loss: 0.7069 - val_mse: 0.7069\n",
      "Epoch 24/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7016 - mse: 0.7016 - val_loss: 0.7177 - val_mse: 0.7177\n",
      "Epoch 25/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6990 - mse: 0.6990 - val_loss: 0.7095 - val_mse: 0.7095\n",
      "Epoch 26/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6980 - mse: 0.6980 - val_loss: 0.7028 - val_mse: 0.7028\n",
      "Epoch 27/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6964 - mse: 0.6964 - val_loss: 0.7009 - val_mse: 0.7009\n",
      "Epoch 28/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6945 - mse: 0.6945 - val_loss: 0.6946 - val_mse: 0.6946\n",
      "Epoch 29/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6937 - mse: 0.6937 - val_loss: 0.7020 - val_mse: 0.7020\n",
      "Epoch 30/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6912 - mse: 0.6912 - val_loss: 0.6968 - val_mse: 0.6968\n",
      "Epoch 31/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6904 - mse: 0.6904 - val_loss: 0.7021 - val_mse: 0.7021\n",
      "Epoch 32/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6886 - mse: 0.6886 - val_loss: 0.7127 - val_mse: 0.7127\n",
      "Epoch 33/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6870 - mse: 0.6870 - val_loss: 0.6946 - val_mse: 0.6946\n",
      "Epoch 34/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6863 - mse: 0.6863 - val_loss: 0.6903 - val_mse: 0.6903\n",
      "Epoch 35/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6842 - mse: 0.6842 - val_loss: 0.7015 - val_mse: 0.7015\n",
      "Epoch 36/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6834 - mse: 0.6834 - val_loss: 0.7002 - val_mse: 0.7002\n",
      "Epoch 37/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6816 - mse: 0.6816 - val_loss: 0.7013 - val_mse: 0.7013\n",
      "Epoch 38/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6807 - mse: 0.6807 - val_loss: 0.6952 - val_mse: 0.6952\n",
      "Epoch 39/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6793 - mse: 0.6793 - val_loss: 0.6899 - val_mse: 0.6899\n",
      "Epoch 40/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6778 - mse: 0.6778 - val_loss: 0.7018 - val_mse: 0.7018\n",
      "Epoch 41/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6769 - mse: 0.6769 - val_loss: 0.6816 - val_mse: 0.6816\n",
      "Epoch 42/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6753 - mse: 0.6753 - val_loss: 0.6984 - val_mse: 0.6984\n",
      "Epoch 43/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6748 - mse: 0.6748 - val_loss: 0.6877 - val_mse: 0.6877\n",
      "Epoch 44/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6736 - mse: 0.6736 - val_loss: 0.6830 - val_mse: 0.6830\n",
      "Epoch 45/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6725 - mse: 0.6725 - val_loss: 0.6832 - val_mse: 0.6832\n",
      "Epoch 46/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6711 - mse: 0.6711 - val_loss: 0.6824 - val_mse: 0.6824\n",
      "Epoch 47/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6700 - mse: 0.6700 - val_loss: 0.6796 - val_mse: 0.6796\n",
      "Epoch 48/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6692 - mse: 0.6692 - val_loss: 0.6760 - val_mse: 0.6760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6678 - mse: 0.6678 - val_loss: 0.6794 - val_mse: 0.6794\n",
      "Epoch 50/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6670 - mse: 0.6670 - val_loss: 0.6764 - val_mse: 0.6764\n",
      "Epoch 51/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6659 - mse: 0.6659 - val_loss: 0.6826 - val_mse: 0.6826\n",
      "Epoch 52/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6647 - mse: 0.6647 - val_loss: 0.6820 - val_mse: 0.6820\n",
      "Epoch 53/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6638 - mse: 0.6638 - val_loss: 0.6862 - val_mse: 0.6862\n",
      "Epoch 54/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6630 - mse: 0.6630 - val_loss: 0.6880 - val_mse: 0.6880\n",
      "Epoch 55/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6619 - mse: 0.6619 - val_loss: 0.6943 - val_mse: 0.6943\n",
      "Epoch 56/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6607 - mse: 0.6607 - val_loss: 0.6816 - val_mse: 0.6816\n",
      "Epoch 57/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6606 - mse: 0.6606 - val_loss: 0.6861 - val_mse: 0.6861\n",
      "Epoch 58/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6587 - mse: 0.6587 - val_loss: 0.6862 - val_mse: 0.6862\n",
      "Epoch 59/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6575 - mse: 0.6575 - val_loss: 0.6761 - val_mse: 0.6761\n",
      "Epoch 60/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6578 - mse: 0.6578 - val_loss: 0.6829 - val_mse: 0.6829\n",
      "Epoch 61/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6558 - mse: 0.6558 - val_loss: 0.6763 - val_mse: 0.6763\n",
      "Epoch 62/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6550 - mse: 0.6550 - val_loss: 0.6922 - val_mse: 0.6922\n",
      "Epoch 63/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6543 - mse: 0.6543 - val_loss: 0.6697 - val_mse: 0.6697\n",
      "Epoch 64/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6533 - mse: 0.6533 - val_loss: 0.6827 - val_mse: 0.6827\n",
      "Epoch 65/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6525 - mse: 0.6525 - val_loss: 0.6708 - val_mse: 0.6708\n",
      "Epoch 66/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6519 - mse: 0.6519 - val_loss: 0.6701 - val_mse: 0.6701\n",
      "Epoch 67/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6504 - mse: 0.6504 - val_loss: 0.6830 - val_mse: 0.6830\n",
      "Epoch 68/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6499 - mse: 0.6499 - val_loss: 0.6769 - val_mse: 0.6769\n",
      "Epoch 69/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6482 - mse: 0.6482 - val_loss: 0.6764 - val_mse: 0.6764\n",
      "Epoch 70/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6484 - mse: 0.6484 - val_loss: 0.6765 - val_mse: 0.6765\n",
      "Epoch 71/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6473 - mse: 0.6473 - val_loss: 0.6700 - val_mse: 0.6700\n",
      "Epoch 72/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6464 - mse: 0.6464 - val_loss: 0.6716 - val_mse: 0.6716\n",
      "Epoch 73/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6454 - mse: 0.6454 - val_loss: 0.6713 - val_mse: 0.6713\n",
      "Epoch 74/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6449 - mse: 0.6449 - val_loss: 0.6681 - val_mse: 0.6681\n",
      "Epoch 75/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6437 - mse: 0.6437 - val_loss: 0.6742 - val_mse: 0.6742\n",
      "Epoch 76/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6436 - mse: 0.6436 - val_loss: 0.6799 - val_mse: 0.6799\n",
      "Epoch 77/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6424 - mse: 0.6424 - val_loss: 0.6774 - val_mse: 0.6774\n",
      "Epoch 78/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6420 - mse: 0.6420 - val_loss: 0.6794 - val_mse: 0.6794\n",
      "Epoch 79/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6412 - mse: 0.6412 - val_loss: 0.6696 - val_mse: 0.6696\n",
      "Epoch 80/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6401 - mse: 0.6401 - val_loss: 0.6713 - val_mse: 0.6713\n",
      "Epoch 81/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6396 - mse: 0.6396 - val_loss: 0.6739 - val_mse: 0.6739\n",
      "Epoch 82/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6385 - mse: 0.6385 - val_loss: 0.6707 - val_mse: 0.6707\n",
      "Epoch 83/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6379 - mse: 0.6379 - val_loss: 0.6877 - val_mse: 0.6877\n",
      "Epoch 84/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6371 - mse: 0.6371 - val_loss: 0.6756 - val_mse: 0.6756\n",
      "Epoch 85/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6372 - mse: 0.6372 - val_loss: 0.6941 - val_mse: 0.6941\n",
      "Epoch 86/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6363 - mse: 0.6363 - val_loss: 0.6794 - val_mse: 0.6794\n",
      "Epoch 87/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6353 - mse: 0.6353 - val_loss: 0.6944 - val_mse: 0.6944\n",
      "Epoch 88/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6344 - mse: 0.6344 - val_loss: 0.6745 - val_mse: 0.6745\n",
      "Epoch 89/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6344 - mse: 0.6344 - val_loss: 0.6702 - val_mse: 0.6702\n",
      "Epoch 90/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6332 - mse: 0.6332 - val_loss: 0.6732 - val_mse: 0.6732\n",
      "Epoch 91/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6332 - mse: 0.6332 - val_loss: 0.6853 - val_mse: 0.6853\n",
      "Epoch 92/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6323 - mse: 0.6323 - val_loss: 0.6754 - val_mse: 0.6754\n",
      "Epoch 93/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6313 - mse: 0.6313 - val_loss: 0.6723 - val_mse: 0.6723\n",
      "Epoch 94/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6312 - mse: 0.6312 - val_loss: 0.6801 - val_mse: 0.6801\n",
      "Epoch 95/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6306 - mse: 0.6306 - val_loss: 0.6815 - val_mse: 0.6815\n",
      "Epoch 96/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6301 - mse: 0.6301 - val_loss: 0.6763 - val_mse: 0.6763\n",
      "Epoch 97/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6290 - mse: 0.6290 - val_loss: 0.6716 - val_mse: 0.6716\n",
      "Epoch 98/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6287 - mse: 0.6287 - val_loss: 0.6803 - val_mse: 0.6803\n",
      "Epoch 99/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6285 - mse: 0.6285 - val_loss: 0.6725 - val_mse: 0.6725\n",
      "Epoch 100/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6274 - mse: 0.6274 - val_loss: 0.6732 - val_mse: 0.6732\n",
      "Epoch 101/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6271 - mse: 0.6271 - val_loss: 0.6803 - val_mse: 0.6803\n",
      "Epoch 102/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6268 - mse: 0.6268 - val_loss: 0.6760 - val_mse: 0.6760\n",
      "Epoch 103/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6261 - mse: 0.6261 - val_loss: 0.6777 - val_mse: 0.6777\n",
      "Epoch 104/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6258 - mse: 0.6258 - val_loss: 0.6793 - val_mse: 0.6793\n",
      "Epoch 105/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6243 - mse: 0.6243 - val_loss: 0.6808 - val_mse: 0.6808\n",
      "Epoch 106/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6240 - mse: 0.6240 - val_loss: 0.6828 - val_mse: 0.6828\n",
      "Epoch 107/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6241 - mse: 0.6241 - val_loss: 0.6789 - val_mse: 0.6789\n",
      "Epoch 108/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6230 - mse: 0.6230 - val_loss: 0.6846 - val_mse: 0.6846\n",
      "Epoch 109/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6230 - mse: 0.6230 - val_loss: 0.6752 - val_mse: 0.6752\n",
      "Epoch 110/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6226 - mse: 0.6226 - val_loss: 0.6710 - val_mse: 0.6710\n",
      "Epoch 111/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6219 - mse: 0.6219 - val_loss: 0.6865 - val_mse: 0.6865\n",
      "Epoch 112/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6213 - mse: 0.6213 - val_loss: 0.6889 - val_mse: 0.6889\n",
      "Epoch 113/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6209 - mse: 0.6209 - val_loss: 0.6763 - val_mse: 0.6763\n",
      "Epoch 114/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6200 - mse: 0.6200 - val_loss: 0.6762 - val_mse: 0.6762\n",
      "Epoch 115/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6197 - mse: 0.6197 - val_loss: 0.6921 - val_mse: 0.6921\n",
      "Epoch 116/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6196 - mse: 0.6196 - val_loss: 0.6799 - val_mse: 0.6799\n",
      "Epoch 117/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6188 - mse: 0.6188 - val_loss: 0.6853 - val_mse: 0.6853\n",
      "Epoch 118/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6192 - mse: 0.6192 - val_loss: 0.6792 - val_mse: 0.6792\n",
      "Epoch 119/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6180 - mse: 0.6180 - val_loss: 0.6991 - val_mse: 0.6991\n",
      "Epoch 120/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6173 - mse: 0.6173 - val_loss: 0.6850 - val_mse: 0.6850\n",
      "Epoch 121/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6173 - mse: 0.6173 - val_loss: 0.6792 - val_mse: 0.6792\n",
      "Epoch 122/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6167 - mse: 0.6167 - val_loss: 0.6786 - val_mse: 0.6786\n",
      "Epoch 123/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6159 - mse: 0.6159 - val_loss: 0.6942 - val_mse: 0.6942\n",
      "Epoch 124/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6158 - mse: 0.6158 - val_loss: 0.6983 - val_mse: 0.6983\n",
      "Epoch 125/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6151 - mse: 0.6151 - val_loss: 0.7008 - val_mse: 0.7008\n",
      "Epoch 126/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6146 - mse: 0.6146 - val_loss: 0.6911 - val_mse: 0.6911\n",
      "Epoch 127/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6141 - mse: 0.6141 - val_loss: 0.6839 - val_mse: 0.6839\n",
      "Epoch 128/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6140 - mse: 0.6140 - val_loss: 0.6750 - val_mse: 0.6750\n",
      "Epoch 129/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6136 - mse: 0.6136 - val_loss: 0.6887 - val_mse: 0.6887\n",
      "Epoch 130/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6129 - mse: 0.6129 - val_loss: 0.6885 - val_mse: 0.6885\n",
      "Epoch 131/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6127 - mse: 0.6127 - val_loss: 0.6866 - val_mse: 0.6866\n",
      "Epoch 132/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6119 - mse: 0.6119 - val_loss: 0.6906 - val_mse: 0.6906\n",
      "Epoch 133/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6116 - mse: 0.6116 - val_loss: 0.6752 - val_mse: 0.6752\n",
      "Epoch 134/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6113 - mse: 0.6113 - val_loss: 0.6811 - val_mse: 0.6811\n",
      "Epoch 135/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6107 - mse: 0.6107 - val_loss: 0.6927 - val_mse: 0.6927\n",
      "Epoch 136/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6107 - mse: 0.6107 - val_loss: 0.6863 - val_mse: 0.6863\n",
      "Epoch 137/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6103 - mse: 0.6103 - val_loss: 0.6856 - val_mse: 0.6856\n",
      "Epoch 138/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6093 - mse: 0.6093 - val_loss: 0.6944 - val_mse: 0.6944\n",
      "Epoch 139/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6086 - mse: 0.6086 - val_loss: 0.6795 - val_mse: 0.6795\n",
      "Epoch 140/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6091 - mse: 0.6091 - val_loss: 0.6959 - val_mse: 0.6959\n",
      "Epoch 141/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6086 - mse: 0.6086 - val_loss: 0.7022 - val_mse: 0.7022\n",
      "Epoch 142/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6077 - mse: 0.6077 - val_loss: 0.6864 - val_mse: 0.6864\n",
      "Epoch 143/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6077 - mse: 0.6077 - val_loss: 0.6801 - val_mse: 0.6801\n",
      "Epoch 144/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6071 - mse: 0.6071 - val_loss: 0.6812 - val_mse: 0.6812\n",
      "Epoch 145/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6064 - mse: 0.6064 - val_loss: 0.6775 - val_mse: 0.6775\n",
      "Epoch 146/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6059 - mse: 0.6059 - val_loss: 0.7070 - val_mse: 0.7070\n",
      "Epoch 147/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6064 - mse: 0.6064 - val_loss: 0.7047 - val_mse: 0.7047\n",
      "Epoch 148/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6051 - mse: 0.6051 - val_loss: 0.6981 - val_mse: 0.6981\n",
      "Epoch 149/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6054 - mse: 0.6054 - val_loss: 0.6907 - val_mse: 0.6907\n",
      "Epoch 150/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6049 - mse: 0.6049 - val_loss: 0.6933 - val_mse: 0.6933\n",
      "Epoch 151/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6042 - mse: 0.6042 - val_loss: 0.6779 - val_mse: 0.6779\n",
      "Epoch 152/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6036 - mse: 0.6036 - val_loss: 0.6758 - val_mse: 0.6758\n",
      "Epoch 153/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6039 - mse: 0.6039 - val_loss: 0.6806 - val_mse: 0.6806\n",
      "Epoch 154/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6035 - mse: 0.6035 - val_loss: 0.6875 - val_mse: 0.6875\n",
      "Epoch 155/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6026 - mse: 0.6026 - val_loss: 0.7058 - val_mse: 0.7058\n",
      "Epoch 156/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6024 - mse: 0.6024 - val_loss: 0.7019 - val_mse: 0.7019\n",
      "Epoch 157/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6022 - mse: 0.6022 - val_loss: 0.6789 - val_mse: 0.6789\n",
      "Epoch 158/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6013 - mse: 0.6013 - val_loss: 0.6769 - val_mse: 0.6769\n",
      "Epoch 159/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6018 - mse: 0.6018 - val_loss: 0.7036 - val_mse: 0.7036\n",
      "Epoch 160/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6012 - mse: 0.6012 - val_loss: 0.6803 - val_mse: 0.6803\n",
      "Epoch 161/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6005 - mse: 0.6005 - val_loss: 0.6755 - val_mse: 0.6755\n",
      "Epoch 162/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6002 - mse: 0.6002 - val_loss: 0.7153 - val_mse: 0.7153\n",
      "Epoch 163/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6000 - mse: 0.6000 - val_loss: 0.6972 - val_mse: 0.6972\n",
      "Epoch 164/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5994 - mse: 0.5994 - val_loss: 0.7059 - val_mse: 0.7059\n",
      "Epoch 165/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5996 - mse: 0.5996 - val_loss: 0.6807 - val_mse: 0.6807\n",
      "Epoch 166/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5988 - mse: 0.5988 - val_loss: 0.6772 - val_mse: 0.6772\n",
      "Epoch 167/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5986 - mse: 0.5986 - val_loss: 0.7031 - val_mse: 0.7031\n",
      "Epoch 168/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5982 - mse: 0.5982 - val_loss: 0.6864 - val_mse: 0.6864\n",
      "Epoch 169/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5988 - mse: 0.5988 - val_loss: 0.6963 - val_mse: 0.6963\n",
      "Epoch 170/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5970 - mse: 0.5970 - val_loss: 0.6815 - val_mse: 0.6815\n",
      "Epoch 171/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5970 - mse: 0.5970 - val_loss: 0.7173 - val_mse: 0.7173\n",
      "Epoch 172/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5969 - mse: 0.5969 - val_loss: 0.7060 - val_mse: 0.7060\n",
      "Epoch 173/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5963 - mse: 0.5963 - val_loss: 0.6912 - val_mse: 0.6912\n",
      "Epoch 174/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5960 - mse: 0.5960 - val_loss: 0.6732 - val_mse: 0.6732\n",
      "Epoch 175/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5958 - mse: 0.5958 - val_loss: 0.6946 - val_mse: 0.6946\n",
      "Epoch 176/200\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.5958 - mse: 0.5958 - val_loss: 0.6935 - val_mse: 0.6935\n",
      "Epoch 177/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5950 - mse: 0.5950 - val_loss: 0.6877 - val_mse: 0.6877\n",
      "Epoch 178/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5948 - mse: 0.5948 - val_loss: 0.6851 - val_mse: 0.6851\n",
      "Epoch 179/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5941 - mse: 0.5941 - val_loss: 0.6900 - val_mse: 0.6900\n",
      "Epoch 180/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5939 - mse: 0.5939 - val_loss: 0.7183 - val_mse: 0.7183\n",
      "Epoch 181/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5940 - mse: 0.5940 - val_loss: 0.7307 - val_mse: 0.7307\n",
      "Epoch 182/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5942 - mse: 0.5942 - val_loss: 0.6709 - val_mse: 0.6709\n",
      "Epoch 183/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5929 - mse: 0.5929 - val_loss: 0.6976 - val_mse: 0.6976\n",
      "Epoch 184/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5929 - mse: 0.5929 - val_loss: 0.7165 - val_mse: 0.7165\n",
      "Epoch 185/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5922 - mse: 0.5922 - val_loss: 0.6902 - val_mse: 0.6902\n",
      "Epoch 186/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5922 - mse: 0.5922 - val_loss: 0.6959 - val_mse: 0.6959\n",
      "Epoch 187/200\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.5922 - mse: 0.5922 - val_loss: 0.6947 - val_mse: 0.6947\n",
      "Epoch 188/200\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.5915 - mse: 0.5915 - val_loss: 0.6726 - val_mse: 0.6726\n",
      "Epoch 189/200\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.5915 - mse: 0.5915 - val_loss: 0.6859 - val_mse: 0.6859\n",
      "Epoch 190/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5909 - mse: 0.5909 - val_loss: 0.6797 - val_mse: 0.6797\n",
      "Epoch 191/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5906 - mse: 0.5906 - val_loss: 0.6928 - val_mse: 0.6928\n",
      "Epoch 192/200\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.5901 - mse: 0.5901 - val_loss: 0.6995 - val_mse: 0.6995\n",
      "Epoch 193/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5903 - mse: 0.5903 - val_loss: 0.6986 - val_mse: 0.6986\n",
      "Epoch 194/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5898 - mse: 0.5898 - val_loss: 0.6872 - val_mse: 0.6872\n",
      "Epoch 195/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5898 - mse: 0.5898 - val_loss: 0.6894 - val_mse: 0.6894\n",
      "Epoch 196/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5894 - mse: 0.5894 - val_loss: 0.7045 - val_mse: 0.7045\n",
      "Epoch 197/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5894 - mse: 0.5894 - val_loss: 0.6767 - val_mse: 0.6767\n",
      "Epoch 198/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5887 - mse: 0.5887 - val_loss: 0.6923 - val_mse: 0.6923\n",
      "Epoch 199/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5880 - mse: 0.5880 - val_loss: 0.6848 - val_mse: 0.6848\n",
      "Epoch 200/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5882 - mse: 0.5882 - val_loss: 0.6994 - val_mse: 0.6994\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/200\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.7939 - mse: 0.7939 - val_loss: 0.7756 - val_mse: 0.7756\n",
      "Epoch 2/200\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.7301 - mse: 0.7301 - val_loss: 0.7662 - val_mse: 0.7662\n",
      "Epoch 3/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7171 - mse: 0.7171 - val_loss: 0.7593 - val_mse: 0.7593\n",
      "Epoch 4/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7069 - mse: 0.7069 - val_loss: 0.7630 - val_mse: 0.7630\n",
      "Epoch 5/200\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.6979 - mse: 0.6979 - val_loss: 0.7530 - val_mse: 0.7530\n",
      "Epoch 6/200\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.6907 - mse: 0.6907 - val_loss: 0.7474 - val_mse: 0.7474\n",
      "Epoch 7/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6841 - mse: 0.6841 - val_loss: 0.7445 - val_mse: 0.7445\n",
      "Epoch 8/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6787 - mse: 0.6787 - val_loss: 0.7460 - val_mse: 0.7460\n",
      "Epoch 9/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6740 - mse: 0.6740 - val_loss: 0.7401 - val_mse: 0.7401\n",
      "Epoch 10/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6693 - mse: 0.6693 - val_loss: 0.7433 - val_mse: 0.7433\n",
      "Epoch 11/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6665 - mse: 0.6665 - val_loss: 0.7399 - val_mse: 0.7399\n",
      "Epoch 12/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6633 - mse: 0.6633 - val_loss: 0.7382 - val_mse: 0.7382\n",
      "Epoch 13/200\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.6601 - mse: 0.6601 - val_loss: 0.7308 - val_mse: 0.7308\n",
      "Epoch 14/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6576 - mse: 0.6576 - val_loss: 0.7318 - val_mse: 0.7318\n",
      "Epoch 15/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6551 - mse: 0.6551 - val_loss: 0.7269 - val_mse: 0.7269\n",
      "Epoch 16/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6530 - mse: 0.6530 - val_loss: 0.7371 - val_mse: 0.7371\n",
      "Epoch 17/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6508 - mse: 0.6508 - val_loss: 0.7451 - val_mse: 0.7451\n",
      "Epoch 18/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6492 - mse: 0.6492 - val_loss: 0.7278 - val_mse: 0.7278\n",
      "Epoch 19/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6474 - mse: 0.6474 - val_loss: 0.7310 - val_mse: 0.7310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/200\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.6456 - mse: 0.6456 - val_loss: 0.7252 - val_mse: 0.7252\n",
      "Epoch 21/200\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.6436 - mse: 0.6436 - val_loss: 0.7244 - val_mse: 0.7244\n",
      "Epoch 22/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6423 - mse: 0.6423 - val_loss: 0.7214 - val_mse: 0.7214\n",
      "Epoch 23/200\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.6406 - mse: 0.6406 - val_loss: 0.7184 - val_mse: 0.7184\n",
      "Epoch 24/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6396 - mse: 0.6396 - val_loss: 0.7169 - val_mse: 0.7169\n",
      "Epoch 25/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6375 - mse: 0.6375 - val_loss: 0.7180 - val_mse: 0.7180\n",
      "Epoch 26/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6363 - mse: 0.6363 - val_loss: 0.7212 - val_mse: 0.7212\n",
      "Epoch 27/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6347 - mse: 0.6347 - val_loss: 0.7207 - val_mse: 0.7207\n",
      "Epoch 28/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6337 - mse: 0.6337 - val_loss: 0.7201 - val_mse: 0.7201\n",
      "Epoch 29/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6322 - mse: 0.6322 - val_loss: 0.7401 - val_mse: 0.7401\n",
      "Epoch 30/200\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.6307 - mse: 0.6307 - val_loss: 0.7155 - val_mse: 0.7155\n",
      "Epoch 31/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6296 - mse: 0.6296 - val_loss: 0.7157 - val_mse: 0.7157\n",
      "Epoch 32/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6281 - mse: 0.6281 - val_loss: 0.7167 - val_mse: 0.7167\n",
      "Epoch 33/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6276 - mse: 0.6276 - val_loss: 0.7112 - val_mse: 0.7112\n",
      "Epoch 34/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6264 - mse: 0.6264 - val_loss: 0.7124 - val_mse: 0.7124\n",
      "Epoch 35/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6247 - mse: 0.6247 - val_loss: 0.7219 - val_mse: 0.7219\n",
      "Epoch 36/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6236 - mse: 0.6236 - val_loss: 0.7250 - val_mse: 0.7250\n",
      "Epoch 37/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6224 - mse: 0.6224 - val_loss: 0.7169 - val_mse: 0.7169\n",
      "Epoch 38/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6217 - mse: 0.6217 - val_loss: 0.7193 - val_mse: 0.7193\n",
      "Epoch 39/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6209 - mse: 0.6209 - val_loss: 0.7124 - val_mse: 0.7124\n",
      "Epoch 40/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6200 - mse: 0.6200 - val_loss: 0.7268 - val_mse: 0.7268\n",
      "Epoch 41/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6185 - mse: 0.6185 - val_loss: 0.7107 - val_mse: 0.7107\n",
      "Epoch 42/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6177 - mse: 0.6177 - val_loss: 0.7065 - val_mse: 0.7065\n",
      "Epoch 43/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6169 - mse: 0.6169 - val_loss: 0.7241 - val_mse: 0.7241\n",
      "Epoch 44/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6157 - mse: 0.6157 - val_loss: 0.7144 - val_mse: 0.7144\n",
      "Epoch 45/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6154 - mse: 0.6154 - val_loss: 0.7172 - val_mse: 0.7172\n",
      "Epoch 46/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6138 - mse: 0.6138 - val_loss: 0.7100 - val_mse: 0.7100\n",
      "Epoch 47/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6134 - mse: 0.6134 - val_loss: 0.7078 - val_mse: 0.7078\n",
      "Epoch 48/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6123 - mse: 0.6123 - val_loss: 0.7072 - val_mse: 0.7072\n",
      "Epoch 49/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6119 - mse: 0.6119 - val_loss: 0.7149 - val_mse: 0.7149\n",
      "Epoch 50/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6110 - mse: 0.6110 - val_loss: 0.7036 - val_mse: 0.7036\n",
      "Epoch 51/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6101 - mse: 0.6101 - val_loss: 0.7067 - val_mse: 0.7067\n",
      "Epoch 52/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6091 - mse: 0.6091 - val_loss: 0.7106 - val_mse: 0.7106\n",
      "Epoch 53/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6085 - mse: 0.6085 - val_loss: 0.7031 - val_mse: 0.7031\n",
      "Epoch 54/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6083 - mse: 0.6083 - val_loss: 0.7068 - val_mse: 0.7068\n",
      "Epoch 55/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6071 - mse: 0.6071 - val_loss: 0.7139 - val_mse: 0.7139\n",
      "Epoch 56/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6065 - mse: 0.6065 - val_loss: 0.7138 - val_mse: 0.7138\n",
      "Epoch 57/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6058 - mse: 0.6058 - val_loss: 0.6978 - val_mse: 0.6978\n",
      "Epoch 58/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6054 - mse: 0.6054 - val_loss: 0.7029 - val_mse: 0.7029\n",
      "Epoch 59/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6045 - mse: 0.6045 - val_loss: 0.7074 - val_mse: 0.7074\n",
      "Epoch 60/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6042 - mse: 0.6042 - val_loss: 0.6991 - val_mse: 0.6991\n",
      "Epoch 61/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6026 - mse: 0.6026 - val_loss: 0.6969 - val_mse: 0.6969\n",
      "Epoch 62/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6024 - mse: 0.6024 - val_loss: 0.7076 - val_mse: 0.7076\n",
      "Epoch 63/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6019 - mse: 0.6019 - val_loss: 0.6994 - val_mse: 0.6994\n",
      "Epoch 64/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6014 - mse: 0.6014 - val_loss: 0.7042 - val_mse: 0.7042\n",
      "Epoch 65/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6002 - mse: 0.6002 - val_loss: 0.7007 - val_mse: 0.7007\n",
      "Epoch 66/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6003 - mse: 0.6003 - val_loss: 0.6939 - val_mse: 0.6939\n",
      "Epoch 67/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5993 - mse: 0.5993 - val_loss: 0.6989 - val_mse: 0.6989\n",
      "Epoch 68/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5985 - mse: 0.5985 - val_loss: 0.7073 - val_mse: 0.7073\n",
      "Epoch 69/200\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.5982 - mse: 0.5982 - val_loss: 0.7004 - val_mse: 0.7004\n",
      "Epoch 70/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5976 - mse: 0.5976 - val_loss: 0.7025 - val_mse: 0.7025\n",
      "Epoch 71/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5971 - mse: 0.5971 - val_loss: 0.6935 - val_mse: 0.6935\n",
      "Epoch 72/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5962 - mse: 0.5962 - val_loss: 0.6973 - val_mse: 0.6973\n",
      "Epoch 73/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5960 - mse: 0.5960 - val_loss: 0.6948 - val_mse: 0.6948\n",
      "Epoch 74/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5956 - mse: 0.5956 - val_loss: 0.6963 - val_mse: 0.6963\n",
      "Epoch 75/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5947 - mse: 0.5947 - val_loss: 0.6984 - val_mse: 0.6984\n",
      "Epoch 76/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5944 - mse: 0.5944 - val_loss: 0.6979 - val_mse: 0.6979\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5935 - mse: 0.5935 - val_loss: 0.7066 - val_mse: 0.7066\n",
      "Epoch 78/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5928 - mse: 0.5928 - val_loss: 0.6961 - val_mse: 0.6961\n",
      "Epoch 79/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5934 - mse: 0.5934 - val_loss: 0.6922 - val_mse: 0.6922\n",
      "Epoch 80/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5920 - mse: 0.5920 - val_loss: 0.6959 - val_mse: 0.6959\n",
      "Epoch 81/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5920 - mse: 0.5920 - val_loss: 0.7307 - val_mse: 0.7307\n",
      "Epoch 82/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5909 - mse: 0.5909 - val_loss: 0.6987 - val_mse: 0.6987\n",
      "Epoch 83/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5911 - mse: 0.5911 - val_loss: 0.6962 - val_mse: 0.6962\n",
      "Epoch 84/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5902 - mse: 0.5902 - val_loss: 0.7030 - val_mse: 0.7030\n",
      "Epoch 85/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5894 - mse: 0.5894 - val_loss: 0.6949 - val_mse: 0.6949\n",
      "Epoch 86/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5899 - mse: 0.5899 - val_loss: 0.6953 - val_mse: 0.6953\n",
      "Epoch 87/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5885 - mse: 0.5885 - val_loss: 0.6928 - val_mse: 0.6928\n",
      "Epoch 88/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5891 - mse: 0.5891 - val_loss: 0.6953 - val_mse: 0.6953\n",
      "Epoch 89/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5878 - mse: 0.5878 - val_loss: 0.6969 - val_mse: 0.6969\n",
      "Epoch 90/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5873 - mse: 0.5873 - val_loss: 0.6991 - val_mse: 0.6991\n",
      "Epoch 91/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5866 - mse: 0.5866 - val_loss: 0.6943 - val_mse: 0.6943\n",
      "Epoch 92/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5863 - mse: 0.5863 - val_loss: 0.6880 - val_mse: 0.6880\n",
      "Epoch 93/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5866 - mse: 0.5866 - val_loss: 0.6995 - val_mse: 0.6995\n",
      "Epoch 94/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.5856 - mse: 0.5856 - val_loss: 0.6928 - val_mse: 0.6928\n",
      "Epoch 95/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5851 - mse: 0.5851 - val_loss: 0.6901 - val_mse: 0.6901\n",
      "Epoch 96/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5851 - mse: 0.5851 - val_loss: 0.6973 - val_mse: 0.6973\n",
      "Epoch 97/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5846 - mse: 0.5846 - val_loss: 0.6987 - val_mse: 0.6987\n",
      "Epoch 98/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5840 - mse: 0.5840 - val_loss: 0.6945 - val_mse: 0.6945\n",
      "Epoch 99/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5837 - mse: 0.5837 - val_loss: 0.6941 - val_mse: 0.6941\n",
      "Epoch 100/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5832 - mse: 0.5832 - val_loss: 0.7037 - val_mse: 0.7037\n",
      "Epoch 101/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5830 - mse: 0.5830 - val_loss: 0.6997 - val_mse: 0.6997\n",
      "Epoch 102/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5824 - mse: 0.5824 - val_loss: 0.6964 - val_mse: 0.6964\n",
      "Epoch 103/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5816 - mse: 0.5816 - val_loss: 0.6973 - val_mse: 0.6973\n",
      "Epoch 104/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5813 - mse: 0.5813 - val_loss: 0.6996 - val_mse: 0.6996\n",
      "Epoch 105/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5813 - mse: 0.5813 - val_loss: 0.7050 - val_mse: 0.7050\n",
      "Epoch 106/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5805 - mse: 0.5805 - val_loss: 0.7120 - val_mse: 0.7120\n",
      "Epoch 107/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5804 - mse: 0.5804 - val_loss: 0.6979 - val_mse: 0.6979\n",
      "Epoch 108/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5802 - mse: 0.5802 - val_loss: 0.6937 - val_mse: 0.6937\n",
      "Epoch 109/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5793 - mse: 0.5793 - val_loss: 0.6982 - val_mse: 0.6982\n",
      "Epoch 110/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5794 - mse: 0.5794 - val_loss: 0.6866 - val_mse: 0.6866\n",
      "Epoch 111/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5786 - mse: 0.5786 - val_loss: 0.6917 - val_mse: 0.6917\n",
      "Epoch 112/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5788 - mse: 0.5788 - val_loss: 0.6992 - val_mse: 0.6992\n",
      "Epoch 113/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5786 - mse: 0.5786 - val_loss: 0.6938 - val_mse: 0.6938\n",
      "Epoch 114/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5777 - mse: 0.5777 - val_loss: 0.7087 - val_mse: 0.7087\n",
      "Epoch 115/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5772 - mse: 0.5772 - val_loss: 0.6887 - val_mse: 0.6887\n",
      "Epoch 116/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5773 - mse: 0.5773 - val_loss: 0.6943 - val_mse: 0.6943\n",
      "Epoch 117/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5763 - mse: 0.5763 - val_loss: 0.6873 - val_mse: 0.6873\n",
      "Epoch 118/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5768 - mse: 0.5768 - val_loss: 0.7096 - val_mse: 0.7096\n",
      "Epoch 119/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5759 - mse: 0.5759 - val_loss: 0.6922 - val_mse: 0.6922\n",
      "Epoch 120/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5753 - mse: 0.5753 - val_loss: 0.6940 - val_mse: 0.6940\n",
      "Epoch 121/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5754 - mse: 0.5754 - val_loss: 0.6968 - val_mse: 0.6968\n",
      "Epoch 122/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5748 - mse: 0.5748 - val_loss: 0.6925 - val_mse: 0.6925\n",
      "Epoch 123/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5745 - mse: 0.5745 - val_loss: 0.6863 - val_mse: 0.6863\n",
      "Epoch 124/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5738 - mse: 0.5738 - val_loss: 0.6967 - val_mse: 0.6967\n",
      "Epoch 125/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5743 - mse: 0.5743 - val_loss: 0.6905 - val_mse: 0.6905\n",
      "Epoch 126/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5735 - mse: 0.5735 - val_loss: 0.6928 - val_mse: 0.6928\n",
      "Epoch 127/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5734 - mse: 0.5734 - val_loss: 0.6925 - val_mse: 0.6925\n",
      "Epoch 128/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5727 - mse: 0.5727 - val_loss: 0.6912 - val_mse: 0.6912\n",
      "Epoch 129/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5721 - mse: 0.5721 - val_loss: 0.6974 - val_mse: 0.6974\n",
      "Epoch 130/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5726 - mse: 0.5726 - val_loss: 0.6867 - val_mse: 0.6867\n",
      "Epoch 131/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5722 - mse: 0.5722 - val_loss: 0.6934 - val_mse: 0.6934\n",
      "Epoch 132/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5716 - mse: 0.5716 - val_loss: 0.6884 - val_mse: 0.6884\n",
      "Epoch 133/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5717 - mse: 0.5717 - val_loss: 0.6997 - val_mse: 0.6997\n",
      "Epoch 134/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5709 - mse: 0.5709 - val_loss: 0.7043 - val_mse: 0.7043\n",
      "Epoch 135/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5709 - mse: 0.5709 - val_loss: 0.6958 - val_mse: 0.6958\n",
      "Epoch 136/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5707 - mse: 0.5707 - val_loss: 0.6913 - val_mse: 0.6913\n",
      "Epoch 137/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5702 - mse: 0.5702 - val_loss: 0.6997 - val_mse: 0.6997\n",
      "Epoch 138/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5696 - mse: 0.5696 - val_loss: 0.6871 - val_mse: 0.6871\n",
      "Epoch 139/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5695 - mse: 0.5695 - val_loss: 0.6949 - val_mse: 0.6949\n",
      "Epoch 140/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5695 - mse: 0.5695 - val_loss: 0.6998 - val_mse: 0.6998\n",
      "Epoch 141/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5688 - mse: 0.5688 - val_loss: 0.6963 - val_mse: 0.6963\n",
      "Epoch 142/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5684 - mse: 0.5684 - val_loss: 0.6921 - val_mse: 0.6921\n",
      "Epoch 143/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5679 - mse: 0.5679 - val_loss: 0.6999 - val_mse: 0.6999\n",
      "Epoch 144/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5681 - mse: 0.5681 - val_loss: 0.6946 - val_mse: 0.6946\n",
      "Epoch 145/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5680 - mse: 0.5680 - val_loss: 0.6928 - val_mse: 0.6928\n",
      "Epoch 146/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5671 - mse: 0.5671 - val_loss: 0.6929 - val_mse: 0.6929\n",
      "Epoch 147/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5672 - mse: 0.5672 - val_loss: 0.6971 - val_mse: 0.6971\n",
      "Epoch 148/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5666 - mse: 0.5666 - val_loss: 0.6887 - val_mse: 0.6887\n",
      "Epoch 149/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5669 - mse: 0.5669 - val_loss: 0.6948 - val_mse: 0.6948\n",
      "Epoch 150/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5667 - mse: 0.5667 - val_loss: 0.6892 - val_mse: 0.6892\n",
      "Epoch 151/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5655 - mse: 0.5655 - val_loss: 0.7043 - val_mse: 0.7043\n",
      "Epoch 152/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5661 - mse: 0.5661 - val_loss: 0.6966 - val_mse: 0.6966\n",
      "Epoch 153/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5649 - mse: 0.5649 - val_loss: 0.7067 - val_mse: 0.7067\n",
      "Epoch 154/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5648 - mse: 0.5648 - val_loss: 0.6922 - val_mse: 0.6922\n",
      "Epoch 155/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5651 - mse: 0.5651 - val_loss: 0.6797 - val_mse: 0.6797\n",
      "Epoch 156/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5644 - mse: 0.5644 - val_loss: 0.6884 - val_mse: 0.6884\n",
      "Epoch 157/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5640 - mse: 0.5640 - val_loss: 0.6884 - val_mse: 0.6884\n",
      "Epoch 158/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5641 - mse: 0.5641 - val_loss: 0.6975 - val_mse: 0.6975\n",
      "Epoch 159/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5635 - mse: 0.5635 - val_loss: 0.6872 - val_mse: 0.6872\n",
      "Epoch 160/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5631 - mse: 0.5631 - val_loss: 0.7066 - val_mse: 0.7066\n",
      "Epoch 161/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5630 - mse: 0.5630 - val_loss: 0.6936 - val_mse: 0.6936\n",
      "Epoch 162/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5629 - mse: 0.5629 - val_loss: 0.6977 - val_mse: 0.6977\n",
      "Epoch 163/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5625 - mse: 0.5625 - val_loss: 0.6927 - val_mse: 0.6927\n",
      "Epoch 164/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5620 - mse: 0.5620 - val_loss: 0.6916 - val_mse: 0.6916\n",
      "Epoch 165/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5626 - mse: 0.5626 - val_loss: 0.6955 - val_mse: 0.6955\n",
      "Epoch 166/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5617 - mse: 0.5617 - val_loss: 0.6884 - val_mse: 0.6884\n",
      "Epoch 167/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5614 - mse: 0.5614 - val_loss: 0.6895 - val_mse: 0.6895\n",
      "Epoch 168/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5614 - mse: 0.5614 - val_loss: 0.6954 - val_mse: 0.6954\n",
      "Epoch 169/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5611 - mse: 0.5611 - val_loss: 0.6844 - val_mse: 0.6844\n",
      "Epoch 170/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5609 - mse: 0.5609 - val_loss: 0.7085 - val_mse: 0.7085\n",
      "Epoch 171/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5599 - mse: 0.5599 - val_loss: 0.6858 - val_mse: 0.6858\n",
      "Epoch 172/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5602 - mse: 0.5602 - val_loss: 0.6900 - val_mse: 0.6900\n",
      "Epoch 173/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5596 - mse: 0.5596 - val_loss: 0.7043 - val_mse: 0.7043\n",
      "Epoch 174/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5596 - mse: 0.5596 - val_loss: 0.6899 - val_mse: 0.6899\n",
      "Epoch 175/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5591 - mse: 0.5591 - val_loss: 0.6934 - val_mse: 0.6934\n",
      "Epoch 176/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5588 - mse: 0.5588 - val_loss: 0.6942 - val_mse: 0.6942\n",
      "Epoch 177/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5585 - mse: 0.5585 - val_loss: 0.6850 - val_mse: 0.6850\n",
      "Epoch 178/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5588 - mse: 0.5588 - val_loss: 0.6930 - val_mse: 0.6930\n",
      "Epoch 179/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5582 - mse: 0.5582 - val_loss: 0.6851 - val_mse: 0.6851\n",
      "Epoch 180/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5580 - mse: 0.5580 - val_loss: 0.6836 - val_mse: 0.6836\n",
      "Epoch 181/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5572 - mse: 0.5572 - val_loss: 0.6946 - val_mse: 0.6946\n",
      "Epoch 182/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5575 - mse: 0.5575 - val_loss: 0.6957 - val_mse: 0.6957\n",
      "Epoch 183/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5572 - mse: 0.5572 - val_loss: 0.6974 - val_mse: 0.6974\n",
      "Epoch 184/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5566 - mse: 0.5566 - val_loss: 0.6998 - val_mse: 0.6998\n",
      "Epoch 185/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5569 - mse: 0.5569 - val_loss: 0.6842 - val_mse: 0.6842\n",
      "Epoch 186/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5564 - mse: 0.5564 - val_loss: 0.6909 - val_mse: 0.6909\n",
      "Epoch 187/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5563 - mse: 0.5563 - val_loss: 0.7080 - val_mse: 0.7080\n",
      "Epoch 188/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5558 - mse: 0.5558 - val_loss: 0.6862 - val_mse: 0.6862\n",
      "Epoch 189/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5562 - mse: 0.5562 - val_loss: 0.6868 - val_mse: 0.6868\n",
      "Epoch 190/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5555 - mse: 0.5555 - val_loss: 0.6870 - val_mse: 0.6870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5552 - mse: 0.5552 - val_loss: 0.7062 - val_mse: 0.7062\n",
      "Epoch 192/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5547 - mse: 0.5547 - val_loss: 0.6901 - val_mse: 0.6901\n",
      "Epoch 193/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5550 - mse: 0.5550 - val_loss: 0.6872 - val_mse: 0.6872\n",
      "Epoch 194/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.5542 - mse: 0.5542 - val_loss: 0.6924 - val_mse: 0.6924\n",
      "Epoch 195/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5546 - mse: 0.5546 - val_loss: 0.6905 - val_mse: 0.6905\n",
      "Epoch 196/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5546 - mse: 0.5546 - val_loss: 0.6885 - val_mse: 0.6885\n",
      "Epoch 197/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5533 - mse: 0.5533 - val_loss: 0.6764 - val_mse: 0.6764\n",
      "Epoch 198/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5531 - mse: 0.5531 - val_loss: 0.6896 - val_mse: 0.6896\n",
      "Epoch 199/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5530 - mse: 0.5530 - val_loss: 0.6956 - val_mse: 0.6956\n",
      "Epoch 200/200\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.5529 - mse: 0.5529 - val_loss: 0.6972 - val_mse: 0.6972\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/200\n",
      "3353318/3353318 [==============================] - 11s 3us/step - loss: 0.9298 - mse: 0.9298 - val_loss: 0.8295 - val_mse: 0.8295\n",
      "Epoch 2/200\n",
      "3353318/3353318 [==============================] - 11s 3us/step - loss: 0.8366 - mse: 0.8366 - val_loss: 0.8401 - val_mse: 0.8401\n",
      "Epoch 3/200\n",
      "3353318/3353318 [==============================] - 11s 3us/step - loss: 0.8182 - mse: 0.8182 - val_loss: 0.8261 - val_mse: 0.8261\n",
      "Epoch 4/200\n",
      "3353318/3353318 [==============================] - 11s 3us/step - loss: 0.8041 - mse: 0.8041 - val_loss: 0.8149 - val_mse: 0.8149\n",
      "Epoch 5/200\n",
      "3353318/3353318 [==============================] - 11s 3us/step - loss: 0.7927 - mse: 0.7927 - val_loss: 0.8120 - val_mse: 0.8120\n",
      "Epoch 6/200\n",
      "3353318/3353318 [==============================] - 11s 3us/step - loss: 0.7830 - mse: 0.7830 - val_loss: 0.8063 - val_mse: 0.8063\n",
      "Epoch 7/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7761 - mse: 0.7761 - val_loss: 0.7857 - val_mse: 0.7857\n",
      "Epoch 8/200\n",
      "3353318/3353318 [==============================] - 11s 3us/step - loss: 0.7691 - mse: 0.7691 - val_loss: 0.7858 - val_mse: 0.7858\n",
      "Epoch 9/200\n",
      "3353318/3353318 [==============================] - 11s 3us/step - loss: 0.7650 - mse: 0.7650 - val_loss: 0.7810 - val_mse: 0.7810\n",
      "Epoch 10/200\n",
      "3353318/3353318 [==============================] - 11s 3us/step - loss: 0.7595 - mse: 0.7595 - val_loss: 0.7709 - val_mse: 0.7709\n",
      "Epoch 11/200\n",
      "3353318/3353318 [==============================] - 11s 3us/step - loss: 0.7561 - mse: 0.7561 - val_loss: 0.7692 - val_mse: 0.7692\n",
      "Epoch 12/200\n",
      "3353318/3353318 [==============================] - 11s 3us/step - loss: 0.7536 - mse: 0.7536 - val_loss: 0.7879 - val_mse: 0.7879\n",
      "Epoch 13/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7501 - mse: 0.7501 - val_loss: 0.7600 - val_mse: 0.7600\n",
      "Epoch 14/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7468 - mse: 0.7468 - val_loss: 0.7637 - val_mse: 0.7637\n",
      "Epoch 15/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7449 - mse: 0.7449 - val_loss: 0.7662 - val_mse: 0.7662\n",
      "Epoch 16/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7423 - mse: 0.7423 - val_loss: 0.7594 - val_mse: 0.7594\n",
      "Epoch 17/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7401 - mse: 0.7401 - val_loss: 0.7517 - val_mse: 0.7517\n",
      "Epoch 18/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7381 - mse: 0.7381 - val_loss: 0.7537 - val_mse: 0.7537\n",
      "Epoch 19/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7361 - mse: 0.7361 - val_loss: 0.7505 - val_mse: 0.7505\n",
      "Epoch 20/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7342 - mse: 0.7342 - val_loss: 0.7485 - val_mse: 0.7485\n",
      "Epoch 21/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7325 - mse: 0.7325 - val_loss: 0.7468 - val_mse: 0.7468\n",
      "Epoch 22/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7304 - mse: 0.7304 - val_loss: 0.7482 - val_mse: 0.7482\n",
      "Epoch 23/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7285 - mse: 0.7285 - val_loss: 0.7445 - val_mse: 0.7445\n",
      "Epoch 24/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7267 - mse: 0.7267 - val_loss: 0.7434 - val_mse: 0.7434\n",
      "Epoch 25/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7263 - mse: 0.7263 - val_loss: 0.7417 - val_mse: 0.7417\n",
      "Epoch 26/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7238 - mse: 0.7238 - val_loss: 0.7439 - val_mse: 0.7439\n",
      "Epoch 27/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7224 - mse: 0.7224 - val_loss: 0.7460 - val_mse: 0.7460\n",
      "Epoch 28/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7209 - mse: 0.7209 - val_loss: 0.7456 - val_mse: 0.7456\n",
      "Epoch 29/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7199 - mse: 0.7199 - val_loss: 0.7392 - val_mse: 0.7392\n",
      "Epoch 30/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7183 - mse: 0.7183 - val_loss: 0.7475 - val_mse: 0.7475\n",
      "Epoch 31/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7168 - mse: 0.7168 - val_loss: 0.7427 - val_mse: 0.7427\n",
      "Epoch 32/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7155 - mse: 0.7155 - val_loss: 0.7341 - val_mse: 0.7341\n",
      "Epoch 33/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7144 - mse: 0.7144 - val_loss: 0.7405 - val_mse: 0.7405\n",
      "Epoch 34/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7132 - mse: 0.7132 - val_loss: 0.7471 - val_mse: 0.7471\n",
      "Epoch 35/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7116 - mse: 0.7116 - val_loss: 0.7485 - val_mse: 0.7485\n",
      "Epoch 36/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7112 - mse: 0.7112 - val_loss: 0.7418 - val_mse: 0.7418\n",
      "Epoch 37/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7092 - mse: 0.7092 - val_loss: 0.7520 - val_mse: 0.7520\n",
      "Epoch 38/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7079 - mse: 0.7079 - val_loss: 0.7479 - val_mse: 0.7479\n",
      "Epoch 39/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.7071 - mse: 0.7071 - val_loss: 0.7498 - val_mse: 0.7498\n",
      "Epoch 40/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7059 - mse: 0.7059 - val_loss: 0.7427 - val_mse: 0.7427\n",
      "Epoch 41/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7048 - mse: 0.7048 - val_loss: 0.7442 - val_mse: 0.7442\n",
      "Epoch 42/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.7037 - mse: 0.7037 - val_loss: 0.7619 - val_mse: 0.7619\n",
      "Epoch 43/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7022 - mse: 0.7022 - val_loss: 0.7461 - val_mse: 0.7461\n",
      "Epoch 44/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.7011 - mse: 0.7011 - val_loss: 0.7423 - val_mse: 0.7423\n",
      "Epoch 45/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.7002 - mse: 0.7002 - val_loss: 0.7411 - val_mse: 0.7411\n",
      "Epoch 46/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6987 - mse: 0.6987 - val_loss: 0.7444 - val_mse: 0.7444\n",
      "Epoch 47/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6978 - mse: 0.6978 - val_loss: 0.7445 - val_mse: 0.7445\n",
      "Epoch 48/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6965 - mse: 0.6965 - val_loss: 0.7389 - val_mse: 0.7389\n",
      "Epoch 49/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6956 - mse: 0.6956 - val_loss: 0.7414 - val_mse: 0.7414\n",
      "Epoch 50/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6946 - mse: 0.6946 - val_loss: 0.7447 - val_mse: 0.7447\n",
      "Epoch 51/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6943 - mse: 0.6943 - val_loss: 0.7368 - val_mse: 0.7368\n",
      "Epoch 52/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6925 - mse: 0.6925 - val_loss: 0.7530 - val_mse: 0.7530\n",
      "Epoch 53/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6915 - mse: 0.6915 - val_loss: 0.7467 - val_mse: 0.7467\n",
      "Epoch 54/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6906 - mse: 0.6906 - val_loss: 0.7423 - val_mse: 0.7423\n",
      "Epoch 55/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6897 - mse: 0.6897 - val_loss: 0.7385 - val_mse: 0.7385\n",
      "Epoch 56/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6891 - mse: 0.6891 - val_loss: 0.7369 - val_mse: 0.7369\n",
      "Epoch 57/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6881 - mse: 0.6881 - val_loss: 0.7357 - val_mse: 0.7357\n",
      "Epoch 58/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6873 - mse: 0.6873 - val_loss: 0.7403 - val_mse: 0.7403\n",
      "Epoch 59/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6859 - mse: 0.6859 - val_loss: 0.7412 - val_mse: 0.7412\n",
      "Epoch 60/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6847 - mse: 0.6847 - val_loss: 0.7441 - val_mse: 0.7441\n",
      "Epoch 61/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6841 - mse: 0.6841 - val_loss: 0.7362 - val_mse: 0.7362\n",
      "Epoch 62/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6836 - mse: 0.6836 - val_loss: 0.7372 - val_mse: 0.7372\n",
      "Epoch 63/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6826 - mse: 0.6826 - val_loss: 0.7433 - val_mse: 0.7433\n",
      "Epoch 64/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6825 - mse: 0.6825 - val_loss: 0.7379 - val_mse: 0.7379\n",
      "Epoch 65/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6805 - mse: 0.6805 - val_loss: 0.7400 - val_mse: 0.7400\n",
      "Epoch 66/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6798 - mse: 0.6798 - val_loss: 0.7335 - val_mse: 0.7335\n",
      "Epoch 67/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6795 - mse: 0.6795 - val_loss: 0.7304 - val_mse: 0.7304\n",
      "Epoch 68/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6786 - mse: 0.6786 - val_loss: 0.7332 - val_mse: 0.7332\n",
      "Epoch 69/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6782 - mse: 0.6782 - val_loss: 0.7324 - val_mse: 0.7324\n",
      "Epoch 70/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6778 - mse: 0.6778 - val_loss: 0.7312 - val_mse: 0.7312\n",
      "Epoch 71/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6760 - mse: 0.6760 - val_loss: 0.7340 - val_mse: 0.7340\n",
      "Epoch 72/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6753 - mse: 0.6753 - val_loss: 0.7289 - val_mse: 0.7289\n",
      "Epoch 73/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6750 - mse: 0.6750 - val_loss: 0.7379 - val_mse: 0.7379\n",
      "Epoch 74/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6740 - mse: 0.6740 - val_loss: 0.7328 - val_mse: 0.7328\n",
      "Epoch 75/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6736 - mse: 0.6736 - val_loss: 0.7453 - val_mse: 0.7453\n",
      "Epoch 76/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6724 - mse: 0.6724 - val_loss: 0.7433 - val_mse: 0.7433\n",
      "Epoch 77/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6721 - mse: 0.6721 - val_loss: 0.7255 - val_mse: 0.7255\n",
      "Epoch 78/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6708 - mse: 0.6708 - val_loss: 0.7322 - val_mse: 0.7322\n",
      "Epoch 79/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6705 - mse: 0.6705 - val_loss: 0.7404 - val_mse: 0.7404\n",
      "Epoch 80/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6697 - mse: 0.6697 - val_loss: 0.7328 - val_mse: 0.7328\n",
      "Epoch 81/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6690 - mse: 0.6690 - val_loss: 0.7391 - val_mse: 0.7391\n",
      "Epoch 82/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6688 - mse: 0.6688 - val_loss: 0.7368 - val_mse: 0.7368\n",
      "Epoch 83/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6679 - mse: 0.6679 - val_loss: 0.7226 - val_mse: 0.7226\n",
      "Epoch 84/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6676 - mse: 0.6676 - val_loss: 0.7410 - val_mse: 0.7410\n",
      "Epoch 85/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6667 - mse: 0.6667 - val_loss: 0.7289 - val_mse: 0.7289\n",
      "Epoch 86/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6666 - mse: 0.6666 - val_loss: 0.7267 - val_mse: 0.7267\n",
      "Epoch 87/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6656 - mse: 0.6656 - val_loss: 0.7300 - val_mse: 0.7300\n",
      "Epoch 88/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6651 - mse: 0.6651 - val_loss: 0.7364 - val_mse: 0.7364\n",
      "Epoch 89/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6637 - mse: 0.6637 - val_loss: 0.7359 - val_mse: 0.7359\n",
      "Epoch 90/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6635 - mse: 0.6635 - val_loss: 0.7229 - val_mse: 0.7229\n",
      "Epoch 91/200\n",
      "3353318/3353318 [==============================] - 11s 3us/step - loss: 0.6629 - mse: 0.6629 - val_loss: 0.7276 - val_mse: 0.7276\n",
      "Epoch 92/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6622 - mse: 0.6622 - val_loss: 0.7238 - val_mse: 0.7238\n",
      "Epoch 93/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6618 - mse: 0.6618 - val_loss: 0.7314 - val_mse: 0.7314\n",
      "Epoch 94/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6611 - mse: 0.6611 - val_loss: 0.7354 - val_mse: 0.7354\n",
      "Epoch 95/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6606 - mse: 0.6606 - val_loss: 0.7306 - val_mse: 0.7306\n",
      "Epoch 96/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6599 - mse: 0.6599 - val_loss: 0.7298 - val_mse: 0.7298\n",
      "Epoch 97/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6599 - mse: 0.6599 - val_loss: 0.7307 - val_mse: 0.7307\n",
      "Epoch 98/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6585 - mse: 0.6585 - val_loss: 0.7410 - val_mse: 0.7410\n",
      "Epoch 99/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6582 - mse: 0.6582 - val_loss: 0.7351 - val_mse: 0.7351\n",
      "Epoch 100/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6572 - mse: 0.6572 - val_loss: 0.7392 - val_mse: 0.7392\n",
      "Epoch 101/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6574 - mse: 0.6574 - val_loss: 0.7447 - val_mse: 0.7447\n",
      "Epoch 102/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6567 - mse: 0.6567 - val_loss: 0.7230 - val_mse: 0.7230\n",
      "Epoch 103/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6557 - mse: 0.6557 - val_loss: 0.7314 - val_mse: 0.7314\n",
      "Epoch 104/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6555 - mse: 0.6555 - val_loss: 0.7286 - val_mse: 0.7286\n",
      "Epoch 105/200\n",
      "3353318/3353318 [==============================] - 11s 3us/step - loss: 0.6553 - mse: 0.6553 - val_loss: 0.7297 - val_mse: 0.7297\n",
      "Epoch 106/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6539 - mse: 0.6539 - val_loss: 0.7254 - val_mse: 0.7254\n",
      "Epoch 107/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6541 - mse: 0.6541 - val_loss: 0.7363 - val_mse: 0.7363\n",
      "Epoch 108/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6538 - mse: 0.6538 - val_loss: 0.7350 - val_mse: 0.7350\n",
      "Epoch 109/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6527 - mse: 0.6527 - val_loss: 0.7302 - val_mse: 0.7302\n",
      "Epoch 110/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6520 - mse: 0.6520 - val_loss: 0.7354 - val_mse: 0.7354\n",
      "Epoch 111/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6521 - mse: 0.6521 - val_loss: 0.7291 - val_mse: 0.7291\n",
      "Epoch 112/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6515 - mse: 0.6515 - val_loss: 0.7283 - val_mse: 0.7283\n",
      "Epoch 113/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6509 - mse: 0.6509 - val_loss: 0.7293 - val_mse: 0.7293\n",
      "Epoch 114/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6506 - mse: 0.6506 - val_loss: 0.7255 - val_mse: 0.7255\n",
      "Epoch 115/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6500 - mse: 0.6500 - val_loss: 0.7241 - val_mse: 0.7241\n",
      "Epoch 116/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6489 - mse: 0.6489 - val_loss: 0.7293 - val_mse: 0.7293\n",
      "Epoch 117/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6490 - mse: 0.6490 - val_loss: 0.7328 - val_mse: 0.7328\n",
      "Epoch 118/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6482 - mse: 0.6482 - val_loss: 0.7312 - val_mse: 0.7312\n",
      "Epoch 119/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6478 - mse: 0.6478 - val_loss: 0.7236 - val_mse: 0.7236\n",
      "Epoch 120/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6478 - mse: 0.6478 - val_loss: 0.7274 - val_mse: 0.7274\n",
      "Epoch 121/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6466 - mse: 0.6466 - val_loss: 0.7358 - val_mse: 0.7358\n",
      "Epoch 122/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6462 - mse: 0.6462 - val_loss: 0.7290 - val_mse: 0.7290\n",
      "Epoch 123/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6454 - mse: 0.6454 - val_loss: 0.7320 - val_mse: 0.7320\n",
      "Epoch 124/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6457 - mse: 0.6457 - val_loss: 0.7300 - val_mse: 0.7300\n",
      "Epoch 125/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6449 - mse: 0.6449 - val_loss: 0.7346 - val_mse: 0.7346\n",
      "Epoch 126/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6449 - mse: 0.6449 - val_loss: 0.7296 - val_mse: 0.7296\n",
      "Epoch 127/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6436 - mse: 0.6436 - val_loss: 0.7336 - val_mse: 0.7336\n",
      "Epoch 128/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6435 - mse: 0.6435 - val_loss: 0.7365 - val_mse: 0.7365\n",
      "Epoch 129/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6428 - mse: 0.6428 - val_loss: 0.7307 - val_mse: 0.7307\n",
      "Epoch 130/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6427 - mse: 0.6427 - val_loss: 0.7351 - val_mse: 0.7351\n",
      "Epoch 131/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6419 - mse: 0.6419 - val_loss: 0.7378 - val_mse: 0.7378\n",
      "Epoch 132/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6413 - mse: 0.6413 - val_loss: 0.7487 - val_mse: 0.7487\n",
      "Epoch 133/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6408 - mse: 0.6408 - val_loss: 0.7261 - val_mse: 0.7261\n",
      "Epoch 134/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6406 - mse: 0.6406 - val_loss: 0.7293 - val_mse: 0.7293\n",
      "Epoch 135/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6400 - mse: 0.6400 - val_loss: 0.7348 - val_mse: 0.7348\n",
      "Epoch 136/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6397 - mse: 0.6397 - val_loss: 0.7294 - val_mse: 0.7294\n",
      "Epoch 137/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6395 - mse: 0.6395 - val_loss: 0.7350 - val_mse: 0.7350\n",
      "Epoch 138/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6390 - mse: 0.6390 - val_loss: 0.7322 - val_mse: 0.7322\n",
      "Epoch 139/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6384 - mse: 0.6384 - val_loss: 0.7459 - val_mse: 0.7459\n",
      "Epoch 140/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6376 - mse: 0.6376 - val_loss: 0.7381 - val_mse: 0.7381\n",
      "Epoch 141/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6376 - mse: 0.6376 - val_loss: 0.7284 - val_mse: 0.7284\n",
      "Epoch 142/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6370 - mse: 0.6370 - val_loss: 0.7400 - val_mse: 0.7400\n",
      "Epoch 143/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6366 - mse: 0.6366 - val_loss: 0.7359 - val_mse: 0.7359\n",
      "Epoch 144/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6359 - mse: 0.6359 - val_loss: 0.7390 - val_mse: 0.7390\n",
      "Epoch 145/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6356 - mse: 0.6356 - val_loss: 0.7334 - val_mse: 0.7334\n",
      "Epoch 146/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6352 - mse: 0.6352 - val_loss: 0.7379 - val_mse: 0.7379\n",
      "Epoch 147/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6349 - mse: 0.6349 - val_loss: 0.7437 - val_mse: 0.7437\n",
      "Epoch 148/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6343 - mse: 0.6343 - val_loss: 0.7329 - val_mse: 0.7329\n",
      "Epoch 149/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6343 - mse: 0.6343 - val_loss: 0.7379 - val_mse: 0.7379\n",
      "Epoch 150/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6334 - mse: 0.6334 - val_loss: 0.7245 - val_mse: 0.7245\n",
      "Epoch 151/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6332 - mse: 0.6332 - val_loss: 0.7335 - val_mse: 0.7335\n",
      "Epoch 152/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6328 - mse: 0.6328 - val_loss: 0.7410 - val_mse: 0.7410\n",
      "Epoch 153/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6321 - mse: 0.6321 - val_loss: 0.7521 - val_mse: 0.7521\n",
      "Epoch 154/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6320 - mse: 0.6320 - val_loss: 0.7510 - val_mse: 0.7510\n",
      "Epoch 155/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6315 - mse: 0.6315 - val_loss: 0.7502 - val_mse: 0.7502\n",
      "Epoch 156/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6309 - mse: 0.6309 - val_loss: 0.7419 - val_mse: 0.7419\n",
      "Epoch 157/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6305 - mse: 0.6305 - val_loss: 0.7605 - val_mse: 0.7605\n",
      "Epoch 158/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6306 - mse: 0.6306 - val_loss: 0.7426 - val_mse: 0.7426\n",
      "Epoch 159/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6295 - mse: 0.6295 - val_loss: 0.7485 - val_mse: 0.7485\n",
      "Epoch 160/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6298 - mse: 0.6298 - val_loss: 0.7491 - val_mse: 0.7491\n",
      "Epoch 161/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6293 - mse: 0.6293 - val_loss: 0.7628 - val_mse: 0.7628\n",
      "Epoch 162/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6288 - mse: 0.6288 - val_loss: 0.7691 - val_mse: 0.7691\n",
      "Epoch 163/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6282 - mse: 0.6282 - val_loss: 0.7381 - val_mse: 0.7381\n",
      "Epoch 164/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6281 - mse: 0.6281 - val_loss: 0.7626 - val_mse: 0.7626\n",
      "Epoch 165/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6280 - mse: 0.6280 - val_loss: 0.7679 - val_mse: 0.7679\n",
      "Epoch 166/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6272 - mse: 0.6272 - val_loss: 0.7651 - val_mse: 0.7651\n",
      "Epoch 167/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6269 - mse: 0.6269 - val_loss: 0.7544 - val_mse: 0.7544\n",
      "Epoch 168/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6269 - mse: 0.6269 - val_loss: 0.7892 - val_mse: 0.7891\n",
      "Epoch 169/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6262 - mse: 0.6262 - val_loss: 0.7653 - val_mse: 0.7653\n",
      "Epoch 170/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6259 - mse: 0.6259 - val_loss: 0.7617 - val_mse: 0.7617\n",
      "Epoch 171/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6255 - mse: 0.6255 - val_loss: 0.7751 - val_mse: 0.7751\n",
      "Epoch 172/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6253 - mse: 0.6253 - val_loss: 0.7889 - val_mse: 0.7889\n",
      "Epoch 173/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6252 - mse: 0.6252 - val_loss: 0.7631 - val_mse: 0.7631\n",
      "Epoch 174/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6242 - mse: 0.6242 - val_loss: 0.7800 - val_mse: 0.7800\n",
      "Epoch 175/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6243 - mse: 0.6243 - val_loss: 0.7903 - val_mse: 0.7903\n",
      "Epoch 176/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6236 - mse: 0.6236 - val_loss: 0.7730 - val_mse: 0.7730\n",
      "Epoch 177/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6234 - mse: 0.6234 - val_loss: 0.7740 - val_mse: 0.7740\n",
      "Epoch 178/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6229 - mse: 0.6229 - val_loss: 0.7693 - val_mse: 0.7693\n",
      "Epoch 179/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6239 - mse: 0.6239 - val_loss: 0.7642 - val_mse: 0.7642\n",
      "Epoch 180/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6229 - mse: 0.6229 - val_loss: 0.7691 - val_mse: 0.7691\n",
      "Epoch 181/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6224 - mse: 0.6224 - val_loss: 0.7863 - val_mse: 0.7863\n",
      "Epoch 182/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6217 - mse: 0.6217 - val_loss: 0.7745 - val_mse: 0.7745\n",
      "Epoch 183/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6209 - mse: 0.6209 - val_loss: 0.7903 - val_mse: 0.7903\n",
      "Epoch 184/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6212 - mse: 0.6212 - val_loss: 0.7766 - val_mse: 0.7766\n",
      "Epoch 185/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6211 - mse: 0.6211 - val_loss: 0.7764 - val_mse: 0.7764\n",
      "Epoch 186/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6209 - mse: 0.6209 - val_loss: 0.7955 - val_mse: 0.7955\n",
      "Epoch 187/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6203 - mse: 0.6203 - val_loss: 0.8307 - val_mse: 0.8307\n",
      "Epoch 188/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6200 - mse: 0.6200 - val_loss: 0.7920 - val_mse: 0.7920\n",
      "Epoch 189/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6194 - mse: 0.6194 - val_loss: 0.7809 - val_mse: 0.7809\n",
      "Epoch 190/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6197 - mse: 0.6197 - val_loss: 0.7970 - val_mse: 0.7970\n",
      "Epoch 191/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6192 - mse: 0.6192 - val_loss: 0.7742 - val_mse: 0.7742\n",
      "Epoch 192/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6187 - mse: 0.6187 - val_loss: 0.7856 - val_mse: 0.7856\n",
      "Epoch 193/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6183 - mse: 0.6183 - val_loss: 0.7976 - val_mse: 0.7976\n",
      "Epoch 194/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6182 - mse: 0.6182 - val_loss: 0.8052 - val_mse: 0.8052\n",
      "Epoch 195/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6183 - mse: 0.6183 - val_loss: 0.8056 - val_mse: 0.8056\n",
      "Epoch 196/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6176 - mse: 0.6176 - val_loss: 0.8005 - val_mse: 0.8005\n",
      "Epoch 197/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6171 - mse: 0.6171 - val_loss: 0.8060 - val_mse: 0.8060\n",
      "Epoch 198/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6175 - mse: 0.6175 - val_loss: 0.7806 - val_mse: 0.7806\n",
      "Epoch 199/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6168 - mse: 0.6168 - val_loss: 0.7948 - val_mse: 0.7948\n",
      "Epoch 200/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6164 - mse: 0.6164 - val_loss: 0.8043 - val_mse: 0.8043\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.9460 - mse: 0.9460 - val_loss: 0.7891 - val_mse: 0.7891\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.8405 - mse: 0.8405 - val_loss: 0.7751 - val_mse: 0.7751\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.8221 - mse: 0.8221 - val_loss: 0.7737 - val_mse: 0.7737\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.8096 - mse: 0.8096 - val_loss: 0.7646 - val_mse: 0.7646\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.7986 - mse: 0.7986 - val_loss: 0.7651 - val_mse: 0.7651\n",
      "1676659/1676659 [==============================] - 3s 2us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8476 - mse: 0.8476 - val_loss: 0.7729 - val_mse: 0.7729\n",
      "Epoch 2/5\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7417 - mse: 0.7417 - val_loss: 0.7701 - val_mse: 0.7701\n",
      "Epoch 3/5\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7286 - mse: 0.7286 - val_loss: 0.7661 - val_mse: 0.7661\n",
      "Epoch 4/5\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7199 - mse: 0.7199 - val_loss: 0.7680 - val_mse: 0.7680\n",
      "Epoch 5/5\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7126 - mse: 0.7126 - val_loss: 0.7562 - val_mse: 0.7562\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/5\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.9902 - mse: 0.9902 - val_loss: 0.8092 - val_mse: 0.8092\n",
      "Epoch 2/5\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8567 - mse: 0.8567 - val_loss: 0.8206 - val_mse: 0.8206\n",
      "Epoch 3/5\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8362 - mse: 0.8362 - val_loss: 0.8646 - val_mse: 0.8646\n",
      "Epoch 4/5\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8232 - mse: 0.8232 - val_loss: 0.8186 - val_mse: 0.8186\n",
      "Epoch 5/5\n",
      "3353318/3353318 [==============================] - 8s 3us/step - loss: 0.8129 - mse: 0.8129 - val_loss: 0.8349 - val_mse: 0.8349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1676658/1676658 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.9574 - mse: 0.9574 - val_loss: 0.7741 - val_mse: 0.7741\n",
      "Epoch 2/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8439 - mse: 0.8439 - val_loss: 0.7900 - val_mse: 0.7900\n",
      "Epoch 3/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8256 - mse: 0.8256 - val_loss: 0.7793 - val_mse: 0.7793\n",
      "Epoch 4/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8126 - mse: 0.8126 - val_loss: 0.7641 - val_mse: 0.7641\n",
      "Epoch 5/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8017 - mse: 0.8017 - val_loss: 0.7811 - val_mse: 0.7811\n",
      "Epoch 6/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7916 - mse: 0.7916 - val_loss: 0.7625 - val_mse: 0.7625\n",
      "Epoch 7/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7821 - mse: 0.7821 - val_loss: 0.7536 - val_mse: 0.7536\n",
      "Epoch 8/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7736 - mse: 0.7736 - val_loss: 0.7511 - val_mse: 0.7511\n",
      "Epoch 9/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7658 - mse: 0.7658 - val_loss: 0.7587 - val_mse: 0.7587\n",
      "Epoch 10/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7589 - mse: 0.7589 - val_loss: 0.7637 - val_mse: 0.7637\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8408 - mse: 0.8408 - val_loss: 0.7760 - val_mse: 0.7760\n",
      "Epoch 2/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7420 - mse: 0.7420 - val_loss: 0.7705 - val_mse: 0.7705\n",
      "Epoch 3/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7295 - mse: 0.7295 - val_loss: 0.7660 - val_mse: 0.7660\n",
      "Epoch 4/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7207 - mse: 0.7207 - val_loss: 0.7630 - val_mse: 0.7630\n",
      "Epoch 5/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7137 - mse: 0.7137 - val_loss: 0.7619 - val_mse: 0.7619\n",
      "Epoch 6/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7073 - mse: 0.7073 - val_loss: 0.7576 - val_mse: 0.7576\n",
      "Epoch 7/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7012 - mse: 0.7012 - val_loss: 0.7548 - val_mse: 0.7548\n",
      "Epoch 8/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6958 - mse: 0.6958 - val_loss: 0.7528 - val_mse: 0.7528\n",
      "Epoch 9/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6912 - mse: 0.6912 - val_loss: 0.7496 - val_mse: 0.7496\n",
      "Epoch 10/10\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6862 - mse: 0.6862 - val_loss: 0.7466 - val_mse: 0.7466\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/10\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.9681 - mse: 0.9681 - val_loss: 0.8175 - val_mse: 0.8175\n",
      "Epoch 2/10\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8549 - mse: 0.8549 - val_loss: 0.8416 - val_mse: 0.8416\n",
      "Epoch 3/10\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8357 - mse: 0.8357 - val_loss: 0.8290 - val_mse: 0.8290\n",
      "Epoch 4/10\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8227 - mse: 0.8227 - val_loss: 0.8335 - val_mse: 0.8335\n",
      "Epoch 5/10\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8126 - mse: 0.8126 - val_loss: 0.8239 - val_mse: 0.8239\n",
      "Epoch 6/10\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8040 - mse: 0.8040 - val_loss: 0.8082 - val_mse: 0.8082\n",
      "Epoch 7/10\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7962 - mse: 0.7962 - val_loss: 0.8060 - val_mse: 0.8060\n",
      "Epoch 8/10\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7897 - mse: 0.7897 - val_loss: 0.8161 - val_mse: 0.8161\n",
      "Epoch 9/10\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7837 - mse: 0.7837 - val_loss: 0.7994 - val_mse: 0.7994\n",
      "Epoch 10/10\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7784 - mse: 0.7784 - val_loss: 0.7916 - val_mse: 0.7916\n",
      "1676658/1676658 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.9600 - mse: 0.9600 - val_loss: 0.7855 - val_mse: 0.7855\n",
      "Epoch 2/30\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.8431 - mse: 0.8431 - val_loss: 0.7748 - val_mse: 0.7748\n",
      "Epoch 3/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8239 - mse: 0.8239 - val_loss: 0.7716 - val_mse: 0.7716\n",
      "Epoch 4/30\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.8108 - mse: 0.8108 - val_loss: 0.7661 - val_mse: 0.7661\n",
      "Epoch 5/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8002 - mse: 0.8002 - val_loss: 0.7627 - val_mse: 0.7627\n",
      "Epoch 6/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7906 - mse: 0.7906 - val_loss: 0.7564 - val_mse: 0.7564\n",
      "Epoch 7/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7822 - mse: 0.7822 - val_loss: 0.7575 - val_mse: 0.7575\n",
      "Epoch 8/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7747 - mse: 0.7747 - val_loss: 0.7584 - val_mse: 0.7584\n",
      "Epoch 9/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7671 - mse: 0.7671 - val_loss: 0.7522 - val_mse: 0.7522\n",
      "Epoch 10/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7600 - mse: 0.7600 - val_loss: 0.7434 - val_mse: 0.7434\n",
      "Epoch 11/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7551 - mse: 0.7551 - val_loss: 0.7434 - val_mse: 0.7434\n",
      "Epoch 12/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7496 - mse: 0.7496 - val_loss: 0.7433 - val_mse: 0.7433\n",
      "Epoch 13/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7450 - mse: 0.7450 - val_loss: 0.7353 - val_mse: 0.7353\n",
      "Epoch 14/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7406 - mse: 0.7406 - val_loss: 0.7381 - val_mse: 0.7381\n",
      "Epoch 15/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7367 - mse: 0.7367 - val_loss: 0.7311 - val_mse: 0.7311\n",
      "Epoch 16/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7335 - mse: 0.7335 - val_loss: 0.7283 - val_mse: 0.7283\n",
      "Epoch 17/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7304 - mse: 0.7304 - val_loss: 0.7315 - val_mse: 0.7315\n",
      "Epoch 18/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7284 - mse: 0.7284 - val_loss: 0.7338 - val_mse: 0.7338\n",
      "Epoch 19/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7252 - mse: 0.7252 - val_loss: 0.7192 - val_mse: 0.7192\n",
      "Epoch 20/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7239 - mse: 0.7239 - val_loss: 0.7249 - val_mse: 0.7249\n",
      "Epoch 21/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7210 - mse: 0.7210 - val_loss: 0.7267 - val_mse: 0.7267\n",
      "Epoch 22/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7190 - mse: 0.7190 - val_loss: 0.7211 - val_mse: 0.7211\n",
      "Epoch 23/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7171 - mse: 0.7171 - val_loss: 0.7217 - val_mse: 0.7217\n",
      "Epoch 24/30\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.7153 - mse: 0.7153 - val_loss: 0.7184 - val_mse: 0.7184\n",
      "Epoch 25/30\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.7133 - mse: 0.7133 - val_loss: 0.7323 - val_mse: 0.7323\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.7116 - mse: 0.7116 - val_loss: 0.7145 - val_mse: 0.7145\n",
      "Epoch 27/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7101 - mse: 0.7101 - val_loss: 0.7206 - val_mse: 0.7206\n",
      "Epoch 28/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7091 - mse: 0.7091 - val_loss: 0.7403 - val_mse: 0.7403\n",
      "Epoch 29/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7076 - mse: 0.7076 - val_loss: 0.7123 - val_mse: 0.7123\n",
      "Epoch 30/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7065 - mse: 0.7065 - val_loss: 0.7084 - val_mse: 0.7084\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8458 - mse: 0.8458 - val_loss: 0.7763 - val_mse: 0.7763\n",
      "Epoch 2/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7422 - mse: 0.7422 - val_loss: 0.7711 - val_mse: 0.7711\n",
      "Epoch 3/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7289 - mse: 0.7289 - val_loss: 0.7667 - val_mse: 0.7667\n",
      "Epoch 4/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7206 - mse: 0.7206 - val_loss: 0.7623 - val_mse: 0.7623\n",
      "Epoch 5/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7132 - mse: 0.7132 - val_loss: 0.7616 - val_mse: 0.7616\n",
      "Epoch 6/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7064 - mse: 0.7064 - val_loss: 0.7567 - val_mse: 0.7567\n",
      "Epoch 7/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7007 - mse: 0.7007 - val_loss: 0.7535 - val_mse: 0.7535\n",
      "Epoch 8/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6951 - mse: 0.6951 - val_loss: 0.7543 - val_mse: 0.7543\n",
      "Epoch 9/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6901 - mse: 0.6901 - val_loss: 0.7499 - val_mse: 0.7499\n",
      "Epoch 10/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6856 - mse: 0.6856 - val_loss: 0.7466 - val_mse: 0.7466\n",
      "Epoch 11/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6815 - mse: 0.6815 - val_loss: 0.7416 - val_mse: 0.7416\n",
      "Epoch 12/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6775 - mse: 0.6775 - val_loss: 0.7447 - val_mse: 0.7447\n",
      "Epoch 13/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6748 - mse: 0.6748 - val_loss: 0.7397 - val_mse: 0.7397\n",
      "Epoch 14/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6715 - mse: 0.6715 - val_loss: 0.7353 - val_mse: 0.7353\n",
      "Epoch 15/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6687 - mse: 0.6687 - val_loss: 0.7394 - val_mse: 0.7394\n",
      "Epoch 16/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6667 - mse: 0.6667 - val_loss: 0.7324 - val_mse: 0.7324\n",
      "Epoch 17/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6640 - mse: 0.6640 - val_loss: 0.7345 - val_mse: 0.7345\n",
      "Epoch 18/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6619 - mse: 0.6619 - val_loss: 0.7302 - val_mse: 0.7302\n",
      "Epoch 19/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6606 - mse: 0.6606 - val_loss: 0.7353 - val_mse: 0.7353\n",
      "Epoch 20/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6583 - mse: 0.6583 - val_loss: 0.7347 - val_mse: 0.7347\n",
      "Epoch 21/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6569 - mse: 0.6569 - val_loss: 0.7283 - val_mse: 0.7283\n",
      "Epoch 22/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6549 - mse: 0.6549 - val_loss: 0.7294 - val_mse: 0.7294\n",
      "Epoch 23/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6534 - mse: 0.6534 - val_loss: 0.7247 - val_mse: 0.7247\n",
      "Epoch 24/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6522 - mse: 0.6522 - val_loss: 0.7280 - val_mse: 0.7280\n",
      "Epoch 25/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6509 - mse: 0.6509 - val_loss: 0.7242 - val_mse: 0.7242\n",
      "Epoch 26/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6493 - mse: 0.6493 - val_loss: 0.7392 - val_mse: 0.7392\n",
      "Epoch 27/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6486 - mse: 0.6486 - val_loss: 0.7228 - val_mse: 0.7228\n",
      "Epoch 28/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6475 - mse: 0.6475 - val_loss: 0.7202 - val_mse: 0.7202\n",
      "Epoch 29/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6459 - mse: 0.6459 - val_loss: 0.7193 - val_mse: 0.7193\n",
      "Epoch 30/30\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6449 - mse: 0.6449 - val_loss: 0.7165 - val_mse: 0.7165\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.9939 - mse: 0.9939 - val_loss: 0.7977 - val_mse: 0.7977\n",
      "Epoch 2/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8567 - mse: 0.8567 - val_loss: 0.8143 - val_mse: 0.8143\n",
      "Epoch 3/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8380 - mse: 0.8380 - val_loss: 0.8250 - val_mse: 0.8250\n",
      "Epoch 4/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8258 - mse: 0.8258 - val_loss: 0.8234 - val_mse: 0.8234\n",
      "Epoch 5/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8154 - mse: 0.8154 - val_loss: 0.8146 - val_mse: 0.8146\n",
      "Epoch 6/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8066 - mse: 0.8066 - val_loss: 0.8160 - val_mse: 0.8160\n",
      "Epoch 7/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7985 - mse: 0.7985 - val_loss: 0.8031 - val_mse: 0.8031\n",
      "Epoch 8/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7915 - mse: 0.7915 - val_loss: 0.7928 - val_mse: 0.7928\n",
      "Epoch 9/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7852 - mse: 0.7852 - val_loss: 0.7943 - val_mse: 0.7943\n",
      "Epoch 10/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7797 - mse: 0.7797 - val_loss: 0.7848 - val_mse: 0.7848\n",
      "Epoch 11/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7755 - mse: 0.7755 - val_loss: 0.8011 - val_mse: 0.8011\n",
      "Epoch 12/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7710 - mse: 0.7710 - val_loss: 0.7786 - val_mse: 0.7786\n",
      "Epoch 13/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7675 - mse: 0.7675 - val_loss: 0.7750 - val_mse: 0.7750\n",
      "Epoch 14/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7644 - mse: 0.7644 - val_loss: 0.7721 - val_mse: 0.7721\n",
      "Epoch 15/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7613 - mse: 0.7613 - val_loss: 0.7718 - val_mse: 0.7718\n",
      "Epoch 16/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7589 - mse: 0.7589 - val_loss: 0.7676 - val_mse: 0.7676\n",
      "Epoch 17/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7562 - mse: 0.7562 - val_loss: 0.7607 - val_mse: 0.7607\n",
      "Epoch 18/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7536 - mse: 0.7536 - val_loss: 0.7630 - val_mse: 0.7630\n",
      "Epoch 19/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7523 - mse: 0.7523 - val_loss: 0.7668 - val_mse: 0.7668\n",
      "Epoch 20/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7502 - mse: 0.7502 - val_loss: 0.7727 - val_mse: 0.7727\n",
      "Epoch 21/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7485 - mse: 0.7485 - val_loss: 0.7580 - val_mse: 0.7580\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7464 - mse: 0.7464 - val_loss: 0.7625 - val_mse: 0.7625\n",
      "Epoch 23/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7449 - mse: 0.7449 - val_loss: 0.7551 - val_mse: 0.7551\n",
      "Epoch 24/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7437 - mse: 0.7437 - val_loss: 0.7549 - val_mse: 0.7549\n",
      "Epoch 25/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7420 - mse: 0.7420 - val_loss: 0.7532 - val_mse: 0.7532\n",
      "Epoch 26/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7406 - mse: 0.7406 - val_loss: 0.7651 - val_mse: 0.7651\n",
      "Epoch 27/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7397 - mse: 0.7397 - val_loss: 0.7590 - val_mse: 0.7590\n",
      "Epoch 28/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7378 - mse: 0.7378 - val_loss: 0.7596 - val_mse: 0.7596\n",
      "Epoch 29/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7367 - mse: 0.7367 - val_loss: 0.7447 - val_mse: 0.7447\n",
      "Epoch 30/30\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7357 - mse: 0.7357 - val_loss: 0.7472 - val_mse: 0.7472\n",
      "1676658/1676658 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.9487 - mse: 0.9487 - val_loss: 0.7788 - val_mse: 0.7788\n",
      "Epoch 2/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8432 - mse: 0.8432 - val_loss: 0.7744 - val_mse: 0.7744\n",
      "Epoch 3/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8246 - mse: 0.8246 - val_loss: 0.7673 - val_mse: 0.7673\n",
      "Epoch 4/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8112 - mse: 0.8112 - val_loss: 0.7734 - val_mse: 0.7734\n",
      "Epoch 5/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8006 - mse: 0.8006 - val_loss: 0.7616 - val_mse: 0.7616\n",
      "Epoch 6/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7910 - mse: 0.7910 - val_loss: 0.7619 - val_mse: 0.7619\n",
      "Epoch 7/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7821 - mse: 0.7821 - val_loss: 0.7554 - val_mse: 0.7554\n",
      "Epoch 8/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7742 - mse: 0.7742 - val_loss: 0.7601 - val_mse: 0.7601\n",
      "Epoch 9/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7669 - mse: 0.7669 - val_loss: 0.7512 - val_mse: 0.7512\n",
      "Epoch 10/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7608 - mse: 0.7608 - val_loss: 0.7483 - val_mse: 0.7483\n",
      "Epoch 11/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7546 - mse: 0.7546 - val_loss: 0.7411 - val_mse: 0.7411\n",
      "Epoch 12/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7498 - mse: 0.7498 - val_loss: 0.7433 - val_mse: 0.7433\n",
      "Epoch 13/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7453 - mse: 0.7453 - val_loss: 0.7513 - val_mse: 0.7513\n",
      "Epoch 14/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7412 - mse: 0.7412 - val_loss: 0.7364 - val_mse: 0.7364\n",
      "Epoch 15/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7374 - mse: 0.7374 - val_loss: 0.7368 - val_mse: 0.7368\n",
      "Epoch 16/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7342 - mse: 0.7342 - val_loss: 0.7316 - val_mse: 0.7316\n",
      "Epoch 17/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7311 - mse: 0.7311 - val_loss: 0.7376 - val_mse: 0.7376\n",
      "Epoch 18/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7285 - mse: 0.7285 - val_loss: 0.7288 - val_mse: 0.7288\n",
      "Epoch 19/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7258 - mse: 0.7258 - val_loss: 0.7253 - val_mse: 0.7253\n",
      "Epoch 20/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7244 - mse: 0.7244 - val_loss: 0.7217 - val_mse: 0.7217\n",
      "Epoch 21/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7221 - mse: 0.7221 - val_loss: 0.7258 - val_mse: 0.7258\n",
      "Epoch 22/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7199 - mse: 0.7199 - val_loss: 0.7190 - val_mse: 0.7190\n",
      "Epoch 23/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7180 - mse: 0.7180 - val_loss: 0.7175 - val_mse: 0.7175\n",
      "Epoch 24/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7166 - mse: 0.7166 - val_loss: 0.7249 - val_mse: 0.7249\n",
      "Epoch 25/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7150 - mse: 0.7150 - val_loss: 0.7312 - val_mse: 0.7312\n",
      "Epoch 26/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7132 - mse: 0.7132 - val_loss: 0.7262 - val_mse: 0.7262\n",
      "Epoch 27/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7114 - mse: 0.7114 - val_loss: 0.7232 - val_mse: 0.7232\n",
      "Epoch 28/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7104 - mse: 0.7104 - val_loss: 0.7220 - val_mse: 0.7220\n",
      "Epoch 29/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7096 - mse: 0.7096 - val_loss: 0.7349 - val_mse: 0.7349\n",
      "Epoch 30/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7081 - mse: 0.7081 - val_loss: 0.7104 - val_mse: 0.7104\n",
      "Epoch 31/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7063 - mse: 0.7063 - val_loss: 0.7153 - val_mse: 0.7153\n",
      "Epoch 32/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7046 - mse: 0.7046 - val_loss: 0.7108 - val_mse: 0.7108\n",
      "Epoch 33/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7042 - mse: 0.7042 - val_loss: 0.7078 - val_mse: 0.7078\n",
      "Epoch 34/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7022 - mse: 0.7022 - val_loss: 0.7055 - val_mse: 0.7055\n",
      "Epoch 35/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7011 - mse: 0.7011 - val_loss: 0.7114 - val_mse: 0.7114\n",
      "Epoch 36/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7000 - mse: 0.7000 - val_loss: 0.7162 - val_mse: 0.7162\n",
      "Epoch 37/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6991 - mse: 0.6991 - val_loss: 0.7030 - val_mse: 0.7030\n",
      "Epoch 38/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6975 - mse: 0.6975 - val_loss: 0.7025 - val_mse: 0.7025\n",
      "Epoch 39/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6969 - mse: 0.6969 - val_loss: 0.7005 - val_mse: 0.7005\n",
      "Epoch 40/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6956 - mse: 0.6956 - val_loss: 0.7007 - val_mse: 0.7007\n",
      "Epoch 41/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6943 - mse: 0.6943 - val_loss: 0.7162 - val_mse: 0.7162\n",
      "Epoch 42/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6931 - mse: 0.6931 - val_loss: 0.7000 - val_mse: 0.7000\n",
      "Epoch 43/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6925 - mse: 0.6925 - val_loss: 0.6987 - val_mse: 0.6987\n",
      "Epoch 44/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6915 - mse: 0.6915 - val_loss: 0.6978 - val_mse: 0.6978\n",
      "Epoch 45/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6901 - mse: 0.6901 - val_loss: 0.7085 - val_mse: 0.7085\n",
      "Epoch 46/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6893 - mse: 0.6893 - val_loss: 0.7026 - val_mse: 0.7026\n",
      "Epoch 47/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6880 - mse: 0.6880 - val_loss: 0.7000 - val_mse: 0.7000\n",
      "Epoch 48/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6878 - mse: 0.6878 - val_loss: 0.6982 - val_mse: 0.6982\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6859 - mse: 0.6859 - val_loss: 0.6962 - val_mse: 0.6962\n",
      "Epoch 50/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6854 - mse: 0.6854 - val_loss: 0.7105 - val_mse: 0.7105\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8414 - mse: 0.8414 - val_loss: 0.7746 - val_mse: 0.7746\n",
      "Epoch 2/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7403 - mse: 0.7403 - val_loss: 0.7682 - val_mse: 0.7682\n",
      "Epoch 3/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7274 - mse: 0.7274 - val_loss: 0.7663 - val_mse: 0.7663\n",
      "Epoch 4/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7186 - mse: 0.7186 - val_loss: 0.7612 - val_mse: 0.7612\n",
      "Epoch 5/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7117 - mse: 0.7117 - val_loss: 0.7601 - val_mse: 0.7601\n",
      "Epoch 6/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7056 - mse: 0.7056 - val_loss: 0.7584 - val_mse: 0.7584\n",
      "Epoch 7/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6993 - mse: 0.6993 - val_loss: 0.7552 - val_mse: 0.7552\n",
      "Epoch 8/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6945 - mse: 0.6945 - val_loss: 0.7517 - val_mse: 0.7517\n",
      "Epoch 9/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6897 - mse: 0.6897 - val_loss: 0.7503 - val_mse: 0.7503\n",
      "Epoch 10/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6849 - mse: 0.6849 - val_loss: 0.7491 - val_mse: 0.7491\n",
      "Epoch 11/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6816 - mse: 0.6816 - val_loss: 0.7479 - val_mse: 0.7479\n",
      "Epoch 12/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6776 - mse: 0.6776 - val_loss: 0.7463 - val_mse: 0.7463\n",
      "Epoch 13/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6749 - mse: 0.6749 - val_loss: 0.7475 - val_mse: 0.7475\n",
      "Epoch 14/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6725 - mse: 0.6725 - val_loss: 0.7388 - val_mse: 0.7388\n",
      "Epoch 15/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6690 - mse: 0.6690 - val_loss: 0.7429 - val_mse: 0.7429\n",
      "Epoch 16/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6671 - mse: 0.6671 - val_loss: 0.7383 - val_mse: 0.7383\n",
      "Epoch 17/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6651 - mse: 0.6651 - val_loss: 0.7384 - val_mse: 0.7384\n",
      "Epoch 18/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6629 - mse: 0.6629 - val_loss: 0.7321 - val_mse: 0.7321\n",
      "Epoch 19/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6608 - mse: 0.6608 - val_loss: 0.7323 - val_mse: 0.7323\n",
      "Epoch 20/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6588 - mse: 0.6588 - val_loss: 0.7343 - val_mse: 0.7343\n",
      "Epoch 21/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6574 - mse: 0.6574 - val_loss: 0.7277 - val_mse: 0.7277\n",
      "Epoch 22/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6564 - mse: 0.6564 - val_loss: 0.7262 - val_mse: 0.7262\n",
      "Epoch 23/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6540 - mse: 0.6540 - val_loss: 0.7290 - val_mse: 0.7290\n",
      "Epoch 24/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6529 - mse: 0.6529 - val_loss: 0.7258 - val_mse: 0.7258\n",
      "Epoch 25/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6516 - mse: 0.6516 - val_loss: 0.7265 - val_mse: 0.7265\n",
      "Epoch 26/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6507 - mse: 0.6507 - val_loss: 0.7200 - val_mse: 0.7200\n",
      "Epoch 27/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6492 - mse: 0.6492 - val_loss: 0.7214 - val_mse: 0.7214\n",
      "Epoch 28/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6480 - mse: 0.6480 - val_loss: 0.7178 - val_mse: 0.7178\n",
      "Epoch 29/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6469 - mse: 0.6469 - val_loss: 0.7185 - val_mse: 0.7185\n",
      "Epoch 30/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6456 - mse: 0.6456 - val_loss: 0.7158 - val_mse: 0.7158\n",
      "Epoch 31/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6441 - mse: 0.6441 - val_loss: 0.7178 - val_mse: 0.7178\n",
      "Epoch 32/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6434 - mse: 0.6434 - val_loss: 0.7175 - val_mse: 0.7175\n",
      "Epoch 33/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6423 - mse: 0.6423 - val_loss: 0.7180 - val_mse: 0.7180\n",
      "Epoch 34/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6409 - mse: 0.6409 - val_loss: 0.7167 - val_mse: 0.7167\n",
      "Epoch 35/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6403 - mse: 0.6403 - val_loss: 0.7325 - val_mse: 0.7325\n",
      "Epoch 36/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6395 - mse: 0.6395 - val_loss: 0.7138 - val_mse: 0.7138\n",
      "Epoch 37/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6382 - mse: 0.6382 - val_loss: 0.7103 - val_mse: 0.7103\n",
      "Epoch 38/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6375 - mse: 0.6375 - val_loss: 0.7120 - val_mse: 0.7120\n",
      "Epoch 39/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6363 - mse: 0.6363 - val_loss: 0.7266 - val_mse: 0.7266\n",
      "Epoch 40/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6357 - mse: 0.6357 - val_loss: 0.7110 - val_mse: 0.7110\n",
      "Epoch 41/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6344 - mse: 0.6344 - val_loss: 0.7081 - val_mse: 0.7081\n",
      "Epoch 42/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6335 - mse: 0.6335 - val_loss: 0.7106 - val_mse: 0.7106\n",
      "Epoch 43/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6327 - mse: 0.6327 - val_loss: 0.7133 - val_mse: 0.7133\n",
      "Epoch 44/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6313 - mse: 0.6313 - val_loss: 0.7066 - val_mse: 0.7066\n",
      "Epoch 45/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6308 - mse: 0.6308 - val_loss: 0.7153 - val_mse: 0.7153\n",
      "Epoch 46/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6300 - mse: 0.6300 - val_loss: 0.7103 - val_mse: 0.7103\n",
      "Epoch 47/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6291 - mse: 0.6291 - val_loss: 0.7107 - val_mse: 0.7107\n",
      "Epoch 48/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6283 - mse: 0.6283 - val_loss: 0.7071 - val_mse: 0.7071\n",
      "Epoch 49/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6280 - mse: 0.6280 - val_loss: 0.7119 - val_mse: 0.7119\n",
      "Epoch 50/50\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6266 - mse: 0.6266 - val_loss: 0.7046 - val_mse: 0.7046\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 1.0098 - mse: 1.0098 - val_loss: 0.7967 - val_mse: 0.7967\n",
      "Epoch 2/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8627 - mse: 0.8627 - val_loss: 0.8171 - val_mse: 0.8171\n",
      "Epoch 3/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8416 - mse: 0.8416 - val_loss: 0.8221 - val_mse: 0.8221\n",
      "Epoch 4/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8272 - mse: 0.8272 - val_loss: 0.8285 - val_mse: 0.8285\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8168 - mse: 0.8168 - val_loss: 0.8303 - val_mse: 0.8303\n",
      "Epoch 6/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8078 - mse: 0.8078 - val_loss: 0.8178 - val_mse: 0.8178\n",
      "Epoch 7/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8002 - mse: 0.8002 - val_loss: 0.8118 - val_mse: 0.8118\n",
      "Epoch 8/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7931 - mse: 0.7931 - val_loss: 0.8284 - val_mse: 0.8284\n",
      "Epoch 9/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7876 - mse: 0.7876 - val_loss: 0.8075 - val_mse: 0.8075\n",
      "Epoch 10/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7821 - mse: 0.7821 - val_loss: 0.8042 - val_mse: 0.8042\n",
      "Epoch 11/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7774 - mse: 0.7774 - val_loss: 0.7997 - val_mse: 0.7997\n",
      "Epoch 12/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7736 - mse: 0.7736 - val_loss: 0.7854 - val_mse: 0.7854\n",
      "Epoch 13/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7694 - mse: 0.7694 - val_loss: 0.7866 - val_mse: 0.7866\n",
      "Epoch 14/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7661 - mse: 0.7661 - val_loss: 0.7845 - val_mse: 0.7845\n",
      "Epoch 15/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7638 - mse: 0.7638 - val_loss: 0.7861 - val_mse: 0.7861\n",
      "Epoch 16/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7602 - mse: 0.7602 - val_loss: 0.7813 - val_mse: 0.7813\n",
      "Epoch 17/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7589 - mse: 0.7589 - val_loss: 0.7678 - val_mse: 0.7678\n",
      "Epoch 18/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7560 - mse: 0.7560 - val_loss: 0.7710 - val_mse: 0.7710\n",
      "Epoch 19/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7537 - mse: 0.7537 - val_loss: 0.7687 - val_mse: 0.7687\n",
      "Epoch 20/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7515 - mse: 0.7515 - val_loss: 0.7716 - val_mse: 0.7716\n",
      "Epoch 21/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7497 - mse: 0.7497 - val_loss: 0.7667 - val_mse: 0.7667\n",
      "Epoch 22/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7483 - mse: 0.7483 - val_loss: 0.7646 - val_mse: 0.7646\n",
      "Epoch 23/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7464 - mse: 0.7464 - val_loss: 0.7673 - val_mse: 0.7673\n",
      "Epoch 24/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7449 - mse: 0.7449 - val_loss: 0.7649 - val_mse: 0.7649\n",
      "Epoch 25/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7434 - mse: 0.7434 - val_loss: 0.7763 - val_mse: 0.7763\n",
      "Epoch 26/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7426 - mse: 0.7426 - val_loss: 0.7633 - val_mse: 0.7633\n",
      "Epoch 27/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7401 - mse: 0.7401 - val_loss: 0.7595 - val_mse: 0.7595\n",
      "Epoch 28/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7395 - mse: 0.7395 - val_loss: 0.7656 - val_mse: 0.7656\n",
      "Epoch 29/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7379 - mse: 0.7379 - val_loss: 0.7761 - val_mse: 0.7761\n",
      "Epoch 30/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7364 - mse: 0.7364 - val_loss: 0.7664 - val_mse: 0.7664\n",
      "Epoch 31/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7349 - mse: 0.7349 - val_loss: 0.7565 - val_mse: 0.7565\n",
      "Epoch 32/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7338 - mse: 0.7338 - val_loss: 0.7630 - val_mse: 0.7630\n",
      "Epoch 33/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7326 - mse: 0.7326 - val_loss: 0.7605 - val_mse: 0.7605\n",
      "Epoch 34/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7317 - mse: 0.7317 - val_loss: 0.7534 - val_mse: 0.7534\n",
      "Epoch 35/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7305 - mse: 0.7305 - val_loss: 0.7556 - val_mse: 0.7556\n",
      "Epoch 36/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7296 - mse: 0.7296 - val_loss: 0.7623 - val_mse: 0.7623\n",
      "Epoch 37/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7282 - mse: 0.7282 - val_loss: 0.7812 - val_mse: 0.7812\n",
      "Epoch 38/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7278 - mse: 0.7278 - val_loss: 0.7650 - val_mse: 0.7650\n",
      "Epoch 39/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7267 - mse: 0.7267 - val_loss: 0.7554 - val_mse: 0.7554\n",
      "Epoch 40/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7251 - mse: 0.7251 - val_loss: 0.7611 - val_mse: 0.7611\n",
      "Epoch 41/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7246 - mse: 0.7246 - val_loss: 0.7589 - val_mse: 0.7589\n",
      "Epoch 42/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7235 - mse: 0.7235 - val_loss: 0.7481 - val_mse: 0.7481\n",
      "Epoch 43/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7227 - mse: 0.7227 - val_loss: 0.7626 - val_mse: 0.7626\n",
      "Epoch 44/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7211 - mse: 0.7211 - val_loss: 0.7578 - val_mse: 0.7578\n",
      "Epoch 45/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7210 - mse: 0.7210 - val_loss: 0.7499 - val_mse: 0.7499\n",
      "Epoch 46/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7201 - mse: 0.7201 - val_loss: 0.7593 - val_mse: 0.7593\n",
      "Epoch 47/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7188 - mse: 0.7188 - val_loss: 0.7519 - val_mse: 0.7519\n",
      "Epoch 48/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7174 - mse: 0.7174 - val_loss: 0.7491 - val_mse: 0.7491\n",
      "Epoch 49/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7170 - mse: 0.7170 - val_loss: 0.7519 - val_mse: 0.7519\n",
      "Epoch 50/50\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7156 - mse: 0.7156 - val_loss: 0.7494 - val_mse: 0.7494\n",
      "1676658/1676658 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.9787 - mse: 0.9787 - val_loss: 0.7869 - val_mse: 0.7869\n",
      "Epoch 2/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8421 - mse: 0.8421 - val_loss: 0.7697 - val_mse: 0.7697\n",
      "Epoch 3/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8236 - mse: 0.8236 - val_loss: 0.7723 - val_mse: 0.7723\n",
      "Epoch 4/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8102 - mse: 0.8102 - val_loss: 0.7772 - val_mse: 0.7772\n",
      "Epoch 5/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8003 - mse: 0.8003 - val_loss: 0.7664 - val_mse: 0.7664\n",
      "Epoch 6/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7913 - mse: 0.7913 - val_loss: 0.7582 - val_mse: 0.7582\n",
      "Epoch 7/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7826 - mse: 0.7826 - val_loss: 0.7554 - val_mse: 0.7554\n",
      "Epoch 8/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7743 - mse: 0.7743 - val_loss: 0.7549 - val_mse: 0.7549\n",
      "Epoch 9/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7672 - mse: 0.7672 - val_loss: 0.7522 - val_mse: 0.7522\n",
      "Epoch 10/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7614 - mse: 0.7614 - val_loss: 0.7428 - val_mse: 0.7428\n",
      "Epoch 11/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7548 - mse: 0.7548 - val_loss: 0.7438 - val_mse: 0.7438\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7503 - mse: 0.7503 - val_loss: 0.7401 - val_mse: 0.7401\n",
      "Epoch 13/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7454 - mse: 0.7454 - val_loss: 0.7325 - val_mse: 0.7325\n",
      "Epoch 14/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7412 - mse: 0.7412 - val_loss: 0.7385 - val_mse: 0.7385\n",
      "Epoch 15/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7384 - mse: 0.7384 - val_loss: 0.7325 - val_mse: 0.7325\n",
      "Epoch 16/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7344 - mse: 0.7344 - val_loss: 0.7425 - val_mse: 0.7425\n",
      "Epoch 17/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7316 - mse: 0.7316 - val_loss: 0.7299 - val_mse: 0.7299\n",
      "Epoch 18/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7289 - mse: 0.7289 - val_loss: 0.7304 - val_mse: 0.7303\n",
      "Epoch 19/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7264 - mse: 0.7264 - val_loss: 0.7270 - val_mse: 0.7270\n",
      "Epoch 20/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7244 - mse: 0.7244 - val_loss: 0.7252 - val_mse: 0.7252\n",
      "Epoch 21/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7224 - mse: 0.7224 - val_loss: 0.7224 - val_mse: 0.7224\n",
      "Epoch 22/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7198 - mse: 0.7198 - val_loss: 0.7204 - val_mse: 0.7204\n",
      "Epoch 23/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7181 - mse: 0.7181 - val_loss: 0.7235 - val_mse: 0.7235\n",
      "Epoch 24/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7163 - mse: 0.7163 - val_loss: 0.7265 - val_mse: 0.7265\n",
      "Epoch 25/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7145 - mse: 0.7145 - val_loss: 0.7173 - val_mse: 0.7173\n",
      "Epoch 26/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7139 - mse: 0.7139 - val_loss: 0.7135 - val_mse: 0.7135\n",
      "Epoch 27/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7117 - mse: 0.7117 - val_loss: 0.7150 - val_mse: 0.7150\n",
      "Epoch 28/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7105 - mse: 0.7105 - val_loss: 0.7114 - val_mse: 0.7114\n",
      "Epoch 29/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7082 - mse: 0.7082 - val_loss: 0.7485 - val_mse: 0.7485\n",
      "Epoch 30/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7075 - mse: 0.7075 - val_loss: 0.7172 - val_mse: 0.7172\n",
      "Epoch 31/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7059 - mse: 0.7059 - val_loss: 0.7064 - val_mse: 0.7064\n",
      "Epoch 32/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7047 - mse: 0.7047 - val_loss: 0.7056 - val_mse: 0.7056\n",
      "Epoch 33/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7042 - mse: 0.7042 - val_loss: 0.7130 - val_mse: 0.7130\n",
      "Epoch 34/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7023 - mse: 0.7023 - val_loss: 0.7196 - val_mse: 0.7196\n",
      "Epoch 35/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7011 - mse: 0.7011 - val_loss: 0.7083 - val_mse: 0.7083\n",
      "Epoch 36/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7000 - mse: 0.7000 - val_loss: 0.7033 - val_mse: 0.7033\n",
      "Epoch 37/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6988 - mse: 0.6988 - val_loss: 0.7034 - val_mse: 0.7034\n",
      "Epoch 38/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6979 - mse: 0.6979 - val_loss: 0.7026 - val_mse: 0.7026\n",
      "Epoch 39/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6968 - mse: 0.6968 - val_loss: 0.7020 - val_mse: 0.7020\n",
      "Epoch 40/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6955 - mse: 0.6955 - val_loss: 0.6970 - val_mse: 0.6970\n",
      "Epoch 41/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6941 - mse: 0.6941 - val_loss: 0.6997 - val_mse: 0.6997\n",
      "Epoch 42/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6935 - mse: 0.6935 - val_loss: 0.7025 - val_mse: 0.7025\n",
      "Epoch 43/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6924 - mse: 0.6924 - val_loss: 0.6999 - val_mse: 0.6999\n",
      "Epoch 44/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6909 - mse: 0.6909 - val_loss: 0.7009 - val_mse: 0.7009\n",
      "Epoch 45/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6900 - mse: 0.6900 - val_loss: 0.7025 - val_mse: 0.7025\n",
      "Epoch 46/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6889 - mse: 0.6889 - val_loss: 0.7018 - val_mse: 0.7018\n",
      "Epoch 47/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6887 - mse: 0.6887 - val_loss: 0.6914 - val_mse: 0.6914\n",
      "Epoch 48/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6870 - mse: 0.6870 - val_loss: 0.7003 - val_mse: 0.7003\n",
      "Epoch 49/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6861 - mse: 0.6861 - val_loss: 0.7035 - val_mse: 0.7035\n",
      "Epoch 50/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6851 - mse: 0.6851 - val_loss: 0.7075 - val_mse: 0.7075\n",
      "Epoch 51/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6842 - mse: 0.6842 - val_loss: 0.6955 - val_mse: 0.6955\n",
      "Epoch 52/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6832 - mse: 0.6832 - val_loss: 0.7131 - val_mse: 0.7131\n",
      "Epoch 53/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6836 - mse: 0.6836 - val_loss: 0.6928 - val_mse: 0.6928\n",
      "Epoch 54/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6813 - mse: 0.6813 - val_loss: 0.6881 - val_mse: 0.6881\n",
      "Epoch 55/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6806 - mse: 0.6806 - val_loss: 0.6943 - val_mse: 0.6943\n",
      "Epoch 56/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6796 - mse: 0.6796 - val_loss: 0.6907 - val_mse: 0.6907\n",
      "Epoch 57/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6795 - mse: 0.6795 - val_loss: 0.6934 - val_mse: 0.6934\n",
      "Epoch 58/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6784 - mse: 0.6784 - val_loss: 0.7116 - val_mse: 0.7116\n",
      "Epoch 59/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6773 - mse: 0.6773 - val_loss: 0.6933 - val_mse: 0.6933\n",
      "Epoch 60/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6770 - mse: 0.6770 - val_loss: 0.7029 - val_mse: 0.7029\n",
      "Epoch 61/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6760 - mse: 0.6760 - val_loss: 0.6839 - val_mse: 0.6839\n",
      "Epoch 62/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6747 - mse: 0.6747 - val_loss: 0.6911 - val_mse: 0.6911\n",
      "Epoch 63/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6737 - mse: 0.6737 - val_loss: 0.6853 - val_mse: 0.6853\n",
      "Epoch 64/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6737 - mse: 0.6737 - val_loss: 0.6925 - val_mse: 0.6925\n",
      "Epoch 65/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6726 - mse: 0.6726 - val_loss: 0.6908 - val_mse: 0.6908\n",
      "Epoch 66/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6715 - mse: 0.6715 - val_loss: 0.6877 - val_mse: 0.6877\n",
      "Epoch 67/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6706 - mse: 0.6706 - val_loss: 0.6853 - val_mse: 0.6853\n",
      "Epoch 68/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6704 - mse: 0.6704 - val_loss: 0.6816 - val_mse: 0.6816\n",
      "Epoch 69/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6695 - mse: 0.6695 - val_loss: 0.6910 - val_mse: 0.6910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6687 - mse: 0.6687 - val_loss: 0.7127 - val_mse: 0.7127\n",
      "Epoch 71/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6685 - mse: 0.6685 - val_loss: 0.6801 - val_mse: 0.6801\n",
      "Epoch 72/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6674 - mse: 0.6674 - val_loss: 0.6826 - val_mse: 0.6826\n",
      "Epoch 73/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6665 - mse: 0.6665 - val_loss: 0.7023 - val_mse: 0.7023\n",
      "Epoch 74/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6658 - mse: 0.6658 - val_loss: 0.6824 - val_mse: 0.6824\n",
      "Epoch 75/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6657 - mse: 0.6657 - val_loss: 0.6819 - val_mse: 0.6819\n",
      "Epoch 76/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6642 - mse: 0.6642 - val_loss: 0.6796 - val_mse: 0.6796\n",
      "Epoch 77/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6638 - mse: 0.6638 - val_loss: 0.6838 - val_mse: 0.6838\n",
      "Epoch 78/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6631 - mse: 0.6631 - val_loss: 0.6802 - val_mse: 0.6802\n",
      "Epoch 79/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6624 - mse: 0.6624 - val_loss: 0.6871 - val_mse: 0.6871\n",
      "Epoch 80/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6620 - mse: 0.6620 - val_loss: 0.6872 - val_mse: 0.6872\n",
      "Epoch 81/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6609 - mse: 0.6609 - val_loss: 0.6843 - val_mse: 0.6843\n",
      "Epoch 82/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6604 - mse: 0.6604 - val_loss: 0.6844 - val_mse: 0.6844\n",
      "Epoch 83/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6599 - mse: 0.6599 - val_loss: 0.6779 - val_mse: 0.6779\n",
      "Epoch 84/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6595 - mse: 0.6595 - val_loss: 0.6804 - val_mse: 0.6804\n",
      "Epoch 85/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6584 - mse: 0.6584 - val_loss: 0.6798 - val_mse: 0.6798\n",
      "Epoch 86/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6581 - mse: 0.6581 - val_loss: 0.6801 - val_mse: 0.6801\n",
      "Epoch 87/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6575 - mse: 0.6575 - val_loss: 0.7008 - val_mse: 0.7008\n",
      "Epoch 88/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6563 - mse: 0.6563 - val_loss: 0.6891 - val_mse: 0.6891\n",
      "Epoch 89/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6558 - mse: 0.6558 - val_loss: 0.6784 - val_mse: 0.6784\n",
      "Epoch 90/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6555 - mse: 0.6555 - val_loss: 0.6827 - val_mse: 0.6827\n",
      "Epoch 91/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6547 - mse: 0.6547 - val_loss: 0.6869 - val_mse: 0.6869\n",
      "Epoch 92/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6546 - mse: 0.6546 - val_loss: 0.6846 - val_mse: 0.6846\n",
      "Epoch 93/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6545 - mse: 0.6545 - val_loss: 0.6866 - val_mse: 0.6866\n",
      "Epoch 94/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6526 - mse: 0.6526 - val_loss: 0.6845 - val_mse: 0.6845\n",
      "Epoch 95/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6527 - mse: 0.6527 - val_loss: 0.6839 - val_mse: 0.6839\n",
      "Epoch 96/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6518 - mse: 0.6518 - val_loss: 0.6819 - val_mse: 0.6819\n",
      "Epoch 97/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6514 - mse: 0.6514 - val_loss: 0.6782 - val_mse: 0.6782\n",
      "Epoch 98/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6511 - mse: 0.6511 - val_loss: 0.6795 - val_mse: 0.6795\n",
      "Epoch 99/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6505 - mse: 0.6505 - val_loss: 0.6758 - val_mse: 0.6758\n",
      "Epoch 100/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6496 - mse: 0.6496 - val_loss: 0.6845 - val_mse: 0.6845\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8594 - mse: 0.8594 - val_loss: 0.7744 - val_mse: 0.7744\n",
      "Epoch 2/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7409 - mse: 0.7409 - val_loss: 0.7725 - val_mse: 0.7725\n",
      "Epoch 3/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7281 - mse: 0.7281 - val_loss: 0.7655 - val_mse: 0.7655\n",
      "Epoch 4/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7197 - mse: 0.7197 - val_loss: 0.7619 - val_mse: 0.7619\n",
      "Epoch 5/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7129 - mse: 0.7129 - val_loss: 0.7636 - val_mse: 0.7636\n",
      "Epoch 6/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7070 - mse: 0.7070 - val_loss: 0.7616 - val_mse: 0.7616\n",
      "Epoch 7/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7009 - mse: 0.7009 - val_loss: 0.7575 - val_mse: 0.7575\n",
      "Epoch 8/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6957 - mse: 0.6957 - val_loss: 0.7515 - val_mse: 0.7515\n",
      "Epoch 9/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6913 - mse: 0.6913 - val_loss: 0.7489 - val_mse: 0.7489\n",
      "Epoch 10/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6863 - mse: 0.6863 - val_loss: 0.7504 - val_mse: 0.7504\n",
      "Epoch 11/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6829 - mse: 0.6829 - val_loss: 0.7469 - val_mse: 0.7469\n",
      "Epoch 12/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6789 - mse: 0.6789 - val_loss: 0.7483 - val_mse: 0.7483\n",
      "Epoch 13/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6756 - mse: 0.6756 - val_loss: 0.7435 - val_mse: 0.7435\n",
      "Epoch 14/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6728 - mse: 0.6728 - val_loss: 0.7528 - val_mse: 0.7528\n",
      "Epoch 15/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6699 - mse: 0.6699 - val_loss: 0.7421 - val_mse: 0.7421\n",
      "Epoch 16/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6672 - mse: 0.6672 - val_loss: 0.7383 - val_mse: 0.7383\n",
      "Epoch 17/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6651 - mse: 0.6651 - val_loss: 0.7392 - val_mse: 0.7392\n",
      "Epoch 18/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6633 - mse: 0.6633 - val_loss: 0.7401 - val_mse: 0.7401\n",
      "Epoch 19/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6616 - mse: 0.6616 - val_loss: 0.7318 - val_mse: 0.7318\n",
      "Epoch 20/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6593 - mse: 0.6593 - val_loss: 0.7311 - val_mse: 0.7311\n",
      "Epoch 21/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6578 - mse: 0.6578 - val_loss: 0.7312 - val_mse: 0.7312\n",
      "Epoch 22/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6565 - mse: 0.6565 - val_loss: 0.7364 - val_mse: 0.7364\n",
      "Epoch 23/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6546 - mse: 0.6546 - val_loss: 0.7309 - val_mse: 0.7309\n",
      "Epoch 24/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6534 - mse: 0.6534 - val_loss: 0.7240 - val_mse: 0.7240\n",
      "Epoch 25/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6522 - mse: 0.6522 - val_loss: 0.7253 - val_mse: 0.7253\n",
      "Epoch 26/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6508 - mse: 0.6508 - val_loss: 0.7365 - val_mse: 0.7365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6492 - mse: 0.6492 - val_loss: 0.7213 - val_mse: 0.7213\n",
      "Epoch 28/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6484 - mse: 0.6484 - val_loss: 0.7270 - val_mse: 0.7270\n",
      "Epoch 29/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6471 - mse: 0.6471 - val_loss: 0.7335 - val_mse: 0.7335\n",
      "Epoch 30/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6457 - mse: 0.6457 - val_loss: 0.7189 - val_mse: 0.7189\n",
      "Epoch 31/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6448 - mse: 0.6448 - val_loss: 0.7250 - val_mse: 0.7250\n",
      "Epoch 32/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6438 - mse: 0.6438 - val_loss: 0.7349 - val_mse: 0.7349\n",
      "Epoch 33/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6429 - mse: 0.6429 - val_loss: 0.7258 - val_mse: 0.7258\n",
      "Epoch 34/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6417 - mse: 0.6417 - val_loss: 0.7318 - val_mse: 0.7318\n",
      "Epoch 35/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6404 - mse: 0.6404 - val_loss: 0.7161 - val_mse: 0.7161\n",
      "Epoch 36/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6401 - mse: 0.6401 - val_loss: 0.7188 - val_mse: 0.7188\n",
      "Epoch 37/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6389 - mse: 0.6389 - val_loss: 0.7212 - val_mse: 0.7212\n",
      "Epoch 38/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6377 - mse: 0.6377 - val_loss: 0.7224 - val_mse: 0.7224\n",
      "Epoch 39/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6369 - mse: 0.6369 - val_loss: 0.7180 - val_mse: 0.7180\n",
      "Epoch 40/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6360 - mse: 0.6360 - val_loss: 0.7193 - val_mse: 0.7193\n",
      "Epoch 41/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6352 - mse: 0.6352 - val_loss: 0.7219 - val_mse: 0.7219\n",
      "Epoch 42/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6350 - mse: 0.6350 - val_loss: 0.7309 - val_mse: 0.7309\n",
      "Epoch 43/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6334 - mse: 0.6334 - val_loss: 0.7191 - val_mse: 0.7191\n",
      "Epoch 44/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6325 - mse: 0.6325 - val_loss: 0.7154 - val_mse: 0.7154\n",
      "Epoch 45/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6324 - mse: 0.6324 - val_loss: 0.7142 - val_mse: 0.7142\n",
      "Epoch 46/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6312 - mse: 0.6312 - val_loss: 0.7150 - val_mse: 0.7150\n",
      "Epoch 47/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6307 - mse: 0.6307 - val_loss: 0.7087 - val_mse: 0.7087\n",
      "Epoch 48/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6298 - mse: 0.6298 - val_loss: 0.7094 - val_mse: 0.7094\n",
      "Epoch 49/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6289 - mse: 0.6289 - val_loss: 0.7131 - val_mse: 0.7131\n",
      "Epoch 50/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6284 - mse: 0.6284 - val_loss: 0.7077 - val_mse: 0.7077\n",
      "Epoch 51/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6273 - mse: 0.6273 - val_loss: 0.7094 - val_mse: 0.7094\n",
      "Epoch 52/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6265 - mse: 0.6265 - val_loss: 0.7107 - val_mse: 0.7107\n",
      "Epoch 53/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6260 - mse: 0.6260 - val_loss: 0.7096 - val_mse: 0.7096\n",
      "Epoch 54/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6257 - mse: 0.6257 - val_loss: 0.7052 - val_mse: 0.7052\n",
      "Epoch 55/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6248 - mse: 0.6248 - val_loss: 0.7155 - val_mse: 0.7155\n",
      "Epoch 56/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6241 - mse: 0.6241 - val_loss: 0.7068 - val_mse: 0.7068\n",
      "Epoch 57/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6232 - mse: 0.6232 - val_loss: 0.7083 - val_mse: 0.7083\n",
      "Epoch 58/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6231 - mse: 0.6231 - val_loss: 0.7043 - val_mse: 0.7043\n",
      "Epoch 59/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6218 - mse: 0.6218 - val_loss: 0.7184 - val_mse: 0.7184\n",
      "Epoch 60/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6218 - mse: 0.6218 - val_loss: 0.7054 - val_mse: 0.7054\n",
      "Epoch 61/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6215 - mse: 0.6215 - val_loss: 0.7076 - val_mse: 0.7076\n",
      "Epoch 62/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6202 - mse: 0.6202 - val_loss: 0.7037 - val_mse: 0.7037\n",
      "Epoch 63/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6192 - mse: 0.6192 - val_loss: 0.7129 - val_mse: 0.7129\n",
      "Epoch 64/100\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6186 - mse: 0.6186 - val_loss: 0.7027 - val_mse: 0.7027\n",
      "Epoch 65/100\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6187 - mse: 0.6187 - val_loss: 0.6994 - val_mse: 0.6994\n",
      "Epoch 66/100\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6179 - mse: 0.6179 - val_loss: 0.7063 - val_mse: 0.7063\n",
      "Epoch 67/100\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6171 - mse: 0.6171 - val_loss: 0.7019 - val_mse: 0.7019\n",
      "Epoch 68/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6163 - mse: 0.6163 - val_loss: 0.7124 - val_mse: 0.7124\n",
      "Epoch 69/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6165 - mse: 0.6165 - val_loss: 0.7031 - val_mse: 0.7031\n",
      "Epoch 70/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6157 - mse: 0.6157 - val_loss: 0.7087 - val_mse: 0.7087\n",
      "Epoch 71/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6151 - mse: 0.6151 - val_loss: 0.7001 - val_mse: 0.7001\n",
      "Epoch 72/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6146 - mse: 0.6146 - val_loss: 0.7001 - val_mse: 0.7001\n",
      "Epoch 73/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6138 - mse: 0.6138 - val_loss: 0.7078 - val_mse: 0.7078\n",
      "Epoch 74/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6138 - mse: 0.6138 - val_loss: 0.7056 - val_mse: 0.7056\n",
      "Epoch 75/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6134 - mse: 0.6134 - val_loss: 0.7026 - val_mse: 0.7026\n",
      "Epoch 76/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6123 - mse: 0.6123 - val_loss: 0.7146 - val_mse: 0.7146\n",
      "Epoch 77/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6117 - mse: 0.6117 - val_loss: 0.7002 - val_mse: 0.7002\n",
      "Epoch 78/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6114 - mse: 0.6114 - val_loss: 0.7008 - val_mse: 0.7008\n",
      "Epoch 79/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6109 - mse: 0.6109 - val_loss: 0.7018 - val_mse: 0.7018\n",
      "Epoch 80/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6105 - mse: 0.6105 - val_loss: 0.7053 - val_mse: 0.7053\n",
      "Epoch 81/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6100 - mse: 0.6100 - val_loss: 0.7091 - val_mse: 0.7091\n",
      "Epoch 82/100\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6102 - mse: 0.6102 - val_loss: 0.6945 - val_mse: 0.6945\n",
      "Epoch 83/100\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6090 - mse: 0.6090 - val_loss: 0.6982 - val_mse: 0.6982\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6084 - mse: 0.6084 - val_loss: 0.6986 - val_mse: 0.6986\n",
      "Epoch 85/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6079 - mse: 0.6079 - val_loss: 0.6987 - val_mse: 0.6987\n",
      "Epoch 86/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6072 - mse: 0.6072 - val_loss: 0.6976 - val_mse: 0.6976\n",
      "Epoch 87/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6075 - mse: 0.6075 - val_loss: 0.7000 - val_mse: 0.7000\n",
      "Epoch 88/100\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.6066 - mse: 0.6066 - val_loss: 0.6971 - val_mse: 0.6971\n",
      "Epoch 89/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6058 - mse: 0.6058 - val_loss: 0.6938 - val_mse: 0.6938\n",
      "Epoch 90/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6062 - mse: 0.6062 - val_loss: 0.7024 - val_mse: 0.7024\n",
      "Epoch 91/100\n",
      "3353317/3353317 [==============================] - 306s 91us/step - loss: 0.6050 - mse: 0.6050 - val_loss: 0.6947 - val_mse: 0.6947\n",
      "Epoch 92/100\n",
      "3353317/3353317 [==============================] - 11s 3us/step - loss: 0.6045 - mse: 0.6045 - val_loss: 0.6935 - val_mse: 0.6935\n",
      "Epoch 93/100\n",
      "3353317/3353317 [==============================] - 895s 267us/step - loss: 0.6044 - mse: 0.6044 - val_loss: 0.6941 - val_mse: 0.6941\n",
      "Epoch 94/100\n",
      "3353317/3353317 [==============================] - 901s 269us/step - loss: 0.6034 - mse: 0.6034 - val_loss: 0.6955 - val_mse: 0.6955\n",
      "Epoch 95/100\n",
      "3353317/3353317 [==============================] - 773s 231us/step - loss: 0.6040 - mse: 0.6040 - val_loss: 0.6930 - val_mse: 0.6930\n",
      "Epoch 96/100\n",
      "3353317/3353317 [==============================] - 7s 2us/step - loss: 0.6032 - mse: 0.6032 - val_loss: 0.6940 - val_mse: 0.6940\n",
      "Epoch 97/100\n",
      "3353317/3353317 [==============================] - 7s 2us/step - loss: 0.6028 - mse: 0.6028 - val_loss: 0.6955 - val_mse: 0.6955\n",
      "Epoch 98/100\n",
      "3353317/3353317 [==============================] - 7s 2us/step - loss: 0.6022 - mse: 0.6022 - val_loss: 0.6869 - val_mse: 0.6869\n",
      "Epoch 99/100\n",
      "3353317/3353317 [==============================] - 7s 2us/step - loss: 0.6016 - mse: 0.6016 - val_loss: 0.7003 - val_mse: 0.7003\n",
      "Epoch 100/100\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6013 - mse: 0.6013 - val_loss: 0.6914 - val_mse: 0.6914\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.9845 - mse: 0.9845 - val_loss: 0.8021 - val_mse: 0.8021\n",
      "Epoch 2/100\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.8589 - mse: 0.8589 - val_loss: 0.8215 - val_mse: 0.8215\n",
      "Epoch 3/100\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.8401 - mse: 0.8401 - val_loss: 0.8284 - val_mse: 0.8284\n",
      "Epoch 4/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8270 - mse: 0.8270 - val_loss: 0.8416 - val_mse: 0.8416\n",
      "Epoch 5/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8166 - mse: 0.8166 - val_loss: 0.8500 - val_mse: 0.8500\n",
      "Epoch 6/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8073 - mse: 0.8073 - val_loss: 0.8209 - val_mse: 0.8209\n",
      "Epoch 7/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7990 - mse: 0.7990 - val_loss: 0.8126 - val_mse: 0.8126\n",
      "Epoch 8/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7915 - mse: 0.7915 - val_loss: 0.7967 - val_mse: 0.7967\n",
      "Epoch 9/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7858 - mse: 0.7858 - val_loss: 0.7978 - val_mse: 0.7978\n",
      "Epoch 10/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7798 - mse: 0.7798 - val_loss: 0.7872 - val_mse: 0.7872\n",
      "Epoch 11/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7756 - mse: 0.7756 - val_loss: 0.7819 - val_mse: 0.7819\n",
      "Epoch 12/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7714 - mse: 0.7714 - val_loss: 0.7769 - val_mse: 0.7769\n",
      "Epoch 13/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7685 - mse: 0.7685 - val_loss: 0.7814 - val_mse: 0.7814\n",
      "Epoch 14/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7644 - mse: 0.7644 - val_loss: 0.7817 - val_mse: 0.7817\n",
      "Epoch 15/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7624 - mse: 0.7624 - val_loss: 0.7682 - val_mse: 0.7682\n",
      "Epoch 16/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7591 - mse: 0.7591 - val_loss: 0.7683 - val_mse: 0.7683\n",
      "Epoch 17/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7566 - mse: 0.7566 - val_loss: 0.7647 - val_mse: 0.7647\n",
      "Epoch 18/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7544 - mse: 0.7544 - val_loss: 0.7615 - val_mse: 0.7615\n",
      "Epoch 19/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7528 - mse: 0.7528 - val_loss: 0.7653 - val_mse: 0.7653\n",
      "Epoch 20/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7509 - mse: 0.7509 - val_loss: 0.7548 - val_mse: 0.7548\n",
      "Epoch 21/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7491 - mse: 0.7491 - val_loss: 0.7503 - val_mse: 0.7503\n",
      "Epoch 22/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7471 - mse: 0.7471 - val_loss: 0.7565 - val_mse: 0.7565\n",
      "Epoch 23/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7453 - mse: 0.7453 - val_loss: 0.7489 - val_mse: 0.7489\n",
      "Epoch 24/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7441 - mse: 0.7441 - val_loss: 0.7501 - val_mse: 0.7501\n",
      "Epoch 25/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7428 - mse: 0.7428 - val_loss: 0.7510 - val_mse: 0.7510\n",
      "Epoch 26/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7411 - mse: 0.7411 - val_loss: 0.7551 - val_mse: 0.7551\n",
      "Epoch 27/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7403 - mse: 0.7403 - val_loss: 0.7516 - val_mse: 0.7516\n",
      "Epoch 28/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7383 - mse: 0.7383 - val_loss: 0.7463 - val_mse: 0.7463\n",
      "Epoch 29/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7371 - mse: 0.7371 - val_loss: 0.7467 - val_mse: 0.7467\n",
      "Epoch 30/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7362 - mse: 0.7362 - val_loss: 0.7433 - val_mse: 0.7433\n",
      "Epoch 31/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7351 - mse: 0.7351 - val_loss: 0.7509 - val_mse: 0.7509\n",
      "Epoch 32/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7340 - mse: 0.7340 - val_loss: 0.7594 - val_mse: 0.7594\n",
      "Epoch 33/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7327 - mse: 0.7327 - val_loss: 0.7393 - val_mse: 0.7393\n",
      "Epoch 34/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7316 - mse: 0.7316 - val_loss: 0.7444 - val_mse: 0.7444\n",
      "Epoch 35/100\n",
      "3353318/3353318 [==============================] - 8s 3us/step - loss: 0.7303 - mse: 0.7303 - val_loss: 0.7413 - val_mse: 0.7413\n",
      "Epoch 36/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7295 - mse: 0.7295 - val_loss: 0.7342 - val_mse: 0.7342\n",
      "Epoch 37/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7284 - mse: 0.7284 - val_loss: 0.7440 - val_mse: 0.7440\n",
      "Epoch 38/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7273 - mse: 0.7273 - val_loss: 0.7374 - val_mse: 0.7374\n",
      "Epoch 39/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7261 - mse: 0.7261 - val_loss: 0.7374 - val_mse: 0.7374\n",
      "Epoch 40/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7256 - mse: 0.7256 - val_loss: 0.7384 - val_mse: 0.7384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7242 - mse: 0.7242 - val_loss: 0.7320 - val_mse: 0.7320\n",
      "Epoch 42/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7230 - mse: 0.7230 - val_loss: 0.7333 - val_mse: 0.7333\n",
      "Epoch 43/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7225 - mse: 0.7225 - val_loss: 0.7380 - val_mse: 0.7380\n",
      "Epoch 44/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7210 - mse: 0.7210 - val_loss: 0.7311 - val_mse: 0.7311\n",
      "Epoch 45/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7203 - mse: 0.7203 - val_loss: 0.7298 - val_mse: 0.7298\n",
      "Epoch 46/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7198 - mse: 0.7198 - val_loss: 0.7353 - val_mse: 0.7353\n",
      "Epoch 47/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7181 - mse: 0.7181 - val_loss: 0.7313 - val_mse: 0.7313\n",
      "Epoch 48/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7171 - mse: 0.7171 - val_loss: 0.7322 - val_mse: 0.7322\n",
      "Epoch 49/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7162 - mse: 0.7162 - val_loss: 0.7312 - val_mse: 0.7312\n",
      "Epoch 50/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7152 - mse: 0.7152 - val_loss: 0.7264 - val_mse: 0.7264\n",
      "Epoch 51/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7144 - mse: 0.7144 - val_loss: 0.7346 - val_mse: 0.7346\n",
      "Epoch 52/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7140 - mse: 0.7140 - val_loss: 0.7363 - val_mse: 0.7363\n",
      "Epoch 53/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7123 - mse: 0.7123 - val_loss: 0.7451 - val_mse: 0.7451\n",
      "Epoch 54/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7122 - mse: 0.7122 - val_loss: 0.7301 - val_mse: 0.7301\n",
      "Epoch 55/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7105 - mse: 0.7105 - val_loss: 0.7310 - val_mse: 0.7310\n",
      "Epoch 56/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7098 - mse: 0.7098 - val_loss: 0.7277 - val_mse: 0.7277\n",
      "Epoch 57/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7090 - mse: 0.7090 - val_loss: 0.7240 - val_mse: 0.7240\n",
      "Epoch 58/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7087 - mse: 0.7087 - val_loss: 0.7288 - val_mse: 0.7288\n",
      "Epoch 59/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7071 - mse: 0.7071 - val_loss: 0.7348 - val_mse: 0.7348\n",
      "Epoch 60/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7066 - mse: 0.7066 - val_loss: 0.7304 - val_mse: 0.7304\n",
      "Epoch 61/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7060 - mse: 0.7060 - val_loss: 0.7384 - val_mse: 0.7384\n",
      "Epoch 62/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7049 - mse: 0.7049 - val_loss: 0.7295 - val_mse: 0.7295\n",
      "Epoch 63/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7045 - mse: 0.7045 - val_loss: 0.7277 - val_mse: 0.7277\n",
      "Epoch 64/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7030 - mse: 0.7030 - val_loss: 0.7244 - val_mse: 0.7244\n",
      "Epoch 65/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7026 - mse: 0.7026 - val_loss: 0.7334 - val_mse: 0.7334\n",
      "Epoch 66/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7026 - mse: 0.7026 - val_loss: 0.7267 - val_mse: 0.7267\n",
      "Epoch 67/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7012 - mse: 0.7012 - val_loss: 0.7323 - val_mse: 0.7323\n",
      "Epoch 68/100\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.7006 - mse: 0.7006 - val_loss: 0.7244 - val_mse: 0.7244\n",
      "Epoch 69/100\n",
      "3353318/3353318 [==============================] - 8s 3us/step - loss: 0.6996 - mse: 0.6996 - val_loss: 0.7263 - val_mse: 0.7263\n",
      "Epoch 70/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6993 - mse: 0.6993 - val_loss: 0.7273 - val_mse: 0.7273\n",
      "Epoch 71/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6980 - mse: 0.6980 - val_loss: 0.7297 - val_mse: 0.7297\n",
      "Epoch 72/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6976 - mse: 0.6976 - val_loss: 0.7325 - val_mse: 0.7325\n",
      "Epoch 73/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6968 - mse: 0.6968 - val_loss: 0.7304 - val_mse: 0.7304\n",
      "Epoch 74/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6964 - mse: 0.6964 - val_loss: 0.7251 - val_mse: 0.7251\n",
      "Epoch 75/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6958 - mse: 0.6958 - val_loss: 0.7253 - val_mse: 0.7253\n",
      "Epoch 76/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6945 - mse: 0.6945 - val_loss: 0.7270 - val_mse: 0.7270\n",
      "Epoch 77/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6942 - mse: 0.6942 - val_loss: 0.7291 - val_mse: 0.7291\n",
      "Epoch 78/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6935 - mse: 0.6935 - val_loss: 0.7239 - val_mse: 0.7239\n",
      "Epoch 79/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6931 - mse: 0.6931 - val_loss: 0.7270 - val_mse: 0.7270\n",
      "Epoch 80/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6919 - mse: 0.6919 - val_loss: 0.7235 - val_mse: 0.7235\n",
      "Epoch 81/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6915 - mse: 0.6915 - val_loss: 0.7259 - val_mse: 0.7259\n",
      "Epoch 82/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6910 - mse: 0.6910 - val_loss: 0.7255 - val_mse: 0.7255\n",
      "Epoch 83/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6910 - mse: 0.6910 - val_loss: 0.7289 - val_mse: 0.7289\n",
      "Epoch 84/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6895 - mse: 0.6895 - val_loss: 0.7235 - val_mse: 0.7235\n",
      "Epoch 85/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6888 - mse: 0.6888 - val_loss: 0.7292 - val_mse: 0.7292\n",
      "Epoch 86/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6885 - mse: 0.6885 - val_loss: 0.7391 - val_mse: 0.7391\n",
      "Epoch 87/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6885 - mse: 0.6885 - val_loss: 0.7246 - val_mse: 0.7246\n",
      "Epoch 88/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6869 - mse: 0.6869 - val_loss: 0.7266 - val_mse: 0.7266\n",
      "Epoch 89/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6869 - mse: 0.6869 - val_loss: 0.7215 - val_mse: 0.7215\n",
      "Epoch 90/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6860 - mse: 0.6860 - val_loss: 0.7286 - val_mse: 0.7286\n",
      "Epoch 91/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6852 - mse: 0.6852 - val_loss: 0.7237 - val_mse: 0.7237\n",
      "Epoch 92/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6849 - mse: 0.6849 - val_loss: 0.7329 - val_mse: 0.7329\n",
      "Epoch 93/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6855 - mse: 0.6855 - val_loss: 0.7301 - val_mse: 0.7301\n",
      "Epoch 94/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6847 - mse: 0.6847 - val_loss: 0.7318 - val_mse: 0.7318\n",
      "Epoch 95/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6831 - mse: 0.6831 - val_loss: 0.7239 - val_mse: 0.7239\n",
      "Epoch 96/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6827 - mse: 0.6827 - val_loss: 0.7247 - val_mse: 0.7247\n",
      "Epoch 97/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6819 - mse: 0.6819 - val_loss: 0.7279 - val_mse: 0.7279\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6816 - mse: 0.6816 - val_loss: 0.7242 - val_mse: 0.7242\n",
      "Epoch 99/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6812 - mse: 0.6812 - val_loss: 0.7399 - val_mse: 0.7399\n",
      "Epoch 100/100\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6810 - mse: 0.6810 - val_loss: 0.7305 - val_mse: 0.7305\n",
      "1676658/1676658 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.9751 - mse: 0.9751 - val_loss: 0.7790 - val_mse: 0.7790\n",
      "Epoch 2/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8415 - mse: 0.8415 - val_loss: 0.7712 - val_mse: 0.7712\n",
      "Epoch 3/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8232 - mse: 0.8232 - val_loss: 0.7629 - val_mse: 0.7629\n",
      "Epoch 4/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8106 - mse: 0.8106 - val_loss: 0.7911 - val_mse: 0.7911\n",
      "Epoch 5/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8006 - mse: 0.8006 - val_loss: 0.7694 - val_mse: 0.7694\n",
      "Epoch 6/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7911 - mse: 0.7911 - val_loss: 0.7572 - val_mse: 0.7572\n",
      "Epoch 7/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7826 - mse: 0.7827 - val_loss: 0.7700 - val_mse: 0.7700\n",
      "Epoch 8/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7745 - mse: 0.7745 - val_loss: 0.7697 - val_mse: 0.7697\n",
      "Epoch 9/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7670 - mse: 0.7670 - val_loss: 0.7577 - val_mse: 0.7577\n",
      "Epoch 10/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7600 - mse: 0.7600 - val_loss: 0.7588 - val_mse: 0.7588\n",
      "Epoch 11/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7548 - mse: 0.7548 - val_loss: 0.7509 - val_mse: 0.7509\n",
      "Epoch 12/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7487 - mse: 0.7487 - val_loss: 0.7480 - val_mse: 0.7480\n",
      "Epoch 13/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7443 - mse: 0.7443 - val_loss: 0.7373 - val_mse: 0.7373\n",
      "Epoch 14/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7396 - mse: 0.7396 - val_loss: 0.7567 - val_mse: 0.7567\n",
      "Epoch 15/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7359 - mse: 0.7359 - val_loss: 0.7288 - val_mse: 0.7288\n",
      "Epoch 16/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7333 - mse: 0.7333 - val_loss: 0.7422 - val_mse: 0.7422\n",
      "Epoch 17/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7294 - mse: 0.7294 - val_loss: 0.7252 - val_mse: 0.7252\n",
      "Epoch 18/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7270 - mse: 0.7270 - val_loss: 0.7327 - val_mse: 0.7327\n",
      "Epoch 19/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7249 - mse: 0.7249 - val_loss: 0.7218 - val_mse: 0.7218\n",
      "Epoch 20/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7222 - mse: 0.7222 - val_loss: 0.7206 - val_mse: 0.7206\n",
      "Epoch 21/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7206 - mse: 0.7206 - val_loss: 0.7450 - val_mse: 0.7450\n",
      "Epoch 22/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7184 - mse: 0.7184 - val_loss: 0.7142 - val_mse: 0.7142\n",
      "Epoch 23/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7163 - mse: 0.7163 - val_loss: 0.7183 - val_mse: 0.7183\n",
      "Epoch 24/150\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.7150 - mse: 0.7150 - val_loss: 0.7157 - val_mse: 0.7157\n",
      "Epoch 25/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7130 - mse: 0.7130 - val_loss: 0.7385 - val_mse: 0.7385\n",
      "Epoch 26/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7111 - mse: 0.7111 - val_loss: 0.7116 - val_mse: 0.7116\n",
      "Epoch 27/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7099 - mse: 0.7099 - val_loss: 0.7102 - val_mse: 0.7102\n",
      "Epoch 28/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7085 - mse: 0.7085 - val_loss: 0.7131 - val_mse: 0.7131\n",
      "Epoch 29/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7067 - mse: 0.7067 - val_loss: 0.7220 - val_mse: 0.7220\n",
      "Epoch 30/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7056 - mse: 0.7056 - val_loss: 0.7131 - val_mse: 0.7131\n",
      "Epoch 31/150\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.7044 - mse: 0.7044 - val_loss: 0.7162 - val_mse: 0.7162\n",
      "Epoch 32/150\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.7027 - mse: 0.7027 - val_loss: 0.7205 - val_mse: 0.7205\n",
      "Epoch 33/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7018 - mse: 0.7018 - val_loss: 0.7088 - val_mse: 0.7088\n",
      "Epoch 34/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7004 - mse: 0.7004 - val_loss: 0.7116 - val_mse: 0.7116\n",
      "Epoch 35/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6993 - mse: 0.6993 - val_loss: 0.7346 - val_mse: 0.7346\n",
      "Epoch 36/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6980 - mse: 0.6980 - val_loss: 0.7105 - val_mse: 0.7105\n",
      "Epoch 37/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6968 - mse: 0.6968 - val_loss: 0.7015 - val_mse: 0.7015\n",
      "Epoch 38/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6961 - mse: 0.6961 - val_loss: 0.7039 - val_mse: 0.7039\n",
      "Epoch 39/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6944 - mse: 0.6944 - val_loss: 0.7002 - val_mse: 0.7002\n",
      "Epoch 40/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6936 - mse: 0.6936 - val_loss: 0.7062 - val_mse: 0.7062\n",
      "Epoch 41/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6931 - mse: 0.6931 - val_loss: 0.7102 - val_mse: 0.7102\n",
      "Epoch 42/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6921 - mse: 0.6921 - val_loss: 0.7067 - val_mse: 0.7067\n",
      "Epoch 43/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6913 - mse: 0.6913 - val_loss: 0.7056 - val_mse: 0.7056\n",
      "Epoch 44/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6900 - mse: 0.6900 - val_loss: 0.7052 - val_mse: 0.7052\n",
      "Epoch 45/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6889 - mse: 0.6889 - val_loss: 0.7019 - val_mse: 0.7019\n",
      "Epoch 46/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6877 - mse: 0.6877 - val_loss: 0.7046 - val_mse: 0.7046\n",
      "Epoch 47/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6874 - mse: 0.6874 - val_loss: 0.6979 - val_mse: 0.6979\n",
      "Epoch 48/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6869 - mse: 0.6869 - val_loss: 0.7058 - val_mse: 0.7058\n",
      "Epoch 49/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6855 - mse: 0.6855 - val_loss: 0.7011 - val_mse: 0.7011\n",
      "Epoch 50/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6857 - mse: 0.6857 - val_loss: 0.6947 - val_mse: 0.6947\n",
      "Epoch 51/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6835 - mse: 0.6835 - val_loss: 0.6971 - val_mse: 0.6971\n",
      "Epoch 52/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6829 - mse: 0.6829 - val_loss: 0.7281 - val_mse: 0.7281\n",
      "Epoch 53/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6823 - mse: 0.6823 - val_loss: 0.7096 - val_mse: 0.7096\n",
      "Epoch 54/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6809 - mse: 0.6809 - val_loss: 0.7031 - val_mse: 0.7031\n",
      "Epoch 55/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6807 - mse: 0.6807 - val_loss: 0.6958 - val_mse: 0.6958\n",
      "Epoch 56/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6800 - mse: 0.6800 - val_loss: 0.7054 - val_mse: 0.7054\n",
      "Epoch 57/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6789 - mse: 0.6789 - val_loss: 0.6983 - val_mse: 0.6983\n",
      "Epoch 58/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6790 - mse: 0.6790 - val_loss: 0.7019 - val_mse: 0.7019\n",
      "Epoch 59/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6774 - mse: 0.6774 - val_loss: 0.6929 - val_mse: 0.6929\n",
      "Epoch 60/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6766 - mse: 0.6766 - val_loss: 0.6861 - val_mse: 0.6861\n",
      "Epoch 61/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6761 - mse: 0.6761 - val_loss: 0.6914 - val_mse: 0.6914\n",
      "Epoch 62/150\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.6756 - mse: 0.6756 - val_loss: 0.6935 - val_mse: 0.6935\n",
      "Epoch 63/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6745 - mse: 0.6745 - val_loss: 0.6815 - val_mse: 0.6815\n",
      "Epoch 64/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6737 - mse: 0.6737 - val_loss: 0.6930 - val_mse: 0.6930\n",
      "Epoch 65/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6727 - mse: 0.6727 - val_loss: 0.6953 - val_mse: 0.6953\n",
      "Epoch 66/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6721 - mse: 0.6721 - val_loss: 0.6863 - val_mse: 0.6863\n",
      "Epoch 67/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6714 - mse: 0.6714 - val_loss: 0.6838 - val_mse: 0.6838\n",
      "Epoch 68/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6704 - mse: 0.6704 - val_loss: 0.6946 - val_mse: 0.6946\n",
      "Epoch 69/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6699 - mse: 0.6699 - val_loss: 0.6825 - val_mse: 0.6825\n",
      "Epoch 70/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6687 - mse: 0.6687 - val_loss: 0.6865 - val_mse: 0.6865\n",
      "Epoch 71/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6677 - mse: 0.6677 - val_loss: 0.6822 - val_mse: 0.6822\n",
      "Epoch 72/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6675 - mse: 0.6675 - val_loss: 0.6822 - val_mse: 0.6822\n",
      "Epoch 73/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6663 - mse: 0.6663 - val_loss: 0.6831 - val_mse: 0.6831\n",
      "Epoch 74/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6657 - mse: 0.6657 - val_loss: 0.6906 - val_mse: 0.6906\n",
      "Epoch 75/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6645 - mse: 0.6645 - val_loss: 0.6851 - val_mse: 0.6851\n",
      "Epoch 76/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6642 - mse: 0.6642 - val_loss: 0.6814 - val_mse: 0.6814\n",
      "Epoch 77/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6641 - mse: 0.6641 - val_loss: 0.6833 - val_mse: 0.6833\n",
      "Epoch 78/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6624 - mse: 0.6624 - val_loss: 0.6839 - val_mse: 0.6839\n",
      "Epoch 79/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6624 - mse: 0.6624 - val_loss: 0.6841 - val_mse: 0.6841\n",
      "Epoch 80/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6622 - mse: 0.6622 - val_loss: 0.6813 - val_mse: 0.6813\n",
      "Epoch 81/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6607 - mse: 0.6607 - val_loss: 0.6985 - val_mse: 0.6985\n",
      "Epoch 82/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6601 - mse: 0.6601 - val_loss: 0.6857 - val_mse: 0.6857\n",
      "Epoch 83/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6594 - mse: 0.6594 - val_loss: 0.6807 - val_mse: 0.6807\n",
      "Epoch 84/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6591 - mse: 0.6591 - val_loss: 0.6856 - val_mse: 0.6856\n",
      "Epoch 85/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6587 - mse: 0.6587 - val_loss: 0.6749 - val_mse: 0.6749\n",
      "Epoch 86/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6583 - mse: 0.6583 - val_loss: 0.6795 - val_mse: 0.6795\n",
      "Epoch 87/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6568 - mse: 0.6568 - val_loss: 0.6883 - val_mse: 0.6883\n",
      "Epoch 88/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6566 - mse: 0.6566 - val_loss: 0.6774 - val_mse: 0.6774\n",
      "Epoch 89/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6559 - mse: 0.6559 - val_loss: 0.6784 - val_mse: 0.6784\n",
      "Epoch 90/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6553 - mse: 0.6553 - val_loss: 0.6831 - val_mse: 0.6831\n",
      "Epoch 91/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6549 - mse: 0.6549 - val_loss: 0.6892 - val_mse: 0.6892\n",
      "Epoch 92/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6541 - mse: 0.6541 - val_loss: 0.6779 - val_mse: 0.6779\n",
      "Epoch 93/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6544 - mse: 0.6544 - val_loss: 0.6710 - val_mse: 0.6710\n",
      "Epoch 94/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6534 - mse: 0.6534 - val_loss: 0.6760 - val_mse: 0.6760\n",
      "Epoch 95/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6532 - mse: 0.6532 - val_loss: 0.6760 - val_mse: 0.6760\n",
      "Epoch 96/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6520 - mse: 0.6520 - val_loss: 0.6786 - val_mse: 0.6786\n",
      "Epoch 97/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6515 - mse: 0.6515 - val_loss: 0.6830 - val_mse: 0.6830\n",
      "Epoch 98/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6517 - mse: 0.6517 - val_loss: 0.6860 - val_mse: 0.6860\n",
      "Epoch 99/150\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6510 - mse: 0.6510 - val_loss: 0.6774 - val_mse: 0.6774\n",
      "Epoch 100/150\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.6509 - mse: 0.6509 - val_loss: 0.6849 - val_mse: 0.6849\n",
      "Epoch 101/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6491 - mse: 0.6491 - val_loss: 0.6753 - val_mse: 0.6753\n",
      "Epoch 102/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6495 - mse: 0.6495 - val_loss: 0.6810 - val_mse: 0.6810\n",
      "Epoch 103/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6484 - mse: 0.6484 - val_loss: 0.6944 - val_mse: 0.6944\n",
      "Epoch 104/150\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6484 - mse: 0.6484 - val_loss: 0.6749 - val_mse: 0.6749\n",
      "Epoch 105/150\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6477 - mse: 0.6477 - val_loss: 0.6790 - val_mse: 0.6790\n",
      "Epoch 106/150\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6473 - mse: 0.6473 - val_loss: 0.6762 - val_mse: 0.6762\n",
      "Epoch 107/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6468 - mse: 0.6468 - val_loss: 0.6924 - val_mse: 0.6924\n",
      "Epoch 108/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6466 - mse: 0.6466 - val_loss: 0.6807 - val_mse: 0.6807\n",
      "Epoch 109/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6456 - mse: 0.6456 - val_loss: 0.6783 - val_mse: 0.6783\n",
      "Epoch 110/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6456 - mse: 0.6456 - val_loss: 0.6756 - val_mse: 0.6756\n",
      "Epoch 111/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6450 - mse: 0.6450 - val_loss: 0.6748 - val_mse: 0.6748\n",
      "Epoch 112/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6446 - mse: 0.6446 - val_loss: 0.6801 - val_mse: 0.6801\n",
      "Epoch 113/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6453 - mse: 0.6453 - val_loss: 0.6768 - val_mse: 0.6768\n",
      "Epoch 114/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6434 - mse: 0.6434 - val_loss: 0.6827 - val_mse: 0.6827\n",
      "Epoch 115/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6429 - mse: 0.6429 - val_loss: 0.6730 - val_mse: 0.6730\n",
      "Epoch 116/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6425 - mse: 0.6425 - val_loss: 0.6764 - val_mse: 0.6764\n",
      "Epoch 117/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6425 - mse: 0.6425 - val_loss: 0.6802 - val_mse: 0.6802\n",
      "Epoch 118/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6414 - mse: 0.6414 - val_loss: 0.6783 - val_mse: 0.6783\n",
      "Epoch 119/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6418 - mse: 0.6418 - val_loss: 0.6823 - val_mse: 0.6823\n",
      "Epoch 120/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6415 - mse: 0.6415 - val_loss: 0.6750 - val_mse: 0.6750\n",
      "Epoch 121/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6410 - mse: 0.6410 - val_loss: 0.6702 - val_mse: 0.6702\n",
      "Epoch 122/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6399 - mse: 0.6399 - val_loss: 0.6696 - val_mse: 0.6696\n",
      "Epoch 123/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6397 - mse: 0.6397 - val_loss: 0.6697 - val_mse: 0.6697\n",
      "Epoch 124/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6391 - mse: 0.6391 - val_loss: 0.6772 - val_mse: 0.6772\n",
      "Epoch 125/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6392 - mse: 0.6392 - val_loss: 0.6757 - val_mse: 0.6757\n",
      "Epoch 126/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6383 - mse: 0.6383 - val_loss: 0.6828 - val_mse: 0.6828\n",
      "Epoch 127/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6380 - mse: 0.6380 - val_loss: 0.6758 - val_mse: 0.6758\n",
      "Epoch 128/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6380 - mse: 0.6380 - val_loss: 0.6668 - val_mse: 0.6668\n",
      "Epoch 129/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6368 - mse: 0.6368 - val_loss: 0.6744 - val_mse: 0.6744\n",
      "Epoch 130/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6366 - mse: 0.6366 - val_loss: 0.6733 - val_mse: 0.6733\n",
      "Epoch 131/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6368 - mse: 0.6368 - val_loss: 0.6747 - val_mse: 0.6747\n",
      "Epoch 132/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6366 - mse: 0.6366 - val_loss: 0.6721 - val_mse: 0.6721\n",
      "Epoch 133/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6358 - mse: 0.6358 - val_loss: 0.6759 - val_mse: 0.6759\n",
      "Epoch 134/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6350 - mse: 0.6350 - val_loss: 0.6757 - val_mse: 0.6757\n",
      "Epoch 135/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6348 - mse: 0.6348 - val_loss: 0.6748 - val_mse: 0.6748\n",
      "Epoch 136/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6343 - mse: 0.6343 - val_loss: 0.6942 - val_mse: 0.6942\n",
      "Epoch 137/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6340 - mse: 0.6340 - val_loss: 0.6666 - val_mse: 0.6666\n",
      "Epoch 138/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6336 - mse: 0.6336 - val_loss: 0.6702 - val_mse: 0.6702\n",
      "Epoch 139/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6330 - mse: 0.6330 - val_loss: 0.6765 - val_mse: 0.6765\n",
      "Epoch 140/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6331 - mse: 0.6331 - val_loss: 0.6745 - val_mse: 0.6745\n",
      "Epoch 141/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6321 - mse: 0.6321 - val_loss: 0.6683 - val_mse: 0.6683\n",
      "Epoch 142/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6317 - mse: 0.6317 - val_loss: 0.6789 - val_mse: 0.6789\n",
      "Epoch 143/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6316 - mse: 0.6316 - val_loss: 0.6911 - val_mse: 0.6911\n",
      "Epoch 144/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6318 - mse: 0.6318 - val_loss: 0.6946 - val_mse: 0.6946\n",
      "Epoch 145/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6310 - mse: 0.6310 - val_loss: 0.6814 - val_mse: 0.6814\n",
      "Epoch 146/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6306 - mse: 0.6306 - val_loss: 0.6754 - val_mse: 0.6754\n",
      "Epoch 147/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6301 - mse: 0.6301 - val_loss: 0.6726 - val_mse: 0.6726\n",
      "Epoch 148/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6298 - mse: 0.6298 - val_loss: 0.6703 - val_mse: 0.6703\n",
      "Epoch 149/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6291 - mse: 0.6291 - val_loss: 0.6647 - val_mse: 0.6647\n",
      "Epoch 150/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6296 - mse: 0.6296 - val_loss: 0.6738 - val_mse: 0.6738\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8430 - mse: 0.8430 - val_loss: 0.7741 - val_mse: 0.7741\n",
      "Epoch 2/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7405 - mse: 0.7405 - val_loss: 0.7744 - val_mse: 0.7744\n",
      "Epoch 3/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7284 - mse: 0.7284 - val_loss: 0.7661 - val_mse: 0.7661\n",
      "Epoch 4/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7202 - mse: 0.7202 - val_loss: 0.7618 - val_mse: 0.7618\n",
      "Epoch 5/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7130 - mse: 0.7130 - val_loss: 0.7594 - val_mse: 0.7594\n",
      "Epoch 6/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7067 - mse: 0.7067 - val_loss: 0.7548 - val_mse: 0.7548\n",
      "Epoch 7/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7011 - mse: 0.7011 - val_loss: 0.7561 - val_mse: 0.7561\n",
      "Epoch 8/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6959 - mse: 0.6959 - val_loss: 0.7552 - val_mse: 0.7552\n",
      "Epoch 9/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6911 - mse: 0.6911 - val_loss: 0.7459 - val_mse: 0.7459\n",
      "Epoch 10/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6864 - mse: 0.6864 - val_loss: 0.7493 - val_mse: 0.7493\n",
      "Epoch 11/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6827 - mse: 0.6827 - val_loss: 0.7440 - val_mse: 0.7440\n",
      "Epoch 12/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6792 - mse: 0.6792 - val_loss: 0.7438 - val_mse: 0.7438\n",
      "Epoch 13/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6757 - mse: 0.6757 - val_loss: 0.7403 - val_mse: 0.7403\n",
      "Epoch 14/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6727 - mse: 0.6727 - val_loss: 0.7435 - val_mse: 0.7435\n",
      "Epoch 15/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6706 - mse: 0.6706 - val_loss: 0.7409 - val_mse: 0.7409\n",
      "Epoch 16/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6675 - mse: 0.6675 - val_loss: 0.7396 - val_mse: 0.7396\n",
      "Epoch 17/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6654 - mse: 0.6654 - val_loss: 0.7364 - val_mse: 0.7364\n",
      "Epoch 18/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6631 - mse: 0.6631 - val_loss: 0.7351 - val_mse: 0.7351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6618 - mse: 0.6618 - val_loss: 0.7318 - val_mse: 0.7318\n",
      "Epoch 20/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6591 - mse: 0.6591 - val_loss: 0.7332 - val_mse: 0.7332\n",
      "Epoch 21/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6585 - mse: 0.6585 - val_loss: 0.7326 - val_mse: 0.7326\n",
      "Epoch 22/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6562 - mse: 0.6562 - val_loss: 0.7309 - val_mse: 0.7309\n",
      "Epoch 23/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6549 - mse: 0.6549 - val_loss: 0.7368 - val_mse: 0.7368\n",
      "Epoch 24/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6543 - mse: 0.6543 - val_loss: 0.7309 - val_mse: 0.7309\n",
      "Epoch 25/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6522 - mse: 0.6522 - val_loss: 0.7237 - val_mse: 0.7237\n",
      "Epoch 26/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6511 - mse: 0.6511 - val_loss: 0.7273 - val_mse: 0.7273\n",
      "Epoch 27/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6503 - mse: 0.6503 - val_loss: 0.7262 - val_mse: 0.7262\n",
      "Epoch 28/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6486 - mse: 0.6486 - val_loss: 0.7234 - val_mse: 0.7234\n",
      "Epoch 29/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6474 - mse: 0.6474 - val_loss: 0.7216 - val_mse: 0.7216\n",
      "Epoch 30/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6464 - mse: 0.6464 - val_loss: 0.7194 - val_mse: 0.7194\n",
      "Epoch 31/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6454 - mse: 0.6454 - val_loss: 0.7242 - val_mse: 0.7242\n",
      "Epoch 32/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6441 - mse: 0.6441 - val_loss: 0.7198 - val_mse: 0.7198\n",
      "Epoch 33/150\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6431 - mse: 0.6431 - val_loss: 0.7169 - val_mse: 0.7169\n",
      "Epoch 34/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6422 - mse: 0.6422 - val_loss: 0.7173 - val_mse: 0.7173\n",
      "Epoch 35/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6412 - mse: 0.6412 - val_loss: 0.7179 - val_mse: 0.7179\n",
      "Epoch 36/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6403 - mse: 0.6403 - val_loss: 0.7150 - val_mse: 0.7150\n",
      "Epoch 37/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6390 - mse: 0.6390 - val_loss: 0.7222 - val_mse: 0.7222\n",
      "Epoch 38/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6384 - mse: 0.6384 - val_loss: 0.7117 - val_mse: 0.7117\n",
      "Epoch 39/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6373 - mse: 0.6373 - val_loss: 0.7169 - val_mse: 0.7169\n",
      "Epoch 40/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6368 - mse: 0.6368 - val_loss: 0.7139 - val_mse: 0.7139\n",
      "Epoch 41/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6356 - mse: 0.6356 - val_loss: 0.7283 - val_mse: 0.7283\n",
      "Epoch 42/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6348 - mse: 0.6348 - val_loss: 0.7200 - val_mse: 0.7200\n",
      "Epoch 43/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6337 - mse: 0.6337 - val_loss: 0.7151 - val_mse: 0.7151\n",
      "Epoch 44/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6328 - mse: 0.6328 - val_loss: 0.7167 - val_mse: 0.7167\n",
      "Epoch 45/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6330 - mse: 0.6330 - val_loss: 0.7158 - val_mse: 0.7158\n",
      "Epoch 46/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6313 - mse: 0.6313 - val_loss: 0.7128 - val_mse: 0.7128\n",
      "Epoch 47/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6309 - mse: 0.6309 - val_loss: 0.7145 - val_mse: 0.7145\n",
      "Epoch 48/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6299 - mse: 0.6299 - val_loss: 0.7068 - val_mse: 0.7068\n",
      "Epoch 49/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6289 - mse: 0.6289 - val_loss: 0.7096 - val_mse: 0.7096\n",
      "Epoch 50/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6282 - mse: 0.6282 - val_loss: 0.7120 - val_mse: 0.7120\n",
      "Epoch 51/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6273 - mse: 0.6273 - val_loss: 0.7094 - val_mse: 0.7094\n",
      "Epoch 52/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6270 - mse: 0.6270 - val_loss: 0.7171 - val_mse: 0.7171\n",
      "Epoch 53/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6267 - mse: 0.6267 - val_loss: 0.7156 - val_mse: 0.7156\n",
      "Epoch 54/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6249 - mse: 0.6249 - val_loss: 0.7120 - val_mse: 0.7120\n",
      "Epoch 55/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6244 - mse: 0.6244 - val_loss: 0.7063 - val_mse: 0.7063\n",
      "Epoch 56/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6234 - mse: 0.6234 - val_loss: 0.7088 - val_mse: 0.7088\n",
      "Epoch 57/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6229 - mse: 0.6229 - val_loss: 0.7014 - val_mse: 0.7014\n",
      "Epoch 58/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6225 - mse: 0.6225 - val_loss: 0.7060 - val_mse: 0.7060\n",
      "Epoch 59/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6216 - mse: 0.6216 - val_loss: 0.7108 - val_mse: 0.7108\n",
      "Epoch 60/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6216 - mse: 0.6216 - val_loss: 0.7077 - val_mse: 0.7077\n",
      "Epoch 61/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6202 - mse: 0.6202 - val_loss: 0.7095 - val_mse: 0.7095\n",
      "Epoch 62/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6196 - mse: 0.6196 - val_loss: 0.7121 - val_mse: 0.7121\n",
      "Epoch 63/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6193 - mse: 0.6193 - val_loss: 0.6994 - val_mse: 0.6994\n",
      "Epoch 64/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6185 - mse: 0.6185 - val_loss: 0.7046 - val_mse: 0.7046\n",
      "Epoch 65/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6178 - mse: 0.6178 - val_loss: 0.7010 - val_mse: 0.7010\n",
      "Epoch 66/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6173 - mse: 0.6173 - val_loss: 0.7100 - val_mse: 0.7100\n",
      "Epoch 67/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6172 - mse: 0.6172 - val_loss: 0.7100 - val_mse: 0.7100\n",
      "Epoch 68/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6157 - mse: 0.6157 - val_loss: 0.7058 - val_mse: 0.7058\n",
      "Epoch 69/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6152 - mse: 0.6152 - val_loss: 0.7035 - val_mse: 0.7035\n",
      "Epoch 70/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6152 - mse: 0.6152 - val_loss: 0.7043 - val_mse: 0.7043\n",
      "Epoch 71/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6143 - mse: 0.6143 - val_loss: 0.6979 - val_mse: 0.6979\n",
      "Epoch 72/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6134 - mse: 0.6134 - val_loss: 0.6974 - val_mse: 0.6974\n",
      "Epoch 73/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6127 - mse: 0.6127 - val_loss: 0.6999 - val_mse: 0.6999\n",
      "Epoch 74/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6125 - mse: 0.6125 - val_loss: 0.7103 - val_mse: 0.7103\n",
      "Epoch 75/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6123 - mse: 0.6123 - val_loss: 0.6980 - val_mse: 0.6980\n",
      "Epoch 76/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6119 - mse: 0.6119 - val_loss: 0.6956 - val_mse: 0.6956\n",
      "Epoch 77/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6107 - mse: 0.6107 - val_loss: 0.6974 - val_mse: 0.6973\n",
      "Epoch 78/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6110 - mse: 0.6110 - val_loss: 0.7054 - val_mse: 0.7054\n",
      "Epoch 79/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6097 - mse: 0.6097 - val_loss: 0.6951 - val_mse: 0.6951\n",
      "Epoch 80/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6093 - mse: 0.6093 - val_loss: 0.6950 - val_mse: 0.6950\n",
      "Epoch 81/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6088 - mse: 0.6088 - val_loss: 0.6975 - val_mse: 0.6975\n",
      "Epoch 82/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6081 - mse: 0.6081 - val_loss: 0.6987 - val_mse: 0.6987\n",
      "Epoch 83/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6080 - mse: 0.6080 - val_loss: 0.7027 - val_mse: 0.7027\n",
      "Epoch 84/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6070 - mse: 0.6070 - val_loss: 0.6910 - val_mse: 0.6910\n",
      "Epoch 85/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6070 - mse: 0.6070 - val_loss: 0.6952 - val_mse: 0.6952\n",
      "Epoch 86/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6061 - mse: 0.6061 - val_loss: 0.6950 - val_mse: 0.6950\n",
      "Epoch 87/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6062 - mse: 0.6062 - val_loss: 0.7017 - val_mse: 0.7017\n",
      "Epoch 88/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6056 - mse: 0.6055 - val_loss: 0.6951 - val_mse: 0.6951\n",
      "Epoch 89/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6051 - mse: 0.6051 - val_loss: 0.6908 - val_mse: 0.6908\n",
      "Epoch 90/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6044 - mse: 0.6044 - val_loss: 0.7001 - val_mse: 0.7001\n",
      "Epoch 91/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6041 - mse: 0.6041 - val_loss: 0.6961 - val_mse: 0.6961\n",
      "Epoch 92/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6039 - mse: 0.6039 - val_loss: 0.6927 - val_mse: 0.6927\n",
      "Epoch 93/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6032 - mse: 0.6032 - val_loss: 0.6989 - val_mse: 0.6989\n",
      "Epoch 94/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6026 - mse: 0.6026 - val_loss: 0.6943 - val_mse: 0.6943\n",
      "Epoch 95/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6026 - mse: 0.6026 - val_loss: 0.6878 - val_mse: 0.6878\n",
      "Epoch 96/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6015 - mse: 0.6015 - val_loss: 0.6875 - val_mse: 0.6875\n",
      "Epoch 97/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6012 - mse: 0.6012 - val_loss: 0.6935 - val_mse: 0.6935\n",
      "Epoch 98/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6011 - mse: 0.6011 - val_loss: 0.6905 - val_mse: 0.6905\n",
      "Epoch 99/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6014 - mse: 0.6014 - val_loss: 0.7011 - val_mse: 0.7011\n",
      "Epoch 100/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6003 - mse: 0.6003 - val_loss: 0.6913 - val_mse: 0.6913\n",
      "Epoch 101/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5997 - mse: 0.5997 - val_loss: 0.6826 - val_mse: 0.6826\n",
      "Epoch 102/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5992 - mse: 0.5992 - val_loss: 0.6851 - val_mse: 0.6851\n",
      "Epoch 103/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5994 - mse: 0.5993 - val_loss: 0.6919 - val_mse: 0.6919\n",
      "Epoch 104/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5988 - mse: 0.5988 - val_loss: 0.6847 - val_mse: 0.6847\n",
      "Epoch 105/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5990 - mse: 0.5990 - val_loss: 0.6860 - val_mse: 0.6860\n",
      "Epoch 106/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5979 - mse: 0.5979 - val_loss: 0.6848 - val_mse: 0.6848\n",
      "Epoch 107/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5983 - mse: 0.5983 - val_loss: 0.6902 - val_mse: 0.6902\n",
      "Epoch 108/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5969 - mse: 0.5969 - val_loss: 0.6858 - val_mse: 0.6858\n",
      "Epoch 109/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5966 - mse: 0.5966 - val_loss: 0.6894 - val_mse: 0.6894\n",
      "Epoch 110/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5964 - mse: 0.5964 - val_loss: 0.6874 - val_mse: 0.6874\n",
      "Epoch 111/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5960 - mse: 0.5960 - val_loss: 0.6834 - val_mse: 0.6834\n",
      "Epoch 112/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5957 - mse: 0.5957 - val_loss: 0.6930 - val_mse: 0.6930\n",
      "Epoch 113/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5951 - mse: 0.5951 - val_loss: 0.6843 - val_mse: 0.6843\n",
      "Epoch 114/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5950 - mse: 0.5950 - val_loss: 0.6870 - val_mse: 0.6870\n",
      "Epoch 115/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5944 - mse: 0.5944 - val_loss: 0.6836 - val_mse: 0.6836\n",
      "Epoch 116/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5944 - mse: 0.5944 - val_loss: 0.6800 - val_mse: 0.6800\n",
      "Epoch 117/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5947 - mse: 0.5947 - val_loss: 0.6839 - val_mse: 0.6839\n",
      "Epoch 118/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5939 - mse: 0.5939 - val_loss: 0.6836 - val_mse: 0.6836\n",
      "Epoch 119/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5931 - mse: 0.5931 - val_loss: 0.6889 - val_mse: 0.6889\n",
      "Epoch 120/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5932 - mse: 0.5932 - val_loss: 0.6847 - val_mse: 0.6847\n",
      "Epoch 121/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5930 - mse: 0.5930 - val_loss: 0.6892 - val_mse: 0.6892\n",
      "Epoch 122/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5921 - mse: 0.5921 - val_loss: 0.6797 - val_mse: 0.6797\n",
      "Epoch 123/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5924 - mse: 0.5924 - val_loss: 0.6855 - val_mse: 0.6855\n",
      "Epoch 124/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5916 - mse: 0.5916 - val_loss: 0.6928 - val_mse: 0.6928\n",
      "Epoch 125/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5911 - mse: 0.5911 - val_loss: 0.6896 - val_mse: 0.6896\n",
      "Epoch 126/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5909 - mse: 0.5909 - val_loss: 0.6854 - val_mse: 0.6854\n",
      "Epoch 127/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5906 - mse: 0.5906 - val_loss: 0.6827 - val_mse: 0.6827\n",
      "Epoch 128/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5908 - mse: 0.5908 - val_loss: 0.6801 - val_mse: 0.6801\n",
      "Epoch 129/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5902 - mse: 0.5902 - val_loss: 0.6819 - val_mse: 0.6819\n",
      "Epoch 130/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5898 - mse: 0.5898 - val_loss: 0.6910 - val_mse: 0.6910\n",
      "Epoch 131/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5893 - mse: 0.5893 - val_loss: 0.6840 - val_mse: 0.6840\n",
      "Epoch 132/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5894 - mse: 0.5894 - val_loss: 0.6791 - val_mse: 0.6791\n",
      "Epoch 133/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5888 - mse: 0.5888 - val_loss: 0.6761 - val_mse: 0.6761\n",
      "Epoch 134/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5890 - mse: 0.5890 - val_loss: 0.6869 - val_mse: 0.6869\n",
      "Epoch 135/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5883 - mse: 0.5883 - val_loss: 0.6813 - val_mse: 0.6813\n",
      "Epoch 136/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5882 - mse: 0.5882 - val_loss: 0.6783 - val_mse: 0.6783\n",
      "Epoch 137/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5874 - mse: 0.5874 - val_loss: 0.6798 - val_mse: 0.6798\n",
      "Epoch 138/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5874 - mse: 0.5874 - val_loss: 0.6953 - val_mse: 0.6953\n",
      "Epoch 139/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5872 - mse: 0.5872 - val_loss: 0.6845 - val_mse: 0.6845\n",
      "Epoch 140/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5868 - mse: 0.5868 - val_loss: 0.6824 - val_mse: 0.6824\n",
      "Epoch 141/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5869 - mse: 0.5869 - val_loss: 0.6811 - val_mse: 0.6811\n",
      "Epoch 142/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5864 - mse: 0.5864 - val_loss: 0.6861 - val_mse: 0.6861\n",
      "Epoch 143/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5860 - mse: 0.5860 - val_loss: 0.6770 - val_mse: 0.6770\n",
      "Epoch 144/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5858 - mse: 0.5858 - val_loss: 0.6824 - val_mse: 0.6824\n",
      "Epoch 145/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5856 - mse: 0.5856 - val_loss: 0.6750 - val_mse: 0.6750\n",
      "Epoch 146/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5855 - mse: 0.5855 - val_loss: 0.6820 - val_mse: 0.6820\n",
      "Epoch 147/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5849 - mse: 0.5849 - val_loss: 0.6826 - val_mse: 0.6826\n",
      "Epoch 148/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5847 - mse: 0.5847 - val_loss: 0.6797 - val_mse: 0.6797\n",
      "Epoch 149/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5843 - mse: 0.5843 - val_loss: 0.6854 - val_mse: 0.6854\n",
      "Epoch 150/150\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.5840 - mse: 0.5840 - val_loss: 0.6748 - val_mse: 0.6748\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.9718 - mse: 0.9718 - val_loss: 0.7999 - val_mse: 0.7999\n",
      "Epoch 2/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8537 - mse: 0.8537 - val_loss: 0.8599 - val_mse: 0.8599\n",
      "Epoch 3/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8352 - mse: 0.8352 - val_loss: 0.8339 - val_mse: 0.8339\n",
      "Epoch 4/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8235 - mse: 0.8235 - val_loss: 0.8596 - val_mse: 0.8596\n",
      "Epoch 5/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8135 - mse: 0.8135 - val_loss: 0.8202 - val_mse: 0.8202\n",
      "Epoch 6/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8052 - mse: 0.8053 - val_loss: 0.8154 - val_mse: 0.8154\n",
      "Epoch 7/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7974 - mse: 0.7974 - val_loss: 0.8235 - val_mse: 0.8235\n",
      "Epoch 8/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7909 - mse: 0.7909 - val_loss: 0.8207 - val_mse: 0.8207\n",
      "Epoch 9/150\n",
      "3353318/3353318 [==============================] - 7s 2us/step - loss: 0.7848 - mse: 0.7848 - val_loss: 0.8076 - val_mse: 0.8076\n",
      "Epoch 10/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7803 - mse: 0.7803 - val_loss: 0.7960 - val_mse: 0.7960\n",
      "Epoch 11/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7751 - mse: 0.7751 - val_loss: 0.7950 - val_mse: 0.7950\n",
      "Epoch 12/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7713 - mse: 0.7713 - val_loss: 0.7842 - val_mse: 0.7842\n",
      "Epoch 13/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7683 - mse: 0.7683 - val_loss: 0.7828 - val_mse: 0.7828\n",
      "Epoch 14/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7641 - mse: 0.7641 - val_loss: 0.7698 - val_mse: 0.7698\n",
      "Epoch 15/150\n",
      "3353318/3353318 [==============================] - 7s 2us/step - loss: 0.7618 - mse: 0.7618 - val_loss: 0.7739 - val_mse: 0.7739\n",
      "Epoch 16/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7588 - mse: 0.7588 - val_loss: 0.7817 - val_mse: 0.7817\n",
      "Epoch 17/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7563 - mse: 0.7563 - val_loss: 0.7702 - val_mse: 0.7702\n",
      "Epoch 18/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7545 - mse: 0.7545 - val_loss: 0.7629 - val_mse: 0.7629\n",
      "Epoch 19/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7525 - mse: 0.7525 - val_loss: 0.7741 - val_mse: 0.7741\n",
      "Epoch 20/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7506 - mse: 0.7506 - val_loss: 0.7690 - val_mse: 0.7690\n",
      "Epoch 21/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7484 - mse: 0.7484 - val_loss: 0.7560 - val_mse: 0.7560\n",
      "Epoch 22/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7467 - mse: 0.7467 - val_loss: 0.7615 - val_mse: 0.7615\n",
      "Epoch 23/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7454 - mse: 0.7454 - val_loss: 0.7545 - val_mse: 0.7545\n",
      "Epoch 24/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7434 - mse: 0.7434 - val_loss: 0.7620 - val_mse: 0.7620\n",
      "Epoch 25/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7421 - mse: 0.7421 - val_loss: 0.7606 - val_mse: 0.7606\n",
      "Epoch 26/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7414 - mse: 0.7414 - val_loss: 0.7470 - val_mse: 0.7470\n",
      "Epoch 27/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7399 - mse: 0.7399 - val_loss: 0.7560 - val_mse: 0.7560\n",
      "Epoch 28/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7380 - mse: 0.7380 - val_loss: 0.7689 - val_mse: 0.7689\n",
      "Epoch 29/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7372 - mse: 0.7372 - val_loss: 0.7606 - val_mse: 0.7606\n",
      "Epoch 30/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7361 - mse: 0.7361 - val_loss: 0.7538 - val_mse: 0.7538\n",
      "Epoch 31/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7340 - mse: 0.7340 - val_loss: 0.7589 - val_mse: 0.7589\n",
      "Epoch 32/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7338 - mse: 0.7338 - val_loss: 0.7608 - val_mse: 0.7608\n",
      "Epoch 33/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7323 - mse: 0.7323 - val_loss: 0.7496 - val_mse: 0.7496\n",
      "Epoch 34/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7312 - mse: 0.7312 - val_loss: 0.7504 - val_mse: 0.7504\n",
      "Epoch 35/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7304 - mse: 0.7304 - val_loss: 0.7666 - val_mse: 0.7666\n",
      "Epoch 36/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7290 - mse: 0.7290 - val_loss: 0.7559 - val_mse: 0.7559\n",
      "Epoch 37/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7284 - mse: 0.7284 - val_loss: 0.7541 - val_mse: 0.7541\n",
      "Epoch 38/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7277 - mse: 0.7277 - val_loss: 0.7516 - val_mse: 0.7516\n",
      "Epoch 39/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7260 - mse: 0.7260 - val_loss: 0.7411 - val_mse: 0.7411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7259 - mse: 0.7259 - val_loss: 0.7440 - val_mse: 0.7440\n",
      "Epoch 41/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7236 - mse: 0.7236 - val_loss: 0.7477 - val_mse: 0.7477\n",
      "Epoch 42/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7228 - mse: 0.7228 - val_loss: 0.7467 - val_mse: 0.7467\n",
      "Epoch 43/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7217 - mse: 0.7217 - val_loss: 0.7480 - val_mse: 0.7480\n",
      "Epoch 44/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7217 - mse: 0.7217 - val_loss: 0.7423 - val_mse: 0.7423\n",
      "Epoch 45/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7202 - mse: 0.7202 - val_loss: 0.7547 - val_mse: 0.7547\n",
      "Epoch 46/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7192 - mse: 0.7192 - val_loss: 0.7406 - val_mse: 0.7406\n",
      "Epoch 47/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7190 - mse: 0.7190 - val_loss: 0.7446 - val_mse: 0.7446\n",
      "Epoch 48/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7173 - mse: 0.7173 - val_loss: 0.7463 - val_mse: 0.7463\n",
      "Epoch 49/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7170 - mse: 0.7170 - val_loss: 0.7392 - val_mse: 0.7392\n",
      "Epoch 50/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7158 - mse: 0.7158 - val_loss: 0.7427 - val_mse: 0.7427\n",
      "Epoch 51/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7149 - mse: 0.7149 - val_loss: 0.7469 - val_mse: 0.7469\n",
      "Epoch 52/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7138 - mse: 0.7138 - val_loss: 0.7439 - val_mse: 0.7439\n",
      "Epoch 53/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7126 - mse: 0.7126 - val_loss: 0.7452 - val_mse: 0.7452\n",
      "Epoch 54/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7123 - mse: 0.7123 - val_loss: 0.7560 - val_mse: 0.7560\n",
      "Epoch 55/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7112 - mse: 0.7112 - val_loss: 0.7421 - val_mse: 0.7421\n",
      "Epoch 56/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7111 - mse: 0.7111 - val_loss: 0.7597 - val_mse: 0.7597\n",
      "Epoch 57/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7098 - mse: 0.7098 - val_loss: 0.7402 - val_mse: 0.7402\n",
      "Epoch 58/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7095 - mse: 0.7095 - val_loss: 0.7393 - val_mse: 0.7393\n",
      "Epoch 59/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7084 - mse: 0.7084 - val_loss: 0.7534 - val_mse: 0.7534\n",
      "Epoch 60/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7077 - mse: 0.7077 - val_loss: 0.7464 - val_mse: 0.7464\n",
      "Epoch 61/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7071 - mse: 0.7071 - val_loss: 0.7358 - val_mse: 0.7358\n",
      "Epoch 62/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7057 - mse: 0.7057 - val_loss: 0.7353 - val_mse: 0.7353\n",
      "Epoch 63/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7050 - mse: 0.7050 - val_loss: 0.7415 - val_mse: 0.7415\n",
      "Epoch 64/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7044 - mse: 0.7044 - val_loss: 0.7460 - val_mse: 0.7460\n",
      "Epoch 65/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7036 - mse: 0.7036 - val_loss: 0.7357 - val_mse: 0.7357\n",
      "Epoch 66/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7029 - mse: 0.7029 - val_loss: 0.7354 - val_mse: 0.7354\n",
      "Epoch 67/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7020 - mse: 0.7020 - val_loss: 0.7440 - val_mse: 0.7440\n",
      "Epoch 68/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7015 - mse: 0.7015 - val_loss: 0.7419 - val_mse: 0.7419\n",
      "Epoch 69/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7009 - mse: 0.7009 - val_loss: 0.7384 - val_mse: 0.7384\n",
      "Epoch 70/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6998 - mse: 0.6998 - val_loss: 0.7361 - val_mse: 0.7361\n",
      "Epoch 71/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6988 - mse: 0.6988 - val_loss: 0.7444 - val_mse: 0.7444\n",
      "Epoch 72/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6980 - mse: 0.6980 - val_loss: 0.7340 - val_mse: 0.7340\n",
      "Epoch 73/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6983 - mse: 0.6983 - val_loss: 0.7430 - val_mse: 0.7430\n",
      "Epoch 74/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6969 - mse: 0.6969 - val_loss: 0.7374 - val_mse: 0.7374\n",
      "Epoch 75/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6965 - mse: 0.6965 - val_loss: 0.7337 - val_mse: 0.7337\n",
      "Epoch 76/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6960 - mse: 0.6960 - val_loss: 0.7436 - val_mse: 0.7436\n",
      "Epoch 77/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6951 - mse: 0.6951 - val_loss: 0.7376 - val_mse: 0.7376\n",
      "Epoch 78/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6942 - mse: 0.6942 - val_loss: 0.7346 - val_mse: 0.7346\n",
      "Epoch 79/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6940 - mse: 0.6940 - val_loss: 0.7425 - val_mse: 0.7425\n",
      "Epoch 80/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6931 - mse: 0.6931 - val_loss: 0.7421 - val_mse: 0.7421\n",
      "Epoch 81/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6922 - mse: 0.6922 - val_loss: 0.7351 - val_mse: 0.7351\n",
      "Epoch 82/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6917 - mse: 0.6917 - val_loss: 0.7412 - val_mse: 0.7412\n",
      "Epoch 83/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6915 - mse: 0.6915 - val_loss: 0.7416 - val_mse: 0.7416\n",
      "Epoch 84/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6904 - mse: 0.6904 - val_loss: 0.7420 - val_mse: 0.7420\n",
      "Epoch 85/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6904 - mse: 0.6904 - val_loss: 0.7390 - val_mse: 0.7390\n",
      "Epoch 86/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6890 - mse: 0.6890 - val_loss: 0.7387 - val_mse: 0.7387\n",
      "Epoch 87/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6889 - mse: 0.6889 - val_loss: 0.7366 - val_mse: 0.7366\n",
      "Epoch 88/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6880 - mse: 0.6880 - val_loss: 0.7363 - val_mse: 0.7363\n",
      "Epoch 89/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6875 - mse: 0.6875 - val_loss: 0.7398 - val_mse: 0.7398\n",
      "Epoch 90/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6873 - mse: 0.6873 - val_loss: 0.7432 - val_mse: 0.7432\n",
      "Epoch 91/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6867 - mse: 0.6867 - val_loss: 0.7374 - val_mse: 0.7374\n",
      "Epoch 92/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6855 - mse: 0.6855 - val_loss: 0.7407 - val_mse: 0.7407\n",
      "Epoch 93/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6851 - mse: 0.6851 - val_loss: 0.7538 - val_mse: 0.7538\n",
      "Epoch 94/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6852 - mse: 0.6852 - val_loss: 0.7443 - val_mse: 0.7443\n",
      "Epoch 95/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6843 - mse: 0.6843 - val_loss: 0.7369 - val_mse: 0.7369\n",
      "Epoch 96/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6837 - mse: 0.6837 - val_loss: 0.7358 - val_mse: 0.7358\n",
      "Epoch 97/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6834 - mse: 0.6834 - val_loss: 0.7310 - val_mse: 0.7310\n",
      "Epoch 98/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6822 - mse: 0.6822 - val_loss: 0.7384 - val_mse: 0.7384\n",
      "Epoch 99/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6818 - mse: 0.6818 - val_loss: 0.7438 - val_mse: 0.7438\n",
      "Epoch 100/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6821 - mse: 0.6821 - val_loss: 0.7395 - val_mse: 0.7395\n",
      "Epoch 101/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6809 - mse: 0.6809 - val_loss: 0.7585 - val_mse: 0.7585\n",
      "Epoch 102/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6802 - mse: 0.6802 - val_loss: 0.7445 - val_mse: 0.7445\n",
      "Epoch 103/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6792 - mse: 0.6792 - val_loss: 0.7381 - val_mse: 0.7381\n",
      "Epoch 104/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6790 - mse: 0.6790 - val_loss: 0.7585 - val_mse: 0.7585\n",
      "Epoch 105/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6787 - mse: 0.6787 - val_loss: 0.7372 - val_mse: 0.7372\n",
      "Epoch 106/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6783 - mse: 0.6783 - val_loss: 0.7370 - val_mse: 0.7370\n",
      "Epoch 107/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6775 - mse: 0.6775 - val_loss: 0.7374 - val_mse: 0.7374\n",
      "Epoch 108/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6770 - mse: 0.6770 - val_loss: 0.7401 - val_mse: 0.7401\n",
      "Epoch 109/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6765 - mse: 0.6765 - val_loss: 0.7377 - val_mse: 0.7377\n",
      "Epoch 110/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6760 - mse: 0.6760 - val_loss: 0.7300 - val_mse: 0.7300\n",
      "Epoch 111/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6755 - mse: 0.6755 - val_loss: 0.7511 - val_mse: 0.7511\n",
      "Epoch 112/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6756 - mse: 0.6756 - val_loss: 0.7499 - val_mse: 0.7499\n",
      "Epoch 113/150\n",
      "3353318/3353318 [==============================] - 7s 2us/step - loss: 0.6744 - mse: 0.6744 - val_loss: 0.7493 - val_mse: 0.7493\n",
      "Epoch 114/150\n",
      "3353318/3353318 [==============================] - 7s 2us/step - loss: 0.6748 - mse: 0.6748 - val_loss: 0.7428 - val_mse: 0.7428\n",
      "Epoch 115/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6743 - mse: 0.6743 - val_loss: 0.7381 - val_mse: 0.7381\n",
      "Epoch 116/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6728 - mse: 0.6728 - val_loss: 0.7529 - val_mse: 0.7529\n",
      "Epoch 117/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6724 - mse: 0.6724 - val_loss: 0.7341 - val_mse: 0.7341\n",
      "Epoch 118/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6725 - mse: 0.6725 - val_loss: 0.7351 - val_mse: 0.7351\n",
      "Epoch 119/150\n",
      "3353318/3353318 [==============================] - 7s 2us/step - loss: 0.6714 - mse: 0.6714 - val_loss: 0.7378 - val_mse: 0.7378\n",
      "Epoch 120/150\n",
      "3353318/3353318 [==============================] - 7s 2us/step - loss: 0.6719 - mse: 0.6719 - val_loss: 0.7471 - val_mse: 0.7471\n",
      "Epoch 121/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6705 - mse: 0.6705 - val_loss: 0.7496 - val_mse: 0.7496\n",
      "Epoch 122/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6703 - mse: 0.6703 - val_loss: 0.7472 - val_mse: 0.7472\n",
      "Epoch 123/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6697 - mse: 0.6697 - val_loss: 0.7509 - val_mse: 0.7509\n",
      "Epoch 124/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6698 - mse: 0.6698 - val_loss: 0.7397 - val_mse: 0.7397\n",
      "Epoch 125/150\n",
      "3353318/3353318 [==============================] - 7s 2us/step - loss: 0.6695 - mse: 0.6695 - val_loss: 0.7453 - val_mse: 0.7453\n",
      "Epoch 126/150\n",
      "3353318/3353318 [==============================] - 7s 2us/step - loss: 0.6683 - mse: 0.6683 - val_loss: 0.7312 - val_mse: 0.7312\n",
      "Epoch 127/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6682 - mse: 0.6682 - val_loss: 0.7426 - val_mse: 0.7426\n",
      "Epoch 128/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6673 - mse: 0.6673 - val_loss: 0.7401 - val_mse: 0.7401\n",
      "Epoch 129/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6671 - mse: 0.6671 - val_loss: 0.7695 - val_mse: 0.7695\n",
      "Epoch 130/150\n",
      "3353318/3353318 [==============================] - 7s 2us/step - loss: 0.6668 - mse: 0.6668 - val_loss: 0.7566 - val_mse: 0.7566\n",
      "Epoch 131/150\n",
      "3353318/3353318 [==============================] - 7s 2us/step - loss: 0.6659 - mse: 0.6659 - val_loss: 0.7404 - val_mse: 0.7404\n",
      "Epoch 132/150\n",
      "3353318/3353318 [==============================] - 7s 2us/step - loss: 0.6659 - mse: 0.6659 - val_loss: 0.7809 - val_mse: 0.7809\n",
      "Epoch 133/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6651 - mse: 0.6651 - val_loss: 0.7546 - val_mse: 0.7546\n",
      "Epoch 134/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6646 - mse: 0.6646 - val_loss: 0.7526 - val_mse: 0.7526\n",
      "Epoch 135/150\n",
      "3353318/3353318 [==============================] - 7s 2us/step - loss: 0.6644 - mse: 0.6644 - val_loss: 0.7429 - val_mse: 0.7429\n",
      "Epoch 136/150\n",
      "3353318/3353318 [==============================] - 7s 2us/step - loss: 0.6638 - mse: 0.6638 - val_loss: 0.7635 - val_mse: 0.7635\n",
      "Epoch 137/150\n",
      "3353318/3353318 [==============================] - 7s 2us/step - loss: 0.6636 - mse: 0.6636 - val_loss: 0.7527 - val_mse: 0.7527\n",
      "Epoch 138/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6627 - mse: 0.6627 - val_loss: 0.7691 - val_mse: 0.7691\n",
      "Epoch 139/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6634 - mse: 0.6634 - val_loss: 0.7610 - val_mse: 0.7610\n",
      "Epoch 140/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6618 - mse: 0.6618 - val_loss: 0.7399 - val_mse: 0.7399\n",
      "Epoch 141/150\n",
      "3353318/3353318 [==============================] - 7s 2us/step - loss: 0.6617 - mse: 0.6617 - val_loss: 0.8319 - val_mse: 0.8319\n",
      "Epoch 142/150\n",
      "3353318/3353318 [==============================] - 7s 2us/step - loss: 0.6620 - mse: 0.6620 - val_loss: 0.7582 - val_mse: 0.7582\n",
      "Epoch 143/150\n",
      "3353318/3353318 [==============================] - 7s 2us/step - loss: 0.6605 - mse: 0.6605 - val_loss: 0.7473 - val_mse: 0.7473\n",
      "Epoch 144/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6598 - mse: 0.6598 - val_loss: 0.7651 - val_mse: 0.7651\n",
      "Epoch 145/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6596 - mse: 0.6596 - val_loss: 0.7550 - val_mse: 0.7550\n",
      "Epoch 146/150\n",
      "3353318/3353318 [==============================] - 7s 2us/step - loss: 0.6592 - mse: 0.6592 - val_loss: 0.7926 - val_mse: 0.7926\n",
      "Epoch 147/150\n",
      "3353318/3353318 [==============================] - 7s 2us/step - loss: 0.6588 - mse: 0.6588 - val_loss: 0.7654 - val_mse: 0.7654\n",
      "Epoch 148/150\n",
      "3353318/3353318 [==============================] - 7s 2us/step - loss: 0.6586 - mse: 0.6586 - val_loss: 0.7622 - val_mse: 0.7622\n",
      "Epoch 149/150\n",
      "3353318/3353318 [==============================] - 7s 2us/step - loss: 0.6581 - mse: 0.6581 - val_loss: 0.7692 - val_mse: 0.7692\n",
      "Epoch 150/150\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6574 - mse: 0.6574 - val_loss: 0.7810 - val_mse: 0.7810\n",
      "1676658/1676658 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/200\n",
      "3353317/3353317 [==============================] - 7s 2us/step - loss: 0.9560 - mse: 0.9560 - val_loss: 0.7756 - val_mse: 0.7756\n",
      "Epoch 2/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8455 - mse: 0.8455 - val_loss: 0.7742 - val_mse: 0.7742\n",
      "Epoch 3/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8267 - mse: 0.8267 - val_loss: 0.7676 - val_mse: 0.7676\n",
      "Epoch 4/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8133 - mse: 0.8133 - val_loss: 0.7631 - val_mse: 0.7631\n",
      "Epoch 5/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8024 - mse: 0.8024 - val_loss: 0.7712 - val_mse: 0.7712\n",
      "Epoch 6/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7926 - mse: 0.7926 - val_loss: 0.7553 - val_mse: 0.7553\n",
      "Epoch 7/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7840 - mse: 0.7840 - val_loss: 0.7569 - val_mse: 0.7569\n",
      "Epoch 8/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7757 - mse: 0.7757 - val_loss: 0.7539 - val_mse: 0.7539\n",
      "Epoch 9/200\n",
      "3353317/3353317 [==============================] - 7s 2us/step - loss: 0.7684 - mse: 0.7684 - val_loss: 0.7450 - val_mse: 0.7450\n",
      "Epoch 10/200\n",
      "3353317/3353317 [==============================] - 7s 2us/step - loss: 0.7613 - mse: 0.7613 - val_loss: 0.7488 - val_mse: 0.7488\n",
      "Epoch 11/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7551 - mse: 0.7551 - val_loss: 0.7596 - val_mse: 0.7596\n",
      "Epoch 12/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7502 - mse: 0.7502 - val_loss: 0.7359 - val_mse: 0.7359\n",
      "Epoch 13/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7452 - mse: 0.7452 - val_loss: 0.7330 - val_mse: 0.7330\n",
      "Epoch 14/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7412 - mse: 0.7412 - val_loss: 0.7350 - val_mse: 0.7350\n",
      "Epoch 15/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7374 - mse: 0.7374 - val_loss: 0.7319 - val_mse: 0.7319\n",
      "Epoch 16/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7341 - mse: 0.7341 - val_loss: 0.7441 - val_mse: 0.7441\n",
      "Epoch 17/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7315 - mse: 0.7315 - val_loss: 0.7374 - val_mse: 0.7374\n",
      "Epoch 18/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7286 - mse: 0.7286 - val_loss: 0.7278 - val_mse: 0.7278\n",
      "Epoch 19/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7264 - mse: 0.7264 - val_loss: 0.7291 - val_mse: 0.7291\n",
      "Epoch 20/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7238 - mse: 0.7238 - val_loss: 0.7233 - val_mse: 0.7233\n",
      "Epoch 21/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7213 - mse: 0.7213 - val_loss: 0.7232 - val_mse: 0.7232\n",
      "Epoch 22/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7200 - mse: 0.7200 - val_loss: 0.7208 - val_mse: 0.7208\n",
      "Epoch 23/200\n",
      "3353317/3353317 [==============================] - 7s 2us/step - loss: 0.7184 - mse: 0.7184 - val_loss: 0.7229 - val_mse: 0.7229\n",
      "Epoch 24/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7163 - mse: 0.7163 - val_loss: 0.7156 - val_mse: 0.7156\n",
      "Epoch 25/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7144 - mse: 0.7144 - val_loss: 0.7203 - val_mse: 0.7203\n",
      "Epoch 26/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7134 - mse: 0.7134 - val_loss: 0.7190 - val_mse: 0.7190\n",
      "Epoch 27/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7121 - mse: 0.7121 - val_loss: 0.7170 - val_mse: 0.7170\n",
      "Epoch 28/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7102 - mse: 0.7102 - val_loss: 0.7190 - val_mse: 0.7190\n",
      "Epoch 29/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7093 - mse: 0.7093 - val_loss: 0.7124 - val_mse: 0.7124\n",
      "Epoch 30/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7072 - mse: 0.7072 - val_loss: 0.7145 - val_mse: 0.7145\n",
      "Epoch 31/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7063 - mse: 0.7063 - val_loss: 0.7098 - val_mse: 0.7098\n",
      "Epoch 32/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7047 - mse: 0.7047 - val_loss: 0.7178 - val_mse: 0.7178\n",
      "Epoch 33/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7034 - mse: 0.7034 - val_loss: 0.7112 - val_mse: 0.7112\n",
      "Epoch 34/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7019 - mse: 0.7019 - val_loss: 0.7172 - val_mse: 0.7172\n",
      "Epoch 35/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7014 - mse: 0.7014 - val_loss: 0.7192 - val_mse: 0.7192\n",
      "Epoch 36/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7003 - mse: 0.7003 - val_loss: 0.7103 - val_mse: 0.7103\n",
      "Epoch 37/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6987 - mse: 0.6987 - val_loss: 0.7052 - val_mse: 0.7052\n",
      "Epoch 38/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6977 - mse: 0.6977 - val_loss: 0.7023 - val_mse: 0.7023\n",
      "Epoch 39/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6961 - mse: 0.6961 - val_loss: 0.7076 - val_mse: 0.7076\n",
      "Epoch 40/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6955 - mse: 0.6955 - val_loss: 0.7222 - val_mse: 0.7222\n",
      "Epoch 41/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6942 - mse: 0.6942 - val_loss: 0.7067 - val_mse: 0.7067\n",
      "Epoch 42/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6935 - mse: 0.6935 - val_loss: 0.7148 - val_mse: 0.7148\n",
      "Epoch 43/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6921 - mse: 0.6921 - val_loss: 0.7100 - val_mse: 0.7100\n",
      "Epoch 44/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6907 - mse: 0.6907 - val_loss: 0.7015 - val_mse: 0.7015\n",
      "Epoch 45/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6900 - mse: 0.6900 - val_loss: 0.6964 - val_mse: 0.6964\n",
      "Epoch 46/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6893 - mse: 0.6893 - val_loss: 0.6971 - val_mse: 0.6971\n",
      "Epoch 47/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6881 - mse: 0.6881 - val_loss: 0.7037 - val_mse: 0.7037\n",
      "Epoch 48/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6874 - mse: 0.6874 - val_loss: 0.7014 - val_mse: 0.7014\n",
      "Epoch 49/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6864 - mse: 0.6864 - val_loss: 0.7113 - val_mse: 0.7113\n",
      "Epoch 50/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6853 - mse: 0.6853 - val_loss: 0.6953 - val_mse: 0.6953\n",
      "Epoch 51/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6847 - mse: 0.6847 - val_loss: 0.7123 - val_mse: 0.7123\n",
      "Epoch 52/200\n",
      "3353317/3353317 [==============================] - 7s 2us/step - loss: 0.6840 - mse: 0.6840 - val_loss: 0.6959 - val_mse: 0.6959\n",
      "Epoch 53/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6825 - mse: 0.6825 - val_loss: 0.7045 - val_mse: 0.7045\n",
      "Epoch 54/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6814 - mse: 0.6814 - val_loss: 0.6923 - val_mse: 0.6923\n",
      "Epoch 55/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6809 - mse: 0.6809 - val_loss: 0.6961 - val_mse: 0.6961\n",
      "Epoch 56/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6799 - mse: 0.6799 - val_loss: 0.6934 - val_mse: 0.6934\n",
      "Epoch 57/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6788 - mse: 0.6788 - val_loss: 0.7088 - val_mse: 0.7088\n",
      "Epoch 58/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6786 - mse: 0.6786 - val_loss: 0.6927 - val_mse: 0.6927\n",
      "Epoch 59/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6773 - mse: 0.6773 - val_loss: 0.6924 - val_mse: 0.6924\n",
      "Epoch 60/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6766 - mse: 0.6766 - val_loss: 0.7064 - val_mse: 0.7064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6756 - mse: 0.6756 - val_loss: 0.6991 - val_mse: 0.6991\n",
      "Epoch 62/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6756 - mse: 0.6756 - val_loss: 0.6913 - val_mse: 0.6913\n",
      "Epoch 63/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6739 - mse: 0.6739 - val_loss: 0.6914 - val_mse: 0.6914\n",
      "Epoch 64/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6733 - mse: 0.6733 - val_loss: 0.6900 - val_mse: 0.6900\n",
      "Epoch 65/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6723 - mse: 0.6723 - val_loss: 0.6912 - val_mse: 0.6912\n",
      "Epoch 66/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6720 - mse: 0.6720 - val_loss: 0.6998 - val_mse: 0.6998\n",
      "Epoch 67/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6710 - mse: 0.6710 - val_loss: 0.6868 - val_mse: 0.6868\n",
      "Epoch 68/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6703 - mse: 0.6703 - val_loss: 0.6857 - val_mse: 0.6857\n",
      "Epoch 69/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6691 - mse: 0.6691 - val_loss: 0.6860 - val_mse: 0.6860\n",
      "Epoch 70/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6692 - mse: 0.6692 - val_loss: 0.6848 - val_mse: 0.6848\n",
      "Epoch 71/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6679 - mse: 0.6679 - val_loss: 0.7101 - val_mse: 0.7101\n",
      "Epoch 72/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6671 - mse: 0.6671 - val_loss: 0.6846 - val_mse: 0.6846\n",
      "Epoch 73/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6668 - mse: 0.6668 - val_loss: 0.6986 - val_mse: 0.6986\n",
      "Epoch 74/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6658 - mse: 0.6658 - val_loss: 0.6856 - val_mse: 0.6856\n",
      "Epoch 75/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6651 - mse: 0.6651 - val_loss: 0.6920 - val_mse: 0.6920\n",
      "Epoch 76/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6646 - mse: 0.6646 - val_loss: 0.6997 - val_mse: 0.6997\n",
      "Epoch 77/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6639 - mse: 0.6639 - val_loss: 0.6804 - val_mse: 0.6804\n",
      "Epoch 78/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6631 - mse: 0.6631 - val_loss: 0.6821 - val_mse: 0.6821\n",
      "Epoch 79/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6623 - mse: 0.6623 - val_loss: 0.6855 - val_mse: 0.6855\n",
      "Epoch 80/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6617 - mse: 0.6617 - val_loss: 0.6857 - val_mse: 0.6857\n",
      "Epoch 81/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6615 - mse: 0.6615 - val_loss: 0.6842 - val_mse: 0.6842\n",
      "Epoch 82/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6609 - mse: 0.6609 - val_loss: 0.6849 - val_mse: 0.6849\n",
      "Epoch 83/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6599 - mse: 0.6599 - val_loss: 0.6808 - val_mse: 0.6808\n",
      "Epoch 84/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6594 - mse: 0.6594 - val_loss: 0.6788 - val_mse: 0.6788\n",
      "Epoch 85/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6587 - mse: 0.6587 - val_loss: 0.6851 - val_mse: 0.6851\n",
      "Epoch 86/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6585 - mse: 0.6585 - val_loss: 0.6862 - val_mse: 0.6862\n",
      "Epoch 87/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6575 - mse: 0.6575 - val_loss: 0.6812 - val_mse: 0.6812\n",
      "Epoch 88/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6571 - mse: 0.6571 - val_loss: 0.6804 - val_mse: 0.6804\n",
      "Epoch 89/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6561 - mse: 0.6561 - val_loss: 0.6781 - val_mse: 0.6781\n",
      "Epoch 90/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6560 - mse: 0.6560 - val_loss: 0.6765 - val_mse: 0.6765\n",
      "Epoch 91/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6555 - mse: 0.6555 - val_loss: 0.6830 - val_mse: 0.6830\n",
      "Epoch 92/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6546 - mse: 0.6546 - val_loss: 0.6778 - val_mse: 0.6778\n",
      "Epoch 93/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6542 - mse: 0.6542 - val_loss: 0.6770 - val_mse: 0.6770\n",
      "Epoch 94/200\n",
      "3353317/3353317 [==============================] - 7s 2us/step - loss: 0.6535 - mse: 0.6535 - val_loss: 0.6782 - val_mse: 0.6782\n",
      "Epoch 95/200\n",
      "3353317/3353317 [==============================] - 7s 2us/step - loss: 0.6534 - mse: 0.6534 - val_loss: 0.6919 - val_mse: 0.6919\n",
      "Epoch 96/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6529 - mse: 0.6529 - val_loss: 0.6906 - val_mse: 0.6906\n",
      "Epoch 97/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6517 - mse: 0.6517 - val_loss: 0.6788 - val_mse: 0.6788\n",
      "Epoch 98/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6521 - mse: 0.6521 - val_loss: 0.6903 - val_mse: 0.6903\n",
      "Epoch 99/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6507 - mse: 0.6507 - val_loss: 0.6741 - val_mse: 0.6741\n",
      "Epoch 100/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6507 - mse: 0.6507 - val_loss: 0.7008 - val_mse: 0.7008\n",
      "Epoch 101/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6498 - mse: 0.6498 - val_loss: 0.6826 - val_mse: 0.6826\n",
      "Epoch 102/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6496 - mse: 0.6496 - val_loss: 0.6766 - val_mse: 0.6766\n",
      "Epoch 103/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6487 - mse: 0.6487 - val_loss: 0.6787 - val_mse: 0.6787\n",
      "Epoch 104/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6492 - mse: 0.6492 - val_loss: 0.6854 - val_mse: 0.6854\n",
      "Epoch 105/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6487 - mse: 0.6487 - val_loss: 0.6807 - val_mse: 0.6807\n",
      "Epoch 106/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6478 - mse: 0.6478 - val_loss: 0.6784 - val_mse: 0.6784\n",
      "Epoch 107/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6469 - mse: 0.6469 - val_loss: 0.6837 - val_mse: 0.6837\n",
      "Epoch 108/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6471 - mse: 0.6471 - val_loss: 0.6779 - val_mse: 0.6779\n",
      "Epoch 109/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6458 - mse: 0.6458 - val_loss: 0.6769 - val_mse: 0.6769\n",
      "Epoch 110/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6456 - mse: 0.6456 - val_loss: 0.6832 - val_mse: 0.6832\n",
      "Epoch 111/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6456 - mse: 0.6456 - val_loss: 0.6752 - val_mse: 0.6752\n",
      "Epoch 112/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6446 - mse: 0.6446 - val_loss: 0.6726 - val_mse: 0.6726\n",
      "Epoch 113/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6441 - mse: 0.6441 - val_loss: 0.6774 - val_mse: 0.6774\n",
      "Epoch 114/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6441 - mse: 0.6441 - val_loss: 0.6735 - val_mse: 0.6735\n",
      "Epoch 115/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6441 - mse: 0.6441 - val_loss: 0.6775 - val_mse: 0.6775\n",
      "Epoch 116/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6434 - mse: 0.6434 - val_loss: 0.6764 - val_mse: 0.6764\n",
      "Epoch 117/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6429 - mse: 0.6429 - val_loss: 0.6865 - val_mse: 0.6865\n",
      "Epoch 118/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6416 - mse: 0.6416 - val_loss: 0.6741 - val_mse: 0.6741\n",
      "Epoch 119/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6413 - mse: 0.6413 - val_loss: 0.6818 - val_mse: 0.6818\n",
      "Epoch 120/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6420 - mse: 0.6420 - val_loss: 0.6732 - val_mse: 0.6732\n",
      "Epoch 121/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6410 - mse: 0.6410 - val_loss: 0.6755 - val_mse: 0.6755\n",
      "Epoch 122/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6401 - mse: 0.6401 - val_loss: 0.6750 - val_mse: 0.6750\n",
      "Epoch 123/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6399 - mse: 0.6399 - val_loss: 0.6803 - val_mse: 0.6803\n",
      "Epoch 124/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6397 - mse: 0.6397 - val_loss: 0.6837 - val_mse: 0.6837\n",
      "Epoch 125/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6398 - mse: 0.6398 - val_loss: 0.6830 - val_mse: 0.6830\n",
      "Epoch 126/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6389 - mse: 0.6389 - val_loss: 0.6701 - val_mse: 0.6701\n",
      "Epoch 127/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6386 - mse: 0.6386 - val_loss: 0.6789 - val_mse: 0.6789\n",
      "Epoch 128/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6381 - mse: 0.6381 - val_loss: 0.6702 - val_mse: 0.6702\n",
      "Epoch 129/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6376 - mse: 0.6376 - val_loss: 0.6748 - val_mse: 0.6748\n",
      "Epoch 130/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6373 - mse: 0.6373 - val_loss: 0.6694 - val_mse: 0.6694\n",
      "Epoch 131/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6370 - mse: 0.6370 - val_loss: 0.6766 - val_mse: 0.6766\n",
      "Epoch 132/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6370 - mse: 0.6370 - val_loss: 0.6716 - val_mse: 0.6716\n",
      "Epoch 133/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6358 - mse: 0.6358 - val_loss: 0.6771 - val_mse: 0.6771\n",
      "Epoch 134/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6356 - mse: 0.6356 - val_loss: 0.6684 - val_mse: 0.6684\n",
      "Epoch 135/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6350 - mse: 0.6350 - val_loss: 0.6657 - val_mse: 0.6657\n",
      "Epoch 136/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6351 - mse: 0.6351 - val_loss: 0.6808 - val_mse: 0.6808\n",
      "Epoch 137/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6341 - mse: 0.6341 - val_loss: 0.6690 - val_mse: 0.6690\n",
      "Epoch 138/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6336 - mse: 0.6336 - val_loss: 0.6731 - val_mse: 0.6731\n",
      "Epoch 139/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6338 - mse: 0.6338 - val_loss: 0.6705 - val_mse: 0.6705\n",
      "Epoch 140/200\n",
      "3353317/3353317 [==============================] - 7s 2us/step - loss: 0.6331 - mse: 0.6331 - val_loss: 0.6800 - val_mse: 0.6800\n",
      "Epoch 141/200\n",
      "3353317/3353317 [==============================] - 7s 2us/step - loss: 0.6325 - mse: 0.6325 - val_loss: 0.6700 - val_mse: 0.6700\n",
      "Epoch 142/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6324 - mse: 0.6324 - val_loss: 0.6705 - val_mse: 0.6705\n",
      "Epoch 143/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6317 - mse: 0.6317 - val_loss: 0.6722 - val_mse: 0.6722\n",
      "Epoch 144/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6317 - mse: 0.6317 - val_loss: 0.6676 - val_mse: 0.6676\n",
      "Epoch 145/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6313 - mse: 0.6313 - val_loss: 0.6759 - val_mse: 0.6759\n",
      "Epoch 146/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6307 - mse: 0.6307 - val_loss: 0.6818 - val_mse: 0.6818\n",
      "Epoch 147/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6299 - mse: 0.6299 - val_loss: 0.6705 - val_mse: 0.6705\n",
      "Epoch 148/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6304 - mse: 0.6304 - val_loss: 0.6726 - val_mse: 0.6726\n",
      "Epoch 149/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6298 - mse: 0.6298 - val_loss: 0.6794 - val_mse: 0.6794\n",
      "Epoch 150/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6294 - mse: 0.6294 - val_loss: 0.6655 - val_mse: 0.6655\n",
      "Epoch 151/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6290 - mse: 0.6290 - val_loss: 0.6692 - val_mse: 0.6692\n",
      "Epoch 152/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6283 - mse: 0.6283 - val_loss: 0.6691 - val_mse: 0.6691\n",
      "Epoch 153/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6289 - mse: 0.6289 - val_loss: 0.6683 - val_mse: 0.6683\n",
      "Epoch 154/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6280 - mse: 0.6280 - val_loss: 0.6672 - val_mse: 0.6672\n",
      "Epoch 155/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6279 - mse: 0.6279 - val_loss: 0.6728 - val_mse: 0.6728\n",
      "Epoch 156/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6274 - mse: 0.6274 - val_loss: 0.6811 - val_mse: 0.6811\n",
      "Epoch 157/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6267 - mse: 0.6267 - val_loss: 0.6659 - val_mse: 0.6659\n",
      "Epoch 158/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6266 - mse: 0.6266 - val_loss: 0.6645 - val_mse: 0.6645\n",
      "Epoch 159/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6263 - mse: 0.6263 - val_loss: 0.6734 - val_mse: 0.6734\n",
      "Epoch 160/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6261 - mse: 0.6261 - val_loss: 0.6659 - val_mse: 0.6659\n",
      "Epoch 161/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6252 - mse: 0.6252 - val_loss: 0.6748 - val_mse: 0.6748\n",
      "Epoch 162/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6252 - mse: 0.6252 - val_loss: 0.6787 - val_mse: 0.6787\n",
      "Epoch 163/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6250 - mse: 0.6250 - val_loss: 0.6867 - val_mse: 0.6867\n",
      "Epoch 164/200\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.6242 - mse: 0.6242 - val_loss: 0.6683 - val_mse: 0.6683\n",
      "Epoch 165/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6239 - mse: 0.6239 - val_loss: 0.6671 - val_mse: 0.6671\n",
      "Epoch 166/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6237 - mse: 0.6237 - val_loss: 0.6700 - val_mse: 0.6700\n",
      "Epoch 167/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6233 - mse: 0.6233 - val_loss: 0.6599 - val_mse: 0.6599\n",
      "Epoch 168/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6232 - mse: 0.6232 - val_loss: 0.6768 - val_mse: 0.6768\n",
      "Epoch 169/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6229 - mse: 0.6229 - val_loss: 0.6696 - val_mse: 0.6696\n",
      "Epoch 170/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6221 - mse: 0.6221 - val_loss: 0.6613 - val_mse: 0.6613\n",
      "Epoch 171/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6217 - mse: 0.6217 - val_loss: 0.6687 - val_mse: 0.6687\n",
      "Epoch 172/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6220 - mse: 0.6220 - val_loss: 0.6739 - val_mse: 0.6739\n",
      "Epoch 173/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6216 - mse: 0.6216 - val_loss: 0.6633 - val_mse: 0.6633\n",
      "Epoch 174/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6206 - mse: 0.6206 - val_loss: 0.6601 - val_mse: 0.6601\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6205 - mse: 0.6205 - val_loss: 0.6636 - val_mse: 0.6636\n",
      "Epoch 176/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6199 - mse: 0.6199 - val_loss: 0.6693 - val_mse: 0.6693\n",
      "Epoch 177/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6198 - mse: 0.6198 - val_loss: 0.6697 - val_mse: 0.6697\n",
      "Epoch 178/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6196 - mse: 0.6196 - val_loss: 0.6667 - val_mse: 0.6667\n",
      "Epoch 179/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6194 - mse: 0.6194 - val_loss: 0.6771 - val_mse: 0.6771\n",
      "Epoch 180/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6193 - mse: 0.6193 - val_loss: 0.6610 - val_mse: 0.6610\n",
      "Epoch 181/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6188 - mse: 0.6188 - val_loss: 0.6664 - val_mse: 0.6664\n",
      "Epoch 182/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6185 - mse: 0.6185 - val_loss: 0.6637 - val_mse: 0.6637\n",
      "Epoch 183/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6177 - mse: 0.6177 - val_loss: 0.6665 - val_mse: 0.6665\n",
      "Epoch 184/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6180 - mse: 0.6180 - val_loss: 0.6752 - val_mse: 0.6752\n",
      "Epoch 185/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6176 - mse: 0.6176 - val_loss: 0.6667 - val_mse: 0.6667\n",
      "Epoch 186/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6173 - mse: 0.6173 - val_loss: 0.6682 - val_mse: 0.6682\n",
      "Epoch 187/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6169 - mse: 0.6169 - val_loss: 0.6798 - val_mse: 0.6798\n",
      "Epoch 188/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6170 - mse: 0.6170 - val_loss: 0.6696 - val_mse: 0.6696\n",
      "Epoch 189/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6155 - mse: 0.6155 - val_loss: 0.6778 - val_mse: 0.6778\n",
      "Epoch 190/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6159 - mse: 0.6159 - val_loss: 0.6634 - val_mse: 0.6634\n",
      "Epoch 191/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6155 - mse: 0.6155 - val_loss: 0.6636 - val_mse: 0.6636\n",
      "Epoch 192/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6159 - mse: 0.6159 - val_loss: 0.6639 - val_mse: 0.6639\n",
      "Epoch 193/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6149 - mse: 0.6149 - val_loss: 0.6701 - val_mse: 0.6701\n",
      "Epoch 194/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6145 - mse: 0.6145 - val_loss: 0.6655 - val_mse: 0.6655\n",
      "Epoch 195/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6141 - mse: 0.6141 - val_loss: 0.6772 - val_mse: 0.6772\n",
      "Epoch 196/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6147 - mse: 0.6147 - val_loss: 0.6729 - val_mse: 0.6729\n",
      "Epoch 197/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6134 - mse: 0.6134 - val_loss: 0.6712 - val_mse: 0.6712\n",
      "Epoch 198/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6134 - mse: 0.6134 - val_loss: 0.6696 - val_mse: 0.6696\n",
      "Epoch 199/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6130 - mse: 0.6130 - val_loss: 0.6593 - val_mse: 0.6593\n",
      "Epoch 200/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6128 - mse: 0.6128 - val_loss: 0.6691 - val_mse: 0.6691\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353317 samples, validate on 221802 samples\n",
      "Epoch 1/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.8474 - mse: 0.8474 - val_loss: 0.7914 - val_mse: 0.7914\n",
      "Epoch 2/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7533 - mse: 0.7533 - val_loss: 0.7923 - val_mse: 0.7923\n",
      "Epoch 3/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7437 - mse: 0.7437 - val_loss: 0.7993 - val_mse: 0.7993\n",
      "Epoch 4/200\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.7367 - mse: 0.7367 - val_loss: 0.7868 - val_mse: 0.7868\n",
      "Epoch 5/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7301 - mse: 0.7301 - val_loss: 0.7857 - val_mse: 0.7857\n",
      "Epoch 6/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7237 - mse: 0.7237 - val_loss: 0.7819 - val_mse: 0.7819\n",
      "Epoch 7/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7180 - mse: 0.7180 - val_loss: 0.7767 - val_mse: 0.7767\n",
      "Epoch 8/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7123 - mse: 0.7123 - val_loss: 0.7702 - val_mse: 0.7702\n",
      "Epoch 9/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7073 - mse: 0.7073 - val_loss: 0.7707 - val_mse: 0.7707\n",
      "Epoch 10/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.7032 - mse: 0.7032 - val_loss: 0.7622 - val_mse: 0.7622\n",
      "Epoch 11/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6990 - mse: 0.6990 - val_loss: 0.7652 - val_mse: 0.7652\n",
      "Epoch 12/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6959 - mse: 0.6959 - val_loss: 0.7699 - val_mse: 0.7699\n",
      "Epoch 13/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6926 - mse: 0.6926 - val_loss: 0.7635 - val_mse: 0.7635\n",
      "Epoch 14/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6899 - mse: 0.6899 - val_loss: 0.7629 - val_mse: 0.7629\n",
      "Epoch 15/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6879 - mse: 0.6879 - val_loss: 0.7554 - val_mse: 0.7554\n",
      "Epoch 16/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6860 - mse: 0.6860 - val_loss: 0.7641 - val_mse: 0.7641\n",
      "Epoch 17/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6839 - mse: 0.6839 - val_loss: 0.7501 - val_mse: 0.7501\n",
      "Epoch 18/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6826 - mse: 0.6826 - val_loss: 0.7537 - val_mse: 0.7537\n",
      "Epoch 19/200\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.6810 - mse: 0.6810 - val_loss: 0.7506 - val_mse: 0.7506\n",
      "Epoch 20/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6793 - mse: 0.6793 - val_loss: 0.7567 - val_mse: 0.7567\n",
      "Epoch 21/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6785 - mse: 0.6785 - val_loss: 0.7474 - val_mse: 0.7474\n",
      "Epoch 22/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6769 - mse: 0.6769 - val_loss: 0.7445 - val_mse: 0.7445\n",
      "Epoch 23/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6756 - mse: 0.6756 - val_loss: 0.7497 - val_mse: 0.7497\n",
      "Epoch 24/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6746 - mse: 0.6746 - val_loss: 0.7595 - val_mse: 0.7595\n",
      "Epoch 25/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6737 - mse: 0.6737 - val_loss: 0.7601 - val_mse: 0.7601\n",
      "Epoch 26/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6723 - mse: 0.6723 - val_loss: 0.7431 - val_mse: 0.7431\n",
      "Epoch 27/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6716 - mse: 0.6716 - val_loss: 0.7416 - val_mse: 0.7416\n",
      "Epoch 28/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6712 - mse: 0.6712 - val_loss: 0.7700 - val_mse: 0.7700\n",
      "Epoch 29/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6697 - mse: 0.6697 - val_loss: 0.7411 - val_mse: 0.7411\n",
      "Epoch 30/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6688 - mse: 0.6688 - val_loss: 0.7497 - val_mse: 0.7497\n",
      "Epoch 31/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6675 - mse: 0.6675 - val_loss: 0.7486 - val_mse: 0.7486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6673 - mse: 0.6673 - val_loss: 0.7568 - val_mse: 0.7568\n",
      "Epoch 33/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6661 - mse: 0.6661 - val_loss: 0.7392 - val_mse: 0.7392\n",
      "Epoch 34/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6649 - mse: 0.6649 - val_loss: 0.7375 - val_mse: 0.7375\n",
      "Epoch 35/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6646 - mse: 0.6646 - val_loss: 0.7383 - val_mse: 0.7383\n",
      "Epoch 36/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6635 - mse: 0.6635 - val_loss: 0.7404 - val_mse: 0.7404\n",
      "Epoch 37/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6634 - mse: 0.6634 - val_loss: 0.7375 - val_mse: 0.7375\n",
      "Epoch 38/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6623 - mse: 0.6623 - val_loss: 0.7383 - val_mse: 0.7383\n",
      "Epoch 39/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6616 - mse: 0.6616 - val_loss: 0.7330 - val_mse: 0.7330\n",
      "Epoch 40/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6605 - mse: 0.6605 - val_loss: 0.7386 - val_mse: 0.7386\n",
      "Epoch 41/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6598 - mse: 0.6598 - val_loss: 0.7348 - val_mse: 0.7348\n",
      "Epoch 42/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6591 - mse: 0.6591 - val_loss: 0.7330 - val_mse: 0.7330\n",
      "Epoch 43/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6585 - mse: 0.6585 - val_loss: 0.7380 - val_mse: 0.7380\n",
      "Epoch 44/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6577 - mse: 0.6577 - val_loss: 0.7370 - val_mse: 0.7370\n",
      "Epoch 45/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6566 - mse: 0.6566 - val_loss: 0.7292 - val_mse: 0.7292\n",
      "Epoch 46/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6565 - mse: 0.6565 - val_loss: 0.7439 - val_mse: 0.7439\n",
      "Epoch 47/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6555 - mse: 0.6555 - val_loss: 0.7350 - val_mse: 0.7350\n",
      "Epoch 48/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6546 - mse: 0.6546 - val_loss: 0.7574 - val_mse: 0.7574\n",
      "Epoch 49/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6545 - mse: 0.6545 - val_loss: 0.7295 - val_mse: 0.7295\n",
      "Epoch 50/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6535 - mse: 0.6535 - val_loss: 0.7332 - val_mse: 0.7332\n",
      "Epoch 51/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6527 - mse: 0.6527 - val_loss: 0.7353 - val_mse: 0.7353\n",
      "Epoch 52/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6526 - mse: 0.6526 - val_loss: 0.7258 - val_mse: 0.7258\n",
      "Epoch 53/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6517 - mse: 0.6517 - val_loss: 0.7316 - val_mse: 0.7316\n",
      "Epoch 54/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6512 - mse: 0.6512 - val_loss: 0.7355 - val_mse: 0.7355\n",
      "Epoch 55/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6501 - mse: 0.6501 - val_loss: 0.7248 - val_mse: 0.7248\n",
      "Epoch 56/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6498 - mse: 0.6498 - val_loss: 0.7279 - val_mse: 0.7279\n",
      "Epoch 57/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6490 - mse: 0.6490 - val_loss: 0.7247 - val_mse: 0.7247\n",
      "Epoch 58/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6488 - mse: 0.6488 - val_loss: 0.7245 - val_mse: 0.7245\n",
      "Epoch 59/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6481 - mse: 0.6481 - val_loss: 0.7310 - val_mse: 0.7310\n",
      "Epoch 60/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6477 - mse: 0.6477 - val_loss: 0.7228 - val_mse: 0.7228\n",
      "Epoch 61/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6471 - mse: 0.6471 - val_loss: 0.7322 - val_mse: 0.7322\n",
      "Epoch 62/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6460 - mse: 0.6460 - val_loss: 0.7364 - val_mse: 0.7364\n",
      "Epoch 63/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6460 - mse: 0.6460 - val_loss: 0.7248 - val_mse: 0.7248\n",
      "Epoch 64/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6457 - mse: 0.6457 - val_loss: 0.7265 - val_mse: 0.7265\n",
      "Epoch 65/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6448 - mse: 0.6448 - val_loss: 0.7244 - val_mse: 0.7244\n",
      "Epoch 66/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6438 - mse: 0.6438 - val_loss: 0.7450 - val_mse: 0.7450\n",
      "Epoch 67/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6440 - mse: 0.6440 - val_loss: 0.7231 - val_mse: 0.7231\n",
      "Epoch 68/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6432 - mse: 0.6432 - val_loss: 0.7203 - val_mse: 0.7203\n",
      "Epoch 69/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6425 - mse: 0.6425 - val_loss: 0.7226 - val_mse: 0.7226\n",
      "Epoch 70/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6424 - mse: 0.6424 - val_loss: 0.7235 - val_mse: 0.7235\n",
      "Epoch 71/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6418 - mse: 0.6418 - val_loss: 0.7219 - val_mse: 0.7219\n",
      "Epoch 72/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6413 - mse: 0.6413 - val_loss: 0.7196 - val_mse: 0.7196\n",
      "Epoch 73/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6408 - mse: 0.6408 - val_loss: 0.7198 - val_mse: 0.7198\n",
      "Epoch 74/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6402 - mse: 0.6402 - val_loss: 0.7379 - val_mse: 0.7379\n",
      "Epoch 75/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6402 - mse: 0.6402 - val_loss: 0.7216 - val_mse: 0.7216\n",
      "Epoch 76/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6398 - mse: 0.6398 - val_loss: 0.7213 - val_mse: 0.7213\n",
      "Epoch 77/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6390 - mse: 0.6390 - val_loss: 0.7193 - val_mse: 0.7193\n",
      "Epoch 78/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6391 - mse: 0.6391 - val_loss: 0.7243 - val_mse: 0.7243\n",
      "Epoch 79/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6381 - mse: 0.6381 - val_loss: 0.7237 - val_mse: 0.7237\n",
      "Epoch 80/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6376 - mse: 0.6376 - val_loss: 0.7318 - val_mse: 0.7318\n",
      "Epoch 81/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6373 - mse: 0.6373 - val_loss: 0.7164 - val_mse: 0.7164\n",
      "Epoch 82/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6369 - mse: 0.6369 - val_loss: 0.7147 - val_mse: 0.7147\n",
      "Epoch 83/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6364 - mse: 0.6364 - val_loss: 0.7372 - val_mse: 0.7372\n",
      "Epoch 84/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6373 - mse: 0.6373 - val_loss: 0.7240 - val_mse: 0.7240\n",
      "Epoch 85/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6359 - mse: 0.6359 - val_loss: 0.7169 - val_mse: 0.7169\n",
      "Epoch 86/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6351 - mse: 0.6351 - val_loss: 0.7165 - val_mse: 0.7165\n",
      "Epoch 87/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6353 - mse: 0.6353 - val_loss: 0.7176 - val_mse: 0.7176\n",
      "Epoch 88/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6347 - mse: 0.6347 - val_loss: 0.7227 - val_mse: 0.7227\n",
      "Epoch 89/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6344 - mse: 0.6344 - val_loss: 0.7168 - val_mse: 0.7168\n",
      "Epoch 90/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6343 - mse: 0.6343 - val_loss: 0.7144 - val_mse: 0.7144\n",
      "Epoch 91/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6336 - mse: 0.6336 - val_loss: 0.7143 - val_mse: 0.7143\n",
      "Epoch 92/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6338 - mse: 0.6338 - val_loss: 0.7224 - val_mse: 0.7224\n",
      "Epoch 93/200\n",
      "3353317/3353317 [==============================] - 7s 2us/step - loss: 0.6336 - mse: 0.6336 - val_loss: 0.7175 - val_mse: 0.7175\n",
      "Epoch 94/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6323 - mse: 0.6323 - val_loss: 0.7226 - val_mse: 0.7226\n",
      "Epoch 95/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6322 - mse: 0.6322 - val_loss: 0.7135 - val_mse: 0.7135\n",
      "Epoch 96/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6318 - mse: 0.6318 - val_loss: 0.7133 - val_mse: 0.7133\n",
      "Epoch 97/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6318 - mse: 0.6318 - val_loss: 0.7121 - val_mse: 0.7121\n",
      "Epoch 98/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6313 - mse: 0.6313 - val_loss: 0.7136 - val_mse: 0.7136\n",
      "Epoch 99/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6309 - mse: 0.6309 - val_loss: 0.7135 - val_mse: 0.7135\n",
      "Epoch 100/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6307 - mse: 0.6307 - val_loss: 0.7219 - val_mse: 0.7219\n",
      "Epoch 101/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6303 - mse: 0.6303 - val_loss: 0.7070 - val_mse: 0.7070\n",
      "Epoch 102/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6306 - mse: 0.6306 - val_loss: 0.7139 - val_mse: 0.7139\n",
      "Epoch 103/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6300 - mse: 0.6300 - val_loss: 0.7212 - val_mse: 0.7212\n",
      "Epoch 104/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6293 - mse: 0.6293 - val_loss: 0.7197 - val_mse: 0.7197\n",
      "Epoch 105/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6295 - mse: 0.6295 - val_loss: 0.7152 - val_mse: 0.7152\n",
      "Epoch 106/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6288 - mse: 0.6288 - val_loss: 0.7193 - val_mse: 0.7193\n",
      "Epoch 107/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6286 - mse: 0.6286 - val_loss: 0.7103 - val_mse: 0.7103\n",
      "Epoch 108/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6282 - mse: 0.6282 - val_loss: 0.7166 - val_mse: 0.7166\n",
      "Epoch 109/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6284 - mse: 0.6284 - val_loss: 0.7174 - val_mse: 0.7174\n",
      "Epoch 110/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6276 - mse: 0.6276 - val_loss: 0.7140 - val_mse: 0.7140\n",
      "Epoch 111/200\n",
      "3353317/3353317 [==============================] - 7s 2us/step - loss: 0.6277 - mse: 0.6277 - val_loss: 0.7072 - val_mse: 0.7072\n",
      "Epoch 112/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6272 - mse: 0.6272 - val_loss: 0.7094 - val_mse: 0.7094\n",
      "Epoch 113/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6270 - mse: 0.6270 - val_loss: 0.7258 - val_mse: 0.7258\n",
      "Epoch 114/200\n",
      "3353317/3353317 [==============================] - 10s 3us/step - loss: 0.6267 - mse: 0.6267 - val_loss: 0.7142 - val_mse: 0.7142\n",
      "Epoch 115/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6268 - mse: 0.6268 - val_loss: 0.7104 - val_mse: 0.7104\n",
      "Epoch 116/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6263 - mse: 0.6263 - val_loss: 0.7112 - val_mse: 0.7112\n",
      "Epoch 117/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6257 - mse: 0.6257 - val_loss: 0.7098 - val_mse: 0.7098\n",
      "Epoch 118/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6257 - mse: 0.6257 - val_loss: 0.7199 - val_mse: 0.7199\n",
      "Epoch 119/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6258 - mse: 0.6258 - val_loss: 0.7119 - val_mse: 0.7119\n",
      "Epoch 120/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6251 - mse: 0.6251 - val_loss: 0.7172 - val_mse: 0.7172\n",
      "Epoch 121/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6254 - mse: 0.6254 - val_loss: 0.7198 - val_mse: 0.7198\n",
      "Epoch 122/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6246 - mse: 0.6246 - val_loss: 0.7097 - val_mse: 0.7097\n",
      "Epoch 123/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6244 - mse: 0.6244 - val_loss: 0.7142 - val_mse: 0.7142\n",
      "Epoch 124/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6241 - mse: 0.6241 - val_loss: 0.7109 - val_mse: 0.7109\n",
      "Epoch 125/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6242 - mse: 0.6242 - val_loss: 0.7073 - val_mse: 0.7073\n",
      "Epoch 126/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6241 - mse: 0.6241 - val_loss: 0.7180 - val_mse: 0.7180\n",
      "Epoch 127/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6236 - mse: 0.6236 - val_loss: 0.7276 - val_mse: 0.7276\n",
      "Epoch 128/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6230 - mse: 0.6230 - val_loss: 0.7160 - val_mse: 0.7160\n",
      "Epoch 129/200\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.6231 - mse: 0.6231 - val_loss: 0.7099 - val_mse: 0.7099\n",
      "Epoch 130/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6226 - mse: 0.6226 - val_loss: 0.7108 - val_mse: 0.7108\n",
      "Epoch 131/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6227 - mse: 0.6227 - val_loss: 0.7272 - val_mse: 0.7272\n",
      "Epoch 132/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6222 - mse: 0.6222 - val_loss: 0.7062 - val_mse: 0.7062\n",
      "Epoch 133/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6224 - mse: 0.6224 - val_loss: 0.7077 - val_mse: 0.7077\n",
      "Epoch 134/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6218 - mse: 0.6218 - val_loss: 0.7056 - val_mse: 0.7056\n",
      "Epoch 135/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6219 - mse: 0.6219 - val_loss: 0.7166 - val_mse: 0.7166\n",
      "Epoch 136/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6216 - mse: 0.6216 - val_loss: 0.7213 - val_mse: 0.7213\n",
      "Epoch 137/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6213 - mse: 0.6213 - val_loss: 0.7080 - val_mse: 0.7080\n",
      "Epoch 138/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6210 - mse: 0.6210 - val_loss: 0.7109 - val_mse: 0.7109\n",
      "Epoch 139/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6211 - mse: 0.6211 - val_loss: 0.7039 - val_mse: 0.7039\n",
      "Epoch 140/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6204 - mse: 0.6204 - val_loss: 0.7056 - val_mse: 0.7056\n",
      "Epoch 141/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6204 - mse: 0.6204 - val_loss: 0.7048 - val_mse: 0.7048\n",
      "Epoch 142/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6202 - mse: 0.6202 - val_loss: 0.7150 - val_mse: 0.7150\n",
      "Epoch 143/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6202 - mse: 0.6202 - val_loss: 0.7041 - val_mse: 0.7041\n",
      "Epoch 144/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6198 - mse: 0.6198 - val_loss: 0.7096 - val_mse: 0.7096\n",
      "Epoch 145/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6196 - mse: 0.6196 - val_loss: 0.7115 - val_mse: 0.7115\n",
      "Epoch 146/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6192 - mse: 0.6192 - val_loss: 0.7087 - val_mse: 0.7087\n",
      "Epoch 147/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6193 - mse: 0.6193 - val_loss: 0.7135 - val_mse: 0.7135\n",
      "Epoch 148/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6192 - mse: 0.6192 - val_loss: 0.7077 - val_mse: 0.7077\n",
      "Epoch 149/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6188 - mse: 0.6188 - val_loss: 0.7025 - val_mse: 0.7025\n",
      "Epoch 150/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6196 - mse: 0.6196 - val_loss: 0.7238 - val_mse: 0.7238\n",
      "Epoch 151/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6185 - mse: 0.6185 - val_loss: 0.7080 - val_mse: 0.7080\n",
      "Epoch 152/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6183 - mse: 0.6183 - val_loss: 0.7080 - val_mse: 0.7080\n",
      "Epoch 153/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6180 - mse: 0.6180 - val_loss: 0.7051 - val_mse: 0.7051\n",
      "Epoch 154/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6181 - mse: 0.6181 - val_loss: 0.7100 - val_mse: 0.7100\n",
      "Epoch 155/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6174 - mse: 0.6174 - val_loss: 0.7023 - val_mse: 0.7023\n",
      "Epoch 156/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6176 - mse: 0.6176 - val_loss: 0.7080 - val_mse: 0.7080\n",
      "Epoch 157/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6171 - mse: 0.6171 - val_loss: 0.7054 - val_mse: 0.7054\n",
      "Epoch 158/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6172 - mse: 0.6172 - val_loss: 0.7104 - val_mse: 0.7104\n",
      "Epoch 159/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6172 - mse: 0.6172 - val_loss: 0.7044 - val_mse: 0.7044\n",
      "Epoch 160/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6164 - mse: 0.6164 - val_loss: 0.7124 - val_mse: 0.7124\n",
      "Epoch 161/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6172 - mse: 0.6172 - val_loss: 0.7047 - val_mse: 0.7047\n",
      "Epoch 162/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6166 - mse: 0.6166 - val_loss: 0.7308 - val_mse: 0.7308\n",
      "Epoch 163/200\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.6163 - mse: 0.6163 - val_loss: 0.7039 - val_mse: 0.7039\n",
      "Epoch 164/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6162 - mse: 0.6162 - val_loss: 0.7098 - val_mse: 0.7098\n",
      "Epoch 165/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6159 - mse: 0.6159 - val_loss: 0.6990 - val_mse: 0.6990\n",
      "Epoch 166/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6161 - mse: 0.6161 - val_loss: 0.7014 - val_mse: 0.7014\n",
      "Epoch 167/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6159 - mse: 0.6159 - val_loss: 0.7026 - val_mse: 0.7026\n",
      "Epoch 168/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6152 - mse: 0.6152 - val_loss: 0.7037 - val_mse: 0.7037\n",
      "Epoch 169/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6156 - mse: 0.6156 - val_loss: 0.7029 - val_mse: 0.7029\n",
      "Epoch 170/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6150 - mse: 0.6150 - val_loss: 0.7073 - val_mse: 0.7073\n",
      "Epoch 171/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6148 - mse: 0.6148 - val_loss: 0.6997 - val_mse: 0.6997\n",
      "Epoch 172/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6147 - mse: 0.6147 - val_loss: 0.7028 - val_mse: 0.7028\n",
      "Epoch 173/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6143 - mse: 0.6143 - val_loss: 0.7035 - val_mse: 0.7035\n",
      "Epoch 174/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6145 - mse: 0.6145 - val_loss: 0.7065 - val_mse: 0.7065\n",
      "Epoch 175/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6143 - mse: 0.6143 - val_loss: 0.7020 - val_mse: 0.7020\n",
      "Epoch 176/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6140 - mse: 0.6140 - val_loss: 0.7033 - val_mse: 0.7033\n",
      "Epoch 177/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6140 - mse: 0.6140 - val_loss: 0.7037 - val_mse: 0.7037\n",
      "Epoch 178/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6136 - mse: 0.6136 - val_loss: 0.7015 - val_mse: 0.7015\n",
      "Epoch 179/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6133 - mse: 0.6133 - val_loss: 0.6989 - val_mse: 0.6989\n",
      "Epoch 180/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6135 - mse: 0.6135 - val_loss: 0.7152 - val_mse: 0.7152\n",
      "Epoch 181/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6132 - mse: 0.6132 - val_loss: 0.7132 - val_mse: 0.7132\n",
      "Epoch 182/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6136 - mse: 0.6136 - val_loss: 0.7023 - val_mse: 0.7023\n",
      "Epoch 183/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6129 - mse: 0.6129 - val_loss: 0.7036 - val_mse: 0.7036\n",
      "Epoch 184/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6129 - mse: 0.6129 - val_loss: 0.7022 - val_mse: 0.7022\n",
      "Epoch 185/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6126 - mse: 0.6126 - val_loss: 0.7019 - val_mse: 0.7019\n",
      "Epoch 186/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6125 - mse: 0.6125 - val_loss: 0.7044 - val_mse: 0.7044\n",
      "Epoch 187/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6123 - mse: 0.6123 - val_loss: 0.6996 - val_mse: 0.6996\n",
      "Epoch 188/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6119 - mse: 0.6119 - val_loss: 0.7090 - val_mse: 0.7090\n",
      "Epoch 189/200\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.6121 - mse: 0.6121 - val_loss: 0.7045 - val_mse: 0.7045\n",
      "Epoch 190/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6115 - mse: 0.6115 - val_loss: 0.7084 - val_mse: 0.7084\n",
      "Epoch 191/200\n",
      "3353317/3353317 [==============================] - 9s 3us/step - loss: 0.6119 - mse: 0.6119 - val_loss: 0.7001 - val_mse: 0.7001\n",
      "Epoch 192/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6117 - mse: 0.6117 - val_loss: 0.6997 - val_mse: 0.6997\n",
      "Epoch 193/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6112 - mse: 0.6112 - val_loss: 0.7017 - val_mse: 0.7017\n",
      "Epoch 194/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6117 - mse: 0.6117 - val_loss: 0.7042 - val_mse: 0.7042\n",
      "Epoch 195/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6110 - mse: 0.6110 - val_loss: 0.7189 - val_mse: 0.7189\n",
      "Epoch 196/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6108 - mse: 0.6108 - val_loss: 0.7024 - val_mse: 0.7024\n",
      "Epoch 197/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6108 - mse: 0.6108 - val_loss: 0.7092 - val_mse: 0.7092\n",
      "Epoch 198/200\n",
      "3353317/3353317 [==============================] - 8s 3us/step - loss: 0.6106 - mse: 0.6106 - val_loss: 0.7011 - val_mse: 0.7011\n",
      "Epoch 199/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6106 - mse: 0.6106 - val_loss: 0.6996 - val_mse: 0.6996\n",
      "Epoch 200/200\n",
      "3353317/3353317 [==============================] - 8s 2us/step - loss: 0.6104 - mse: 0.6104 - val_loss: 0.6980 - val_mse: 0.6980\n",
      "1676659/1676659 [==============================] - 2s 1us/step\n",
      "Train on 3353318 samples, validate on 221802 samples\n",
      "Epoch 1/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.9931 - mse: 0.9931 - val_loss: 0.7999 - val_mse: 0.7999\n",
      "Epoch 2/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8588 - mse: 0.8588 - val_loss: 0.8195 - val_mse: 0.8195\n",
      "Epoch 3/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8398 - mse: 0.8398 - val_loss: 0.8339 - val_mse: 0.8339\n",
      "Epoch 4/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8269 - mse: 0.8269 - val_loss: 0.8323 - val_mse: 0.8323\n",
      "Epoch 5/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8169 - mse: 0.8169 - val_loss: 0.8293 - val_mse: 0.8293\n",
      "Epoch 6/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8083 - mse: 0.8083 - val_loss: 0.8258 - val_mse: 0.8258\n",
      "Epoch 7/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.8008 - mse: 0.8008 - val_loss: 0.8293 - val_mse: 0.8293\n",
      "Epoch 8/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7934 - mse: 0.7934 - val_loss: 0.8075 - val_mse: 0.8075\n",
      "Epoch 9/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7876 - mse: 0.7876 - val_loss: 0.8040 - val_mse: 0.8040\n",
      "Epoch 10/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7826 - mse: 0.7826 - val_loss: 0.7985 - val_mse: 0.7985\n",
      "Epoch 11/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7776 - mse: 0.7776 - val_loss: 0.8176 - val_mse: 0.8176\n",
      "Epoch 12/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7736 - mse: 0.7736 - val_loss: 0.7884 - val_mse: 0.7884\n",
      "Epoch 13/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7701 - mse: 0.7701 - val_loss: 0.7985 - val_mse: 0.7985\n",
      "Epoch 14/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7665 - mse: 0.7665 - val_loss: 0.7778 - val_mse: 0.7778\n",
      "Epoch 15/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7633 - mse: 0.7633 - val_loss: 0.7839 - val_mse: 0.7839\n",
      "Epoch 16/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7603 - mse: 0.7603 - val_loss: 0.7813 - val_mse: 0.7813\n",
      "Epoch 17/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7585 - mse: 0.7585 - val_loss: 0.7724 - val_mse: 0.7724\n",
      "Epoch 18/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7558 - mse: 0.7558 - val_loss: 0.7731 - val_mse: 0.7731\n",
      "Epoch 19/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7536 - mse: 0.7536 - val_loss: 0.7706 - val_mse: 0.7706\n",
      "Epoch 20/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7520 - mse: 0.7520 - val_loss: 0.7798 - val_mse: 0.7798\n",
      "Epoch 21/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7495 - mse: 0.7495 - val_loss: 0.7757 - val_mse: 0.7757\n",
      "Epoch 22/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7476 - mse: 0.7476 - val_loss: 0.7653 - val_mse: 0.7653\n",
      "Epoch 23/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7462 - mse: 0.7462 - val_loss: 0.7642 - val_mse: 0.7642\n",
      "Epoch 24/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7446 - mse: 0.7446 - val_loss: 0.7767 - val_mse: 0.7767\n",
      "Epoch 25/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7432 - mse: 0.7432 - val_loss: 0.7596 - val_mse: 0.7596\n",
      "Epoch 26/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7412 - mse: 0.7412 - val_loss: 0.7799 - val_mse: 0.7799\n",
      "Epoch 27/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7403 - mse: 0.7403 - val_loss: 0.7592 - val_mse: 0.7592\n",
      "Epoch 28/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7384 - mse: 0.7384 - val_loss: 0.7675 - val_mse: 0.7675\n",
      "Epoch 29/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7379 - mse: 0.7379 - val_loss: 0.7636 - val_mse: 0.7636\n",
      "Epoch 30/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7361 - mse: 0.7361 - val_loss: 0.7786 - val_mse: 0.7786\n",
      "Epoch 31/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.7351 - mse: 0.7351 - val_loss: 0.7501 - val_mse: 0.7501\n",
      "Epoch 32/200\n",
      "3353318/3353318 [==============================] - 8s 3us/step - loss: 0.7340 - mse: 0.7340 - val_loss: 0.7819 - val_mse: 0.7819\n",
      "Epoch 33/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.7328 - mse: 0.7328 - val_loss: 0.7561 - val_mse: 0.7561\n",
      "Epoch 34/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7320 - mse: 0.7320 - val_loss: 0.7519 - val_mse: 0.7519\n",
      "Epoch 35/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7309 - mse: 0.7309 - val_loss: 0.7533 - val_mse: 0.7533\n",
      "Epoch 36/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7293 - mse: 0.7293 - val_loss: 0.8014 - val_mse: 0.8014\n",
      "Epoch 37/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7287 - mse: 0.7287 - val_loss: 0.7604 - val_mse: 0.7604\n",
      "Epoch 38/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7276 - mse: 0.7276 - val_loss: 0.7486 - val_mse: 0.7486\n",
      "Epoch 39/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.7266 - mse: 0.7266 - val_loss: 0.7897 - val_mse: 0.7897\n",
      "Epoch 40/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.7256 - mse: 0.7256 - val_loss: 0.7669 - val_mse: 0.7669\n",
      "Epoch 41/200\n",
      "3353318/3353318 [==============================] - 8s 3us/step - loss: 0.7249 - mse: 0.7249 - val_loss: 0.7819 - val_mse: 0.7819\n",
      "Epoch 42/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7236 - mse: 0.7236 - val_loss: 0.7651 - val_mse: 0.7651\n",
      "Epoch 43/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7225 - mse: 0.7225 - val_loss: 0.8074 - val_mse: 0.8074\n",
      "Epoch 44/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7220 - mse: 0.7220 - val_loss: 0.7669 - val_mse: 0.7669\n",
      "Epoch 45/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7207 - mse: 0.7207 - val_loss: 0.7645 - val_mse: 0.7645\n",
      "Epoch 46/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7200 - mse: 0.7200 - val_loss: 0.7939 - val_mse: 0.7939\n",
      "Epoch 47/200\n",
      "3353318/3353318 [==============================] - 8s 3us/step - loss: 0.7192 - mse: 0.7192 - val_loss: 0.7734 - val_mse: 0.7734\n",
      "Epoch 48/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7179 - mse: 0.7179 - val_loss: 0.7648 - val_mse: 0.7648\n",
      "Epoch 49/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7179 - mse: 0.7179 - val_loss: 0.7573 - val_mse: 0.7573\n",
      "Epoch 50/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7163 - mse: 0.7163 - val_loss: 0.7589 - val_mse: 0.7589\n",
      "Epoch 51/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7156 - mse: 0.7156 - val_loss: 0.7663 - val_mse: 0.7663\n",
      "Epoch 52/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7148 - mse: 0.7148 - val_loss: 0.7856 - val_mse: 0.7856\n",
      "Epoch 53/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7139 - mse: 0.7139 - val_loss: 0.7878 - val_mse: 0.7878\n",
      "Epoch 54/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7127 - mse: 0.7127 - val_loss: 0.7694 - val_mse: 0.7694\n",
      "Epoch 55/200\n",
      "3353318/3353318 [==============================] - 8s 3us/step - loss: 0.7122 - mse: 0.7122 - val_loss: 0.7529 - val_mse: 0.7529\n",
      "Epoch 56/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7124 - mse: 0.7124 - val_loss: 0.7745 - val_mse: 0.7745\n",
      "Epoch 57/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7106 - mse: 0.7106 - val_loss: 0.7849 - val_mse: 0.7849\n",
      "Epoch 58/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7099 - mse: 0.7099 - val_loss: 0.7593 - val_mse: 0.7593\n",
      "Epoch 59/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7096 - mse: 0.7096 - val_loss: 0.7560 - val_mse: 0.7560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7087 - mse: 0.7087 - val_loss: 0.7596 - val_mse: 0.7596\n",
      "Epoch 61/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7077 - mse: 0.7077 - val_loss: 0.7725 - val_mse: 0.7725\n",
      "Epoch 62/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7071 - mse: 0.7071 - val_loss: 0.7639 - val_mse: 0.7639\n",
      "Epoch 63/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7067 - mse: 0.7067 - val_loss: 0.7658 - val_mse: 0.7658\n",
      "Epoch 64/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7062 - mse: 0.7062 - val_loss: 0.7809 - val_mse: 0.7809\n",
      "Epoch 65/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7049 - mse: 0.7049 - val_loss: 0.7664 - val_mse: 0.7664\n",
      "Epoch 66/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7047 - mse: 0.7047 - val_loss: 0.7759 - val_mse: 0.7759\n",
      "Epoch 67/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7035 - mse: 0.7035 - val_loss: 0.7558 - val_mse: 0.7558\n",
      "Epoch 68/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7036 - mse: 0.7036 - val_loss: 0.7688 - val_mse: 0.7688\n",
      "Epoch 69/200\n",
      "3353318/3353318 [==============================] - 8s 3us/step - loss: 0.7025 - mse: 0.7025 - val_loss: 0.7749 - val_mse: 0.7749\n",
      "Epoch 70/200\n",
      "3353318/3353318 [==============================] - 8s 3us/step - loss: 0.7015 - mse: 0.7015 - val_loss: 0.7635 - val_mse: 0.7635\n",
      "Epoch 71/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7008 - mse: 0.7008 - val_loss: 0.7762 - val_mse: 0.7762\n",
      "Epoch 72/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.7004 - mse: 0.7004 - val_loss: 0.7534 - val_mse: 0.7534\n",
      "Epoch 73/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6997 - mse: 0.6997 - val_loss: 0.7644 - val_mse: 0.7644\n",
      "Epoch 74/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6994 - mse: 0.6994 - val_loss: 0.7634 - val_mse: 0.7634\n",
      "Epoch 75/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6988 - mse: 0.6988 - val_loss: 0.7677 - val_mse: 0.7677\n",
      "Epoch 76/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6977 - mse: 0.6977 - val_loss: 0.7482 - val_mse: 0.7482\n",
      "Epoch 77/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6973 - mse: 0.6973 - val_loss: 0.7618 - val_mse: 0.7618\n",
      "Epoch 78/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6967 - mse: 0.6967 - val_loss: 0.7629 - val_mse: 0.7629\n",
      "Epoch 79/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6957 - mse: 0.6957 - val_loss: 0.7752 - val_mse: 0.7752\n",
      "Epoch 80/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6951 - mse: 0.6951 - val_loss: 0.7651 - val_mse: 0.7651\n",
      "Epoch 81/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6952 - mse: 0.6952 - val_loss: 0.7669 - val_mse: 0.7669\n",
      "Epoch 82/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6942 - mse: 0.6942 - val_loss: 0.7578 - val_mse: 0.7578\n",
      "Epoch 83/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6932 - mse: 0.6932 - val_loss: 0.7606 - val_mse: 0.7606\n",
      "Epoch 84/200\n",
      "3353318/3353318 [==============================] - 8s 3us/step - loss: 0.6923 - mse: 0.6923 - val_loss: 0.7694 - val_mse: 0.7694\n",
      "Epoch 85/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6926 - mse: 0.6926 - val_loss: 0.7552 - val_mse: 0.7552\n",
      "Epoch 86/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6921 - mse: 0.6921 - val_loss: 0.7517 - val_mse: 0.7517\n",
      "Epoch 87/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6911 - mse: 0.6911 - val_loss: 0.7556 - val_mse: 0.7556\n",
      "Epoch 88/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6904 - mse: 0.6904 - val_loss: 0.7589 - val_mse: 0.7589\n",
      "Epoch 89/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6904 - mse: 0.6904 - val_loss: 0.7480 - val_mse: 0.7480\n",
      "Epoch 90/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6897 - mse: 0.6897 - val_loss: 0.7591 - val_mse: 0.7591\n",
      "Epoch 91/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6889 - mse: 0.6889 - val_loss: 0.7533 - val_mse: 0.7533\n",
      "Epoch 92/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6882 - mse: 0.6882 - val_loss: 0.7669 - val_mse: 0.7669\n",
      "Epoch 93/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6874 - mse: 0.6874 - val_loss: 0.7548 - val_mse: 0.7548\n",
      "Epoch 94/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6872 - mse: 0.6872 - val_loss: 0.7608 - val_mse: 0.7608\n",
      "Epoch 95/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6866 - mse: 0.6866 - val_loss: 0.7526 - val_mse: 0.7526\n",
      "Epoch 96/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6864 - mse: 0.6864 - val_loss: 0.7516 - val_mse: 0.7516\n",
      "Epoch 97/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6850 - mse: 0.6850 - val_loss: 0.7644 - val_mse: 0.7644\n",
      "Epoch 98/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6849 - mse: 0.6849 - val_loss: 0.7542 - val_mse: 0.7542\n",
      "Epoch 99/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6845 - mse: 0.6845 - val_loss: 0.7720 - val_mse: 0.7720\n",
      "Epoch 100/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6833 - mse: 0.6833 - val_loss: 0.7593 - val_mse: 0.7593\n",
      "Epoch 101/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6836 - mse: 0.6836 - val_loss: 0.7623 - val_mse: 0.7623\n",
      "Epoch 102/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6829 - mse: 0.6829 - val_loss: 0.7607 - val_mse: 0.7607\n",
      "Epoch 103/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6826 - mse: 0.6826 - val_loss: 0.7566 - val_mse: 0.7566\n",
      "Epoch 104/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6820 - mse: 0.6820 - val_loss: 0.7595 - val_mse: 0.7595\n",
      "Epoch 105/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6813 - mse: 0.6813 - val_loss: 0.7607 - val_mse: 0.7607\n",
      "Epoch 106/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6810 - mse: 0.6810 - val_loss: 0.7710 - val_mse: 0.7710\n",
      "Epoch 107/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6805 - mse: 0.6805 - val_loss: 0.7703 - val_mse: 0.7703\n",
      "Epoch 108/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6802 - mse: 0.6802 - val_loss: 0.7609 - val_mse: 0.7609\n",
      "Epoch 109/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6791 - mse: 0.6791 - val_loss: 0.7619 - val_mse: 0.7619\n",
      "Epoch 110/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6786 - mse: 0.6786 - val_loss: 0.7561 - val_mse: 0.7561\n",
      "Epoch 111/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6787 - mse: 0.6787 - val_loss: 0.7618 - val_mse: 0.7618\n",
      "Epoch 112/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6778 - mse: 0.6778 - val_loss: 0.7660 - val_mse: 0.7660\n",
      "Epoch 113/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6771 - mse: 0.6771 - val_loss: 0.7615 - val_mse: 0.7615\n",
      "Epoch 114/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6766 - mse: 0.6766 - val_loss: 0.7612 - val_mse: 0.7612\n",
      "Epoch 115/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6760 - mse: 0.6760 - val_loss: 0.7668 - val_mse: 0.7668\n",
      "Epoch 116/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6757 - mse: 0.6757 - val_loss: 0.7541 - val_mse: 0.7541\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 8s 3us/step - loss: 0.6754 - mse: 0.6754 - val_loss: 0.7592 - val_mse: 0.7592\n",
      "Epoch 118/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6746 - mse: 0.6746 - val_loss: 0.7683 - val_mse: 0.7683\n",
      "Epoch 119/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6749 - mse: 0.6749 - val_loss: 0.7635 - val_mse: 0.7635\n",
      "Epoch 120/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6746 - mse: 0.6746 - val_loss: 0.7623 - val_mse: 0.7623\n",
      "Epoch 121/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6735 - mse: 0.6735 - val_loss: 0.7619 - val_mse: 0.7619\n",
      "Epoch 122/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6731 - mse: 0.6731 - val_loss: 0.7585 - val_mse: 0.7585\n",
      "Epoch 123/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6727 - mse: 0.6727 - val_loss: 0.7629 - val_mse: 0.7629\n",
      "Epoch 124/200\n",
      "3353318/3353318 [==============================] - 8s 3us/step - loss: 0.6722 - mse: 0.6722 - val_loss: 0.7608 - val_mse: 0.7608\n",
      "Epoch 125/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6714 - mse: 0.6714 - val_loss: 0.7551 - val_mse: 0.7551\n",
      "Epoch 126/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6711 - mse: 0.6711 - val_loss: 0.7689 - val_mse: 0.7689\n",
      "Epoch 127/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6708 - mse: 0.6708 - val_loss: 0.7582 - val_mse: 0.7582\n",
      "Epoch 128/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6705 - mse: 0.6705 - val_loss: 0.7618 - val_mse: 0.7618\n",
      "Epoch 129/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6698 - mse: 0.6698 - val_loss: 0.7548 - val_mse: 0.7548\n",
      "Epoch 130/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6699 - mse: 0.6699 - val_loss: 0.7526 - val_mse: 0.7526\n",
      "Epoch 131/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6693 - mse: 0.6693 - val_loss: 0.7682 - val_mse: 0.7682\n",
      "Epoch 132/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6688 - mse: 0.6688 - val_loss: 0.7716 - val_mse: 0.7716\n",
      "Epoch 133/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6681 - mse: 0.6681 - val_loss: 0.7560 - val_mse: 0.7560\n",
      "Epoch 134/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6677 - mse: 0.6677 - val_loss: 0.7582 - val_mse: 0.7582\n",
      "Epoch 135/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6670 - mse: 0.6670 - val_loss: 0.7653 - val_mse: 0.7653\n",
      "Epoch 136/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6669 - mse: 0.6669 - val_loss: 0.7693 - val_mse: 0.7693\n",
      "Epoch 137/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6666 - mse: 0.6666 - val_loss: 0.7542 - val_mse: 0.7542\n",
      "Epoch 138/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6660 - mse: 0.6660 - val_loss: 0.7611 - val_mse: 0.7611\n",
      "Epoch 139/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6657 - mse: 0.6657 - val_loss: 0.7642 - val_mse: 0.7642\n",
      "Epoch 140/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6653 - mse: 0.6653 - val_loss: 0.7586 - val_mse: 0.7586\n",
      "Epoch 141/200\n",
      "3353318/3353318 [==============================] - 8s 3us/step - loss: 0.6645 - mse: 0.6645 - val_loss: 0.7597 - val_mse: 0.7597\n",
      "Epoch 142/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6642 - mse: 0.6642 - val_loss: 0.7622 - val_mse: 0.7622\n",
      "Epoch 143/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6636 - mse: 0.6636 - val_loss: 0.7589 - val_mse: 0.7589\n",
      "Epoch 144/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6637 - mse: 0.6637 - val_loss: 0.7681 - val_mse: 0.7681\n",
      "Epoch 145/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6633 - mse: 0.6633 - val_loss: 0.7610 - val_mse: 0.7610\n",
      "Epoch 146/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6627 - mse: 0.6627 - val_loss: 0.7702 - val_mse: 0.7702\n",
      "Epoch 147/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6624 - mse: 0.6624 - val_loss: 0.7644 - val_mse: 0.7644\n",
      "Epoch 148/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6615 - mse: 0.6615 - val_loss: 0.7556 - val_mse: 0.7556\n",
      "Epoch 149/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6617 - mse: 0.6617 - val_loss: 0.7648 - val_mse: 0.7648\n",
      "Epoch 150/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6607 - mse: 0.6607 - val_loss: 0.7798 - val_mse: 0.7798\n",
      "Epoch 151/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6606 - mse: 0.6606 - val_loss: 0.7617 - val_mse: 0.7617\n",
      "Epoch 152/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6606 - mse: 0.6606 - val_loss: 0.7632 - val_mse: 0.7632\n",
      "Epoch 153/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6598 - mse: 0.6598 - val_loss: 0.7634 - val_mse: 0.7634\n",
      "Epoch 154/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6597 - mse: 0.6597 - val_loss: 0.7691 - val_mse: 0.7691\n",
      "Epoch 155/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6592 - mse: 0.6592 - val_loss: 0.7598 - val_mse: 0.7598\n",
      "Epoch 156/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6588 - mse: 0.6588 - val_loss: 0.7612 - val_mse: 0.7612\n",
      "Epoch 157/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6586 - mse: 0.6586 - val_loss: 0.7750 - val_mse: 0.7750\n",
      "Epoch 158/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6582 - mse: 0.6582 - val_loss: 0.7784 - val_mse: 0.7784\n",
      "Epoch 159/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6577 - mse: 0.6577 - val_loss: 0.7780 - val_mse: 0.7780\n",
      "Epoch 160/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6576 - mse: 0.6576 - val_loss: 0.7752 - val_mse: 0.7752\n",
      "Epoch 161/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6571 - mse: 0.6571 - val_loss: 0.7687 - val_mse: 0.7687\n",
      "Epoch 162/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6564 - mse: 0.6564 - val_loss: 0.7880 - val_mse: 0.7880\n",
      "Epoch 163/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6558 - mse: 0.6558 - val_loss: 0.7831 - val_mse: 0.7831\n",
      "Epoch 164/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6556 - mse: 0.6556 - val_loss: 0.7737 - val_mse: 0.7737\n",
      "Epoch 165/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6558 - mse: 0.6558 - val_loss: 0.7719 - val_mse: 0.7719\n",
      "Epoch 166/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6549 - mse: 0.6549 - val_loss: 0.7807 - val_mse: 0.7807\n",
      "Epoch 167/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6543 - mse: 0.6543 - val_loss: 0.7796 - val_mse: 0.7796\n",
      "Epoch 168/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6542 - mse: 0.6542 - val_loss: 0.8408 - val_mse: 0.8408\n",
      "Epoch 169/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6540 - mse: 0.6540 - val_loss: 0.8067 - val_mse: 0.8067\n",
      "Epoch 170/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6538 - mse: 0.6538 - val_loss: 0.7777 - val_mse: 0.7777\n",
      "Epoch 171/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6534 - mse: 0.6534 - val_loss: 0.8081 - val_mse: 0.8081\n",
      "Epoch 172/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6534 - mse: 0.6534 - val_loss: 0.7912 - val_mse: 0.7912\n",
      "Epoch 173/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6522 - mse: 0.6522 - val_loss: 0.8098 - val_mse: 0.8098\n",
      "Epoch 174/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6520 - mse: 0.6520 - val_loss: 0.7909 - val_mse: 0.7909\n",
      "Epoch 175/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6522 - mse: 0.6522 - val_loss: 0.7931 - val_mse: 0.7931\n",
      "Epoch 176/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6516 - mse: 0.6516 - val_loss: 0.7968 - val_mse: 0.7968\n",
      "Epoch 177/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6511 - mse: 0.6511 - val_loss: 0.7958 - val_mse: 0.7958\n",
      "Epoch 178/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6510 - mse: 0.6510 - val_loss: 0.7867 - val_mse: 0.7867\n",
      "Epoch 179/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6509 - mse: 0.6509 - val_loss: 0.8052 - val_mse: 0.8052\n",
      "Epoch 180/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6506 - mse: 0.6506 - val_loss: 0.7928 - val_mse: 0.7928\n",
      "Epoch 181/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6496 - mse: 0.6496 - val_loss: 0.7824 - val_mse: 0.7824\n",
      "Epoch 182/200\n",
      "3353318/3353318 [==============================] - 12s 4us/step - loss: 0.6496 - mse: 0.6496 - val_loss: 0.8013 - val_mse: 0.8013\n",
      "Epoch 183/200\n",
      "3353318/3353318 [==============================] - 12s 3us/step - loss: 0.6487 - mse: 0.6487 - val_loss: 0.8155 - val_mse: 0.8155\n",
      "Epoch 184/200\n",
      "3353318/3353318 [==============================] - 10s 3us/step - loss: 0.6486 - mse: 0.6486 - val_loss: 0.7967 - val_mse: 0.7967\n",
      "Epoch 185/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6490 - mse: 0.6490 - val_loss: 0.8191 - val_mse: 0.8191\n",
      "Epoch 186/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6484 - mse: 0.6484 - val_loss: 0.7903 - val_mse: 0.7903\n",
      "Epoch 187/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6479 - mse: 0.6479 - val_loss: 0.8130 - val_mse: 0.8130\n",
      "Epoch 188/200\n",
      "3353318/3353318 [==============================] - 8s 3us/step - loss: 0.6476 - mse: 0.6476 - val_loss: 0.8351 - val_mse: 0.8351\n",
      "Epoch 189/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6472 - mse: 0.6472 - val_loss: 0.8130 - val_mse: 0.8130\n",
      "Epoch 190/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6473 - mse: 0.6473 - val_loss: 0.8050 - val_mse: 0.8050\n",
      "Epoch 191/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6470 - mse: 0.6470 - val_loss: 0.8135 - val_mse: 0.8135\n",
      "Epoch 192/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6463 - mse: 0.6463 - val_loss: 0.8032 - val_mse: 0.8032\n",
      "Epoch 193/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6470 - mse: 0.6470 - val_loss: 0.8175 - val_mse: 0.8175\n",
      "Epoch 194/200\n",
      "3353318/3353318 [==============================] - 8s 2us/step - loss: 0.6456 - mse: 0.6456 - val_loss: 0.8118 - val_mse: 0.8118\n",
      "Epoch 195/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6454 - mse: 0.6454 - val_loss: 0.8140 - val_mse: 0.8140\n",
      "Epoch 196/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6452 - mse: 0.6452 - val_loss: 0.8095 - val_mse: 0.8095\n",
      "Epoch 197/200\n",
      "3353318/3353318 [==============================] - 8s 3us/step - loss: 0.6448 - mse: 0.6448 - val_loss: 0.7983 - val_mse: 0.7983\n",
      "Epoch 198/200\n",
      "3353318/3353318 [==============================] - 8s 3us/step - loss: 0.6446 - mse: 0.6446 - val_loss: 0.8022 - val_mse: 0.8022\n",
      "Epoch 199/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6440 - mse: 0.6440 - val_loss: 0.8215 - val_mse: 0.8215\n",
      "Epoch 200/200\n",
      "3353318/3353318 [==============================] - 9s 3us/step - loss: 0.6437 - mse: 0.6437 - val_loss: 0.8282 - val_mse: 0.8282\n",
      "1676658/1676658 [==============================] - 3s 2us/step\n",
      "Train on 5029976 samples, validate on 221802 samples\n",
      "Epoch 1/50\n",
      "5029976/5029976 [==============================] - 22s 4us/step - loss: 0.8559 - mse: 0.8559 - val_loss: 0.7739 - val_mse: 0.7739\n",
      "Epoch 2/50\n",
      "5029976/5029976 [==============================] - 22s 4us/step - loss: 0.7889 - mse: 0.7889 - val_loss: 0.7772 - val_mse: 0.7772\n",
      "Epoch 3/50\n",
      "5029976/5029976 [==============================] - 21s 4us/step - loss: 0.7701 - mse: 0.7701 - val_loss: 0.7548 - val_mse: 0.7548\n",
      "Epoch 4/50\n",
      "5029976/5029976 [==============================] - 22s 4us/step - loss: 0.7551 - mse: 0.7551 - val_loss: 0.7545 - val_mse: 0.7545\n",
      "Epoch 5/50\n",
      "5029976/5029976 [==============================] - 22s 4us/step - loss: 0.7436 - mse: 0.7436 - val_loss: 0.7420 - val_mse: 0.7420\n",
      "Epoch 6/50\n",
      "5029976/5029976 [==============================] - 22s 4us/step - loss: 0.7345 - mse: 0.7345 - val_loss: 0.7422 - val_mse: 0.7422\n",
      "Epoch 7/50\n",
      "5029976/5029976 [==============================] - 22s 4us/step - loss: 0.7273 - mse: 0.7273 - val_loss: 0.7305 - val_mse: 0.7305\n",
      "Epoch 8/50\n",
      "5029976/5029976 [==============================] - 22s 4us/step - loss: 0.7217 - mse: 0.7217 - val_loss: 0.7382 - val_mse: 0.7382\n",
      "Epoch 9/50\n",
      "5029976/5029976 [==============================] - 23s 5us/step - loss: 0.7167 - mse: 0.7167 - val_loss: 0.7240 - val_mse: 0.7240\n",
      "Epoch 10/50\n",
      "5029976/5029976 [==============================] - 22s 4us/step - loss: 0.7127 - mse: 0.7127 - val_loss: 0.7227 - val_mse: 0.7227\n",
      "Epoch 11/50\n",
      "5029976/5029976 [==============================] - 21s 4us/step - loss: 0.7092 - mse: 0.7092 - val_loss: 0.7162 - val_mse: 0.7162\n",
      "Epoch 12/50\n",
      "5029976/5029976 [==============================] - 21s 4us/step - loss: 0.7061 - mse: 0.7061 - val_loss: 0.7189 - val_mse: 0.7189\n",
      "Epoch 13/50\n",
      "5029976/5029976 [==============================] - 21s 4us/step - loss: 0.7033 - mse: 0.7033 - val_loss: 0.7102 - val_mse: 0.7102\n",
      "Epoch 14/50\n",
      "5029976/5029976 [==============================] - 23s 5us/step - loss: 0.7004 - mse: 0.7004 - val_loss: 0.7052 - val_mse: 0.7052\n",
      "Epoch 15/50\n",
      "5029976/5029976 [==============================] - 21s 4us/step - loss: 0.6984 - mse: 0.6984 - val_loss: 0.7128 - val_mse: 0.7128\n",
      "Epoch 16/50\n",
      "5029976/5029976 [==============================] - 20s 4us/step - loss: 0.6957 - mse: 0.6957 - val_loss: 0.7034 - val_mse: 0.7034\n",
      "Epoch 17/50\n",
      "5029976/5029976 [==============================] - 20s 4us/step - loss: 0.6938 - mse: 0.6938 - val_loss: 0.7215 - val_mse: 0.7215\n",
      "Epoch 18/50\n",
      "5029976/5029976 [==============================] - 19s 4us/step - loss: 0.6916 - mse: 0.6916 - val_loss: 0.7084 - val_mse: 0.7084\n",
      "Epoch 19/50\n",
      "5029976/5029976 [==============================] - 20s 4us/step - loss: 0.6895 - mse: 0.6895 - val_loss: 0.7053 - val_mse: 0.7053\n",
      "Epoch 20/50\n",
      "5029976/5029976 [==============================] - 19s 4us/step - loss: 0.6877 - mse: 0.6877 - val_loss: 0.6994 - val_mse: 0.6994\n",
      "Epoch 21/50\n",
      "5029976/5029976 [==============================] - 21s 4us/step - loss: 0.6856 - mse: 0.6856 - val_loss: 0.7010 - val_mse: 0.7010\n",
      "Epoch 22/50\n",
      "5029976/5029976 [==============================] - 21s 4us/step - loss: 0.6836 - mse: 0.6836 - val_loss: 0.7306 - val_mse: 0.7306\n",
      "Epoch 23/50\n",
      "5029976/5029976 [==============================] - 20s 4us/step - loss: 0.6816 - mse: 0.6816 - val_loss: 0.6977 - val_mse: 0.6977\n",
      "Epoch 24/50\n",
      "5029976/5029976 [==============================] - 20s 4us/step - loss: 0.6805 - mse: 0.6805 - val_loss: 0.6932 - val_mse: 0.6932\n",
      "Epoch 25/50\n",
      "5029976/5029976 [==============================] - 20s 4us/step - loss: 0.6785 - mse: 0.6785 - val_loss: 0.6995 - val_mse: 0.6995\n",
      "Epoch 26/50\n",
      "5029976/5029976 [==============================] - 19s 4us/step - loss: 0.6769 - mse: 0.6769 - val_loss: 0.7002 - val_mse: 0.7002\n",
      "Epoch 27/50\n",
      "5029976/5029976 [==============================] - 20s 4us/step - loss: 0.6754 - mse: 0.6754 - val_loss: 0.7241 - val_mse: 0.7241\n",
      "Epoch 28/50\n",
      "5029976/5029976 [==============================] - 20s 4us/step - loss: 0.6737 - mse: 0.6737 - val_loss: 0.7058 - val_mse: 0.7058\n",
      "Epoch 29/50\n",
      "5029976/5029976 [==============================] - 20s 4us/step - loss: 0.6723 - mse: 0.6723 - val_loss: 0.6904 - val_mse: 0.6904\n",
      "Epoch 30/50\n",
      "5029976/5029976 [==============================] - 20s 4us/step - loss: 0.6711 - mse: 0.6711 - val_loss: 0.6986 - val_mse: 0.6986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50\n",
      "5029976/5029976 [==============================] - 19s 4us/step - loss: 0.6695 - mse: 0.6695 - val_loss: 0.7130 - val_mse: 0.7130\n",
      "Epoch 32/50\n",
      "5029976/5029976 [==============================] - 19s 4us/step - loss: 0.6681 - mse: 0.6681 - val_loss: 0.6999 - val_mse: 0.6999\n",
      "Epoch 33/50\n",
      "5029976/5029976 [==============================] - 20s 4us/step - loss: 0.6666 - mse: 0.6666 - val_loss: 0.6947 - val_mse: 0.6947\n",
      "Epoch 34/50\n",
      "5029976/5029976 [==============================] - 20s 4us/step - loss: 0.6655 - mse: 0.6655 - val_loss: 0.7027 - val_mse: 0.7027\n",
      "Epoch 35/50\n",
      "5029976/5029976 [==============================] - 20s 4us/step - loss: 0.6638 - mse: 0.6638 - val_loss: 0.7033 - val_mse: 0.7033\n",
      "Epoch 36/50\n",
      "5029976/5029976 [==============================] - 20s 4us/step - loss: 0.6624 - mse: 0.6624 - val_loss: 0.6947 - val_mse: 0.6947\n",
      "Epoch 37/50\n",
      "5029976/5029976 [==============================] - 19s 4us/step - loss: 0.6615 - mse: 0.6615 - val_loss: 0.7110 - val_mse: 0.7110\n",
      "Epoch 38/50\n",
      "5029976/5029976 [==============================] - 20s 4us/step - loss: 0.6603 - mse: 0.6603 - val_loss: 0.6865 - val_mse: 0.6865\n",
      "Epoch 39/50\n",
      "5029976/5029976 [==============================] - 20s 4us/step - loss: 0.6592 - mse: 0.6592 - val_loss: 0.6936 - val_mse: 0.6936\n",
      "Epoch 40/50\n",
      "5029976/5029976 [==============================] - 20s 4us/step - loss: 0.6574 - mse: 0.6574 - val_loss: 0.6861 - val_mse: 0.6861\n",
      "Epoch 41/50\n",
      "5029976/5029976 [==============================] - 19s 4us/step - loss: 0.6567 - mse: 0.6567 - val_loss: 0.6915 - val_mse: 0.6915\n",
      "Epoch 42/50\n",
      "5029976/5029976 [==============================] - 19s 4us/step - loss: 0.6553 - mse: 0.6553 - val_loss: 0.6880 - val_mse: 0.6880\n",
      "Epoch 43/50\n",
      "5029976/5029976 [==============================] - 19s 4us/step - loss: 0.6547 - mse: 0.6547 - val_loss: 0.6945 - val_mse: 0.6945\n",
      "Epoch 44/50\n",
      "5029976/5029976 [==============================] - 19s 4us/step - loss: 0.6533 - mse: 0.6533 - val_loss: 0.6850 - val_mse: 0.6850\n",
      "Epoch 45/50\n",
      "5029976/5029976 [==============================] - 18s 4us/step - loss: 0.6527 - mse: 0.6527 - val_loss: 0.6850 - val_mse: 0.6850\n",
      "Epoch 46/50\n",
      "5029976/5029976 [==============================] - 18s 4us/step - loss: 0.6514 - mse: 0.6514 - val_loss: 0.6824 - val_mse: 0.6824\n",
      "Epoch 47/50\n",
      "5029976/5029976 [==============================] - 19s 4us/step - loss: 0.6506 - mse: 0.6506 - val_loss: 0.7025 - val_mse: 0.7025\n",
      "Epoch 48/50\n",
      "5029976/5029976 [==============================] - 19s 4us/step - loss: 0.6494 - mse: 0.6494 - val_loss: 0.6840 - val_mse: 0.6840\n",
      "Epoch 49/50\n",
      "5029976/5029976 [==============================] - 19s 4us/step - loss: 0.6488 - mse: 0.6488 - val_loss: 0.6803 - val_mse: 0.6803\n",
      "Epoch 50/50\n",
      "5029976/5029976 [==============================] - 19s 4us/step - loss: 0.6477 - mse: 0.6477 - val_loss: 0.6807 - val_mse: 0.6807\n",
      "0.8572210474710048\n",
      "{'activation': 'tanh', 'activation2': 'relu', 'batch_size': 5000, 'dropout_rate': 0.0, 'epochs': 50, 'init': 'he_normal', 'neurons': 200, 'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(optimizer=['Adam'], epochs=epochs, batch_size=batch_size, init=['he_normal'], activation=['tanh'], dropout_rate=[0.0], neurons=[200], activation2=['relu'])\n",
    "\n",
    "model = KerasRegressor(build_fn=create_model, verbose=1)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(train_x, train_y, validation_data=(valid_x, valid_y))\n",
    "\n",
    "print(np.sqrt(-grid_result.best_score_))\n",
    "print(grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'tanh',\n",
       " 'activation2': 'relu',\n",
       " 'batch_size': 5000,\n",
       " 'dropout_rate': 0.0,\n",
       " 'epochs': 50,\n",
       " 'init': 'he_normal',\n",
       " 'neurons': 200,\n",
       " 'optimizer': 'Adam'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_params = grid_result.best_params_\n",
    "keras_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save untrained model to file \n",
    "\n",
    "Pkl_Filename = \"Keras_Params.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(keras_params, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  27 | elapsed: 13.8min remaining: 23.5min\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  27 | elapsed: 15.1min remaining:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed: 15.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9206571766665913\n",
      "{'alpha': 0.0001, 'penalty': 'l1', 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "model = SGDRegressor(early_stopping=True, n_iter_no_change=10)\n",
    "param_grid={\n",
    "    'penalty':['l2','l1','elasticnet'],\n",
    "    'alpha': [0.0001,0.001,0.01], \n",
    "    'random_state': [0]\n",
    "}\n",
    "\n",
    "model, pred = algorithm_pipeline(X_train, X_valid, Y_train, Y_valid, model, \n",
    "                                 param_grid, cv=3)\n",
    "\n",
    "print(np.sqrt(-model.best_score_))\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.0001, 'penalty': 'l1', 'random_state': 0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_params = model.best_params_\n",
    "sgd_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save untrained model to file \n",
    "\n",
    "Pkl_Filename = \"SGD_Params.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(sgd_params, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
