{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentations with Gaussian classification on the Winery data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will experiment with the Gaussian classification of the winery data, writing the code from scratch. It will use at univariate, bivariate and multivariate Gaussian.\n",
    "\n",
    "The data is from the UCI repository (https://archive.ics.uci.edu/ml/datasets/wine).\n",
    "\n",
    "The data contains 178 labeled data points, each corresponding to a bottle of wine:\n",
    "* The features (x): a 13-dimensional vector consisting of features for the bottle of wine (features are: Alcohol, Malic acid, Ash, Alcalinity of ash, Magnesium, Total phenols, Flavanoids, Nonflavanoid phenols, Proanthocyanins, Color intensity, Hue, OD280/OD315 of diluted wines, Proline\n",
    "* The label (y): the winery where the bottle came from (1,2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('wine.data.txt', delimiter=',')\n",
    "\n",
    "featurenames = ['Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash','Magnesium', 'Total phenols', \n",
    "                'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', \n",
    "                'OD280/OD315 of diluted wines', 'Proline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data training set (trainx, trainy) of size 130 and test set (testx, testy) of size 48\n",
    "np.random.seed(0)\n",
    "perm = np.random.permutation(178)\n",
    "trainx = data[perm[0:130],1:14]\n",
    "trainy = data[perm[0:130],0]\n",
    "testx = data[perm[130:178], 1:14]\n",
    "testy = data[perm[130:178],0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43, 54, 33)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of each label in the train data\n",
    "sum(trainy==1), sum(trainy==2), sum(trainy==3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 17, 15)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of each label in test data\n",
    "sum(testy==1), sum(testy==2), sum(testy==3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions of each feature for each winery\n",
    "\n",
    "A histogram will be created for each feature under each class (winery), along with the Gaussian fit to the distribution. A slider functionality will be used to scroll through classes and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf3a6fab7324a01b4e37178535fc0a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='feature', max=12), IntSlider(value=1, description='label…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual( feature=IntSlider(0,0,12), label=IntSlider(1,1,3))\n",
    "def density_plot(feature, label):\n",
    "    plt.hist(trainx[trainy==label,feature], density=True)\n",
    "    #\n",
    "    mu = np.mean(trainx[trainy==label,feature]) # mean\n",
    "    var = np.var(trainx[trainy==label,feature]) # variance\n",
    "    std = np.sqrt(var) # standard deviation\n",
    "    #\n",
    "    x_axis = np.linspace(mu - 3*std, mu + 3*std, 1000)\n",
    "    plt.plot(x_axis, norm.pdf(x_axis,mu,std), 'r', lw=2)\n",
    "    plt.title(\"Winery \"+str(label) )\n",
    "    plt.xlabel(featurenames[feature], fontsize=14, color='red')\n",
    "    plt.ylabel('Density', fontsize=14, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Gaussian generative model to each class for each feature individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fit generative model for 3 classes for a single feature\n",
    "def fit_generative_model(x,y,feature):\n",
    "    k = 3 # number of classes\n",
    "    mu = np.zeros(k+1) # list of means\n",
    "    var = np.zeros(k+1) # list of variances\n",
    "    pi = np.zeros(k+1) # list of class weights\n",
    "    for label in range(1,k+1):\n",
    "        indices = (y==label)\n",
    "        mu[label] = np.mean(x[indices,feature])\n",
    "        var[label] = np.var(x[indices,feature])\n",
    "        pi[label] = float(sum(indices))/float(len(y))\n",
    "    return mu, var, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0bb9557d6c4c21a8dd8ab0c9043505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='feature', max=12), Button(description='Run Interact', st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display Gaussian distribution for each of the classes on the same plot\n",
    "@interact_manual( feature=IntSlider(0,0,12) )\n",
    "def show_densities(feature):\n",
    "    mu, var, pi = fit_generative_model(trainx, trainy, feature)\n",
    "    colors = ['r', 'k', 'g']\n",
    "    for label in range(1,4):\n",
    "        m = mu[label]\n",
    "        s = np.sqrt(var[label])\n",
    "        x_axis = np.linspace(m - 3*s, m+3*s, 1000)\n",
    "        plt.plot(x_axis, norm.pdf(x_axis,m,s), colors[label-1], label=\"class \" + str(label))\n",
    "    plt.xlabel(featurenames[feature], fontsize=14, color='red')\n",
    "    plt.ylabel('Density', fontsize=14, color='red')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training error for each feature \n",
    "\n",
    "How many classes are incorrectly predicted based on the Gaussian model for each feature individually?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf5e7c276c7448b8ee61ba12a34fd1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='feature', max=12), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact( feature=IntSlider(0,0,12) )\n",
    "def train_model(feature):\n",
    "    mu, var, pi = fit_generative_model(trainx, trainy, feature)\n",
    "\n",
    "    k = 3 \n",
    "    n_train = len(trainy) \n",
    "    score = np.zeros((n_train,k+1))\n",
    "    for i in range(0,n_train):\n",
    "        for label in range(1,k+1):\n",
    "            score[i,label] = np.log(pi[label]) + \\\n",
    "            norm.logpdf(trainx[i,feature], mu[label], np.sqrt(var[label]))\n",
    "    predictions = np.argmax(score[:,1:4], axis=1) + 1\n",
    "    errors = np.sum(predictions != trainy)\n",
    "    print(\"Train error using feature \" + featurenames[feature] + \": \" + str(errors) + \"/\" + str(n_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting labels for the test set using each feature individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e98d0e326f441b3856b5cdd7245cae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='feature', max=12), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact( feature=IntSlider(0,0,12) )\n",
    "def test_model(feature):\n",
    "    mu, var, pi = fit_generative_model(trainx, trainy, feature)\n",
    "\n",
    "    k = 3 \n",
    "    n_test = len(testy) \n",
    "    score = np.zeros((n_test,k+1))\n",
    "    for i in range(0,n_test):\n",
    "        for label in range(1,k+1):\n",
    "            score[i,label] = np.log(pi[label]) + \\\n",
    "            norm.logpdf(testx[i,feature], mu[label], np.sqrt(var[label]))\n",
    "    predictions = np.argmax(score[:,1:4], axis=1) + 1\n",
    "    errors = np.sum(predictions != testy)\n",
    "    print(\"Test error using feature \" + featurenames[feature] + \": \" + str(errors) + \"/\" + str(n_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Train Error</th>\n",
       "      <th>Test Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alcohol</td>\n",
       "      <td>0.338462</td>\n",
       "      <td>0.354167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Malic acid</td>\n",
       "      <td>0.376923</td>\n",
       "      <td>0.479167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ash</td>\n",
       "      <td>0.507692</td>\n",
       "      <td>0.604167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alcalinity of ash</td>\n",
       "      <td>0.523077</td>\n",
       "      <td>0.479167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Magnesium</td>\n",
       "      <td>0.469231</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Total phenols</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Flavanoids</td>\n",
       "      <td>0.207692</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nonflavanoid phenols</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.479167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Proanthocyanins</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Color intensity</td>\n",
       "      <td>0.292308</td>\n",
       "      <td>0.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hue</td>\n",
       "      <td>0.369231</td>\n",
       "      <td>0.291667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>OD280/OD315 of diluted wines</td>\n",
       "      <td>0.361538</td>\n",
       "      <td>0.395833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Proline</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.354167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Feature  Train Error  Test Error\n",
       "0                        Alcohol     0.338462    0.354167\n",
       "1                     Malic acid     0.376923    0.479167\n",
       "2                            Ash     0.507692    0.604167\n",
       "3              Alcalinity of ash     0.523077    0.479167\n",
       "4                      Magnesium     0.469231    0.437500\n",
       "5                  Total phenols     0.353846    0.333333\n",
       "6                     Flavanoids     0.207692    0.166667\n",
       "7           Nonflavanoid phenols     0.423077    0.479167\n",
       "8                Proanthocyanins     0.461538    0.333333\n",
       "9                Color intensity     0.292308    0.208333\n",
       "10                           Hue     0.369231    0.291667\n",
       "11  OD280/OD315 of diluted wines     0.361538    0.395833\n",
       "12                       Proline     0.269231    0.354167"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_error(feature):\n",
    "    mu, var, pi = fit_generative_model(trainx, trainy, feature)\n",
    "\n",
    "    k = 3 \n",
    "    n_train = len(trainy) \n",
    "    score = np.zeros((n_train,k+1))\n",
    "    train_error = []\n",
    "    for i in range(0,n_train):\n",
    "        for label in range(1,k+1):\n",
    "            score[i,label] = np.log(pi[label]) + \\\n",
    "            norm.logpdf(trainx[i,feature], mu[label], np.sqrt(var[label]))\n",
    "    predictions = np.argmax(score[:,1:4], axis=1) + 1\n",
    "    errors = np.sum(predictions != trainy)\n",
    "    return errors/n_train\n",
    "\n",
    "def test_error(feature):\n",
    "    mu, var, pi = fit_generative_model(trainx, trainy, feature)\n",
    "\n",
    "    k = 3 \n",
    "    n_test = len(testy) \n",
    "    score = np.zeros((n_test,k+1))\n",
    "    for i in range(0,n_test):\n",
    "        for label in range(1,k+1):\n",
    "            score[i,label] = np.log(pi[label]) + \\\n",
    "            norm.logpdf(testx[i,feature], mu[label], np.sqrt(var[label]))\n",
    "    predictions = np.argmax(score[:,1:4], axis=1) + 1\n",
    "    errors = np.sum(predictions != testy)\n",
    "    return errors/n_test\n",
    "    \n",
    "num = len(featurenames)\n",
    "train_errors = []\n",
    "for i in range(num):\n",
    "    errors = train_error(i)\n",
    "    train_errors.append(errors)\n",
    "\n",
    "test_errors = []\n",
    "for i in range(num):\n",
    "    errors = test_error(i)\n",
    "    test_errors.append(errors)\n",
    "\n",
    "df = pd.DataFrame({'Feature': featurenames, 'Train Error': train_errors, 'Test Error': test_errors})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bivariate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of two features from one winery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "\n",
    "# Function to fit a Gaussian to a data set using the selected features\n",
    "def fit_gaussian(x, features):\n",
    "    mu = np.mean(x[:,features], axis=0)\n",
    "    covar = np.cov(x[:,features], rowvar=0, bias=1)\n",
    "    return mu, covar\n",
    "\n",
    "# Find the range within which an array of numbers lie, with a little buffer\n",
    "def find_range(x):\n",
    "    lower = min(x)\n",
    "    upper = max(x)\n",
    "    width = upper - lower\n",
    "    lower = lower - 0.2 * width\n",
    "    upper = upper + 0.2 * width\n",
    "    return lower, upper\n",
    "\n",
    "#function to plot a few contour lines of a given 2D Gaussian\n",
    "def plot_contours(mu, cov, x1g, x2g, col):\n",
    "    rv = multivariate_normal(mean=mu, cov=cov)\n",
    "    z = np.zeros((len(x1g),len(x2g)))\n",
    "    for i in range(0,len(x1g)):\n",
    "        for j in range(0,len(x2g)):\n",
    "            z[j,i] = rv.logpdf([x1g[i], x2g[j]]) \n",
    "    sign, logdet = np.linalg.slogdet(cov)\n",
    "    normalizer = -0.5 * (2 * np.log(6.28) + sign * logdet)\n",
    "    for offset in range(1,4):\n",
    "        plt.contour(x1g,x2g,z, levels=[normalizer - offset], colors=col, linewidths=2.0, linestyles='solid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62beadd9424d4d5eaada9113a943dbca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='f1', max=12), IntSlider(value=6, description='f2', max=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create function to visualise the distribution for individual wineries for a pair of features\n",
    "#using slider functionality\n",
    "@interact_manual( f1=IntSlider(0,0,12,1), f2=IntSlider(6,0,12,1), label=IntSlider(1,1,3,1) )\n",
    "def two_features_plot(f1,f2,label):\n",
    "    if f1 == f2: # we need f1 != f2\n",
    "        print(\"Please choose different features for f1 and f2.\")\n",
    "        return  \n",
    "    \n",
    "    # Set up plot\n",
    "    x1_lower, x1_upper = find_range(trainx[trainy==label,f1])\n",
    "    x2_lower, x2_upper = find_range(trainx[trainy==label,f2])\n",
    "    plt.xlim(x1_lower, x1_upper) # limit along x1-axis\n",
    "    plt.ylim(x2_lower, x2_upper) # limit along x2-axis\n",
    "    \n",
    "    # Plot training points along the two selected features\n",
    "    plt.plot(trainx[trainy==label, f1], trainx[trainy==label, f2], 'ro')\n",
    "\n",
    "    # Define a grid along each axis; the density will be computed at each grid point\n",
    "    res = 200 # resolution\n",
    "    x1g = np.linspace(x1_lower, x1_upper, res)\n",
    "    x2g = np.linspace(x2_lower, x2_upper, res)\n",
    "\n",
    "    # plot a few contour lines of the density\n",
    "    mu, cov = fit_gaussian(trainx[trainy==label,:], [f1,f2])\n",
    "    plot_contours(mu, cov, x1g, x2g, 'k')\n",
    "    \n",
    "    plt.xlabel(featurenames[f1], fontsize=14, color='red')\n",
    "    plt.ylabel(featurenames[f2], fontsize=14, color='red')\n",
    "    plt.title('Class ' + str(label), fontsize=14, color='blue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Gaussian generative model to each class for a pair of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fit generative model based on 2 features\n",
    "def fit_generative_model(x, y, features):\n",
    "    k = 3 #classes\n",
    "    d = len(features) # number of features\n",
    "    mu = np.zeros((k+1,d)) # list of means\n",
    "    covar = np.zeros((k+1,d,d)) # list of covariance matrices\n",
    "    pi = np.zeros(k+1) # list of class weights\n",
    "    for label in range(1,k+1):\n",
    "        indices = (y==label)\n",
    "        mu[label,:], covar[label,:,:] = fit_gaussian(x[indices,:], features)\n",
    "        pi[label] = float(sum(indices))/float(len(y))\n",
    "    return mu, covar, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4718048d294e61ab4d30187a9460a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='f1', max=12), IntSlider(value=6, description='f2', max=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the Gaussians for all 3 wineries for a pair of features\n",
    "#using slider functionality to change pair of features\n",
    "@interact_manual( f1=IntSlider(0,0,12,1), f2=IntSlider(6,0,12,1) )\n",
    "def three_class_plot(f1,f2):\n",
    "    if f1 == f2: # we need f1 != f2\n",
    "        print(\"Please choose different features for f1 and f2.\")\n",
    "        return  \n",
    "    \n",
    "    # Set up plot\n",
    "    x1_lower, x1_upper = find_range(trainx[:,f1])\n",
    "    x2_lower, x2_upper = find_range(trainx[:,f2])\n",
    "    plt.xlim(x1_lower, x1_upper) # limit along x1-axis\n",
    "    plt.ylim(x2_lower, x2_upper) # limit along x2-axis\n",
    "    \n",
    "    # Plot the training points along the two selected features\n",
    "    colors = ['r', 'k', 'g']\n",
    "    for label in range(1,4):\n",
    "        plt.plot(trainx[trainy==label,f1], trainx[trainy==label,f2], marker='o', ls='None', c=colors[label-1])\n",
    "\n",
    "    # Define a grid along each axis; the density will be computed at each grid point\n",
    "    res = 200 # resolution\n",
    "    x1g = np.linspace(x1_lower, x1_upper, res)\n",
    "    x2g = np.linspace(x2_lower, x2_upper, res)\n",
    "\n",
    "    # Show the Gaussian fit to each class, using features f1,f2\n",
    "    mu, covar, pi = fit_generative_model(trainx, trainy, [f1,f2])\n",
    "    for label in range(1,4):\n",
    "        gmean = mu[label,:]\n",
    "        gcov = covar[label,:,:]\n",
    "        plot_contours(gmean, gcov, x1g, x2g, colors[label-1])\n",
    "\n",
    "    plt.xlabel(featurenames[f1], fontsize=14, color='red')\n",
    "    plt.ylabel(featurenames[f2], fontsize=14, color='red')\n",
    "    plt.title('Wine data', fontsize=14, color='blue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict labels for test point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a27dfb8e4f42da9168164037b33c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='f1', max=12), IntSlider(value=6, description='f2', max=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test performance of predictor based on a subset of features\n",
    "@interact( f1=IntSlider(0,0,12,1), f2=IntSlider(6,0,12,1) )\n",
    "def test_model(f1, f2):\n",
    "    if f1 == f2: # need f1 != f2\n",
    "        print(\"Please choose different features for f1 and f2.\")\n",
    "        return  \n",
    "    features= [f1,f2]\n",
    "    mu, covar, pi = fit_generative_model(trainx, trainy, features)\n",
    "    \n",
    "    k = 3 \n",
    "    nt = len(testy) # Number of test points\n",
    "    score = np.zeros((nt,k+1))\n",
    "    for i in range(0,nt):\n",
    "        for label in range(1,k+1):\n",
    "            score[i,label] = np.log(pi[label]) + \\\n",
    "            multivariate_normal.logpdf(testx[i,features], mean=mu[label,:], cov=covar[label,:,:])\n",
    "    predictions = np.argmax(score[:,1:4], axis=1) + 1\n",
    "    # Finally, tally up score\n",
    "    errors = np.sum(predictions != testy)\n",
    "    print(\"Test error using feature(s): \",)\n",
    "    for f in features:\n",
    "        print(\"'\" + featurenames[f] + \"'\" + \" \",)\n",
    "    print\n",
    "    print(\"Errors: \" + str(errors) + \"/\" + str(nt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundary\n",
    "\n",
    "A function will now be created that plots the decision boundary between the 3 wineries based on a specified pair of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd57621c1284522ac363a0d39f24f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='f1', max=12), IntSlider(value=6, description='f2', max=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual( f1=IntSlider(0,0,12,1), f2=IntSlider(6,0,12,1) )\n",
    "def show_decision_boundary(f1,f2):\n",
    "    # Fit Gaussian to each class\n",
    "    mu, covar, pi = fit_generative_model(trainx, trainy, [f1,f2])\n",
    "    \n",
    "    # Set up dimensions of plot\n",
    "    x1_lower, x1_upper = find_range(trainx[:,f1])\n",
    "    x2_lower, x2_upper = find_range(trainx[:,f2])\n",
    "    plt.xlim([x1_lower,x1_upper])\n",
    "    plt.ylim([x2_lower,x2_upper])\n",
    "\n",
    "    # Plot points in training set\n",
    "    colors = ['r', 'k', 'g']\n",
    "    for label in range(1,4):\n",
    "        plt.plot(trainx[trainy==label,f1], trainx[trainy==label,f2], marker='o', ls='None', c=colors[label-1])\n",
    "\n",
    "    # Define a dense grid; every point in the grid will be classified according to the generative model\n",
    "    res = 200\n",
    "    x1g = np.linspace(x1_lower, x1_upper, res)\n",
    "    x2g = np.linspace(x2_lower, x2_upper, res)\n",
    "\n",
    "    # Declare random variables corresponding to each class density\n",
    "    random_vars = {}\n",
    "    for label in range(1,4):\n",
    "        random_vars[label] = multivariate_normal(mean=mu[label,:],cov=covar[label,:,:])\n",
    "\n",
    "    # Classify every point in the grid; these are stored in an array Z[]\n",
    "    Z = np.zeros((len(x1g), len(x2g)))\n",
    "    for i in range(0,len(x1g)):\n",
    "        for j in range(0,len(x2g)):\n",
    "            scores = []\n",
    "            for label in range(1,4):\n",
    "                scores.append(np.log(pi[label]) + random_vars[label].logpdf([x1g[i],x2g[j]]))\n",
    "            Z[i,j] = np.argmax(scores) + 1\n",
    "\n",
    "    # Plot the contour lines\n",
    "    plt.contour(x1g,x2g,Z.T,3,cmap='seismic')\n",
    "    \n",
    "    # Finally, show the image\n",
    "    plt.xlabel(featurenames[f1], fontsize=14, color='red')\n",
    "    plt.ylabel(featurenames[f2], fontsize=14, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d76ff676d74a118c7a972c0b4d97aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='f1', max=12), IntSlider(value=6, description='f2', max=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#the same function as above will be used to visualise the decision boundaries found from the training data\n",
    "#in relation to the test data\n",
    "\n",
    "@interact_manual( f1=IntSlider(0,0,12,1), f2=IntSlider(6,0,12,1) )\n",
    "def show_decision_boundary(f1,f2):\n",
    "    # Fit Gaussian to each class\n",
    "    mu, covar, pi = fit_generative_model(trainx, trainy, [f1,f2])\n",
    "    \n",
    "    # Set up dimensions of plot\n",
    "    x1_lower, x1_upper = find_range(testx[:,f1])\n",
    "    x2_lower, x2_upper = find_range(testx[:,f2])\n",
    "    plt.xlim([x1_lower,x1_upper])\n",
    "    plt.ylim([x2_lower,x2_upper])\n",
    "\n",
    "    # Plot points in training set\n",
    "    colors = ['r', 'k', 'g']\n",
    "    for label in range(1,4):\n",
    "        plt.plot(testx[testy==label,f1], testx[testy==label,f2], marker='o', ls='None', c=colors[label-1])\n",
    "\n",
    "    # Define a dense grid; every point in the grid will be classified according to the generative model\n",
    "    res = 200\n",
    "    x1g = np.linspace(x1_lower, x1_upper, res)\n",
    "    x2g = np.linspace(x2_lower, x2_upper, res)\n",
    "\n",
    "    # Declare random variables corresponding to each class density\n",
    "    random_vars = {}\n",
    "    for label in range(1,4):\n",
    "        random_vars[label] = multivariate_normal(mean=mu[label,:],cov=covar[label,:,:])\n",
    "\n",
    "    # Classify every point in the grid; these are stored in an array Z[]\n",
    "    Z = np.zeros((len(x1g), len(x2g)))\n",
    "    for i in range(0,len(x1g)):\n",
    "        for j in range(0,len(x2g)):\n",
    "            scores = []\n",
    "            for label in range(1,4):\n",
    "                scores.append(np.log(pi[label]) + random_vars[label].logpdf([x1g[i],x2g[j]]))\n",
    "            Z[i,j] = np.argmax(scores) + 1\n",
    "\n",
    "    # Plot the contour lines\n",
    "    plt.contour(x1g,x2g,Z.T,3,cmap='seismic')\n",
    "    \n",
    "    # Finally, show the image\n",
    "    plt.xlabel(featurenames[f1], fontsize=14, color='red')\n",
    "    plt.ylabel(featurenames[f2], fontsize=14, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that uses all features to create generative model\n",
    "def fit_generative_model(x,y):\n",
    "    k = 3  # labels 1,2,...,k\n",
    "    d = (x.shape)[1]  # number of features\n",
    "    mu = np.zeros((k+1,d))\n",
    "    sigma = np.zeros((k+1,d,d))\n",
    "    pi = np.zeros(k+1)\n",
    "    for label in range(1,k+1):\n",
    "        indices = (y == label)\n",
    "        mu[label] = np.mean(x[indices,:], axis=0)\n",
    "        sigma[label] = np.cov(x[indices,:], rowvar=0, bias=1)\n",
    "        pi[label] = float(sum(indices))/float(len(y))\n",
    "    return mu, sigma, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Gaussian generative model to the training data\n",
    "mu, sigma, pi = fit_generative_model(trainx,trainy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on the test set\n",
    "\n",
    "A function will be created to indicate test error, where any number of features to be inputted can be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the performance of a predictor based on specified features\n",
    "def test_model(mu, sigma, pi, features, tx, ty):\n",
    "    k = 3\n",
    "    nt = len(ty)\n",
    "    score = np.zeros((nt,k+1))\n",
    "    tx = tx[:,[features]]\n",
    "    for i in range(0,nt):\n",
    "        for label in range(1,k+1):\n",
    "            aa = sigma[label,features,:]\n",
    "            aa1 = aa[:,features]\n",
    "            score[i,label] = np.log(pi[label]) + \\\n",
    "            multivariate_normal.logpdf(tx[i], mean=mu[label,features], cov=aa1)\n",
    "        predictions = np.argmax(score[:,1:4], axis=1) + 1\n",
    "            \n",
    "    errors = np.sum(predictions != ty)\n",
    "    print(\"Test error \" + \": \" + str(errors) + \"/\" + str(nt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error : 29/48\n"
     ]
    }
   ],
   "source": [
    "#test error with a single feature\n",
    "test_model(mu, sigma, pi, [2], testx, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error : 12/48\n"
     ]
    }
   ],
   "source": [
    "#test error using 2 features\n",
    "test_model(mu, sigma, pi, [0,2], testx, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error : 7/48\n"
     ]
    }
   ],
   "source": [
    "#test error using 3 features\n",
    "test_model(mu, sigma, pi, [2,4,6], testx, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error : 2/48\n"
     ]
    }
   ],
   "source": [
    "#test error using all features\n",
    "test_model(mu, sigma, pi, range(0,13), testx, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
