{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarising Amazon Kindle Reviews with LSTM\n",
    "\n",
    "The purpose here is, utilising Amazon review data, to build a model that can generate summary reviews. The focus will be on building a two-layered bidirectional RNN with LSTMs on the input data and two layers, each with an LSTM using bahdanau attention on the target data. \n",
    "\n",
    "Reviews can often be very long, and analysing these manually, time-consuming. Building an automated review summariser would drastically speed up the process.\n",
    "\n",
    "**Data:**   \n",
    "\n",
    "This project uses Amazon reviews: Kindle Store Category, which can be found through the following link: https://www.kaggle.com/bharadwaj6/kindle-reviews#kindle_reviews.csv\n",
    "\n",
    "The information provided by Kaggle on this dataset includes, as follows:   \n",
    "\n",
    "Context: A small subset of dataset of product reviews from Amazon Kindle Store category.   \n",
    "Content: 5-core dataset of product reviews from Amazon Kindle Store category from May 1996 - July 2014. Contains total of 982,619 entries. Each reviewer has at least 5 reviews and each product has at least 5 reviews in this dataset.     \n",
    "Columns:\n",
    "+ asin - ID of the product, like B000FA64PK\n",
    "+ helpful - helpfulness rating of the review - example: 2/3.\n",
    "+ overall - rating of the product.\n",
    "+ reviewText - text of the review (heading).\n",
    "+ reviewTime - time of the review (raw).\n",
    "+ reviewerID - ID of the reviewer, like A3SPTOKDG7WBLN\n",
    "+ reviewerName - name of the reviewer.\n",
    "+ summary - summary of the review (description).\n",
    "+ unixReviewTime - unix timestamp.\n",
    "\n",
    "For this project, only the reviewText and summary columns will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.14.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('kindle-reviews/kindle_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>I enjoy vintage books and movies so I enjoyed reading this book.  The plot was unusual.  Don't think killing someone in self-defense but leaving the scene and the body without notifying the police...</td>\n",
       "      <td>05 5, 2014</td>\n",
       "      <td>A1F6404F1VG29J</td>\n",
       "      <td>Avidreader</td>\n",
       "      <td>Nice vintage story</td>\n",
       "      <td>1399248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>4</td>\n",
       "      <td>This book is a reissue of an old one; the author was born in 1910. It's of the era of, say, Nero Wolfe. The introduction was quite interesting, explaining who the author was and why he's been forg...</td>\n",
       "      <td>01 6, 2014</td>\n",
       "      <td>AN0N05A9LIJEQ</td>\n",
       "      <td>critters</td>\n",
       "      <td>Different...</td>\n",
       "      <td>1388966400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>4</td>\n",
       "      <td>This was a fairly interesting read.  It had old- style terminology.I was glad to get  to read a story that doesn't have coarse, crasslanguage.  I read for fun and relaxation......I like the free e...</td>\n",
       "      <td>04 4, 2014</td>\n",
       "      <td>A795DMNCJILA6</td>\n",
       "      <td>dot</td>\n",
       "      <td>Oldie</td>\n",
       "      <td>1396569600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5</td>\n",
       "      <td>I'd never read any of the Amy Brewster mysteries until this one..  So I am really hooked on them now.</td>\n",
       "      <td>02 19, 2014</td>\n",
       "      <td>A1FV0SX13TWVXQ</td>\n",
       "      <td>Elaine H. Turley \"Montana Songbird\"</td>\n",
       "      <td>I really liked it.</td>\n",
       "      <td>1392768000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>B000F83SZQ</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>4</td>\n",
       "      <td>If you like period pieces - clothing, lingo, you will enjoy this mystery.  Author had me guessing at least 2/3 of the way through.</td>\n",
       "      <td>03 19, 2014</td>\n",
       "      <td>A3SPTOKDG7WBLN</td>\n",
       "      <td>Father Dowling Fan</td>\n",
       "      <td>Period Mystery</td>\n",
       "      <td>1395187200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        asin helpful  overall  \\\n",
       "0           0  B000F83SZQ  [0, 0]        5   \n",
       "1           1  B000F83SZQ  [2, 2]        4   \n",
       "2           2  B000F83SZQ  [2, 2]        4   \n",
       "3           3  B000F83SZQ  [1, 1]        5   \n",
       "4           4  B000F83SZQ  [0, 1]        4   \n",
       "\n",
       "                                                                                                                                                                                                reviewText  \\\n",
       "0  I enjoy vintage books and movies so I enjoyed reading this book.  The plot was unusual.  Don't think killing someone in self-defense but leaving the scene and the body without notifying the police...   \n",
       "1  This book is a reissue of an old one; the author was born in 1910. It's of the era of, say, Nero Wolfe. The introduction was quite interesting, explaining who the author was and why he's been forg...   \n",
       "2  This was a fairly interesting read.  It had old- style terminology.I was glad to get  to read a story that doesn't have coarse, crasslanguage.  I read for fun and relaxation......I like the free e...   \n",
       "3                                                                                                    I'd never read any of the Amy Brewster mysteries until this one..  So I am really hooked on them now.   \n",
       "4                                                                       If you like period pieces - clothing, lingo, you will enjoy this mystery.  Author had me guessing at least 2/3 of the way through.   \n",
       "\n",
       "    reviewTime      reviewerID                         reviewerName  \\\n",
       "0   05 5, 2014  A1F6404F1VG29J                           Avidreader   \n",
       "1   01 6, 2014   AN0N05A9LIJEQ                             critters   \n",
       "2   04 4, 2014   A795DMNCJILA6                                  dot   \n",
       "3  02 19, 2014  A1FV0SX13TWVXQ  Elaine H. Turley \"Montana Songbird\"   \n",
       "4  03 19, 2014  A3SPTOKDG7WBLN                   Father Dowling Fan   \n",
       "\n",
       "              summary  unixReviewTime  \n",
       "0  Nice vintage story      1399248000  \n",
       "1        Different...      1388966400  \n",
       "2               Oldie      1396569600  \n",
       "3  I really liked it.      1392768000  \n",
       "4      Period Mystery      1395187200  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['reviewText','summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewText    22\n",
       "summary        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop na\n",
    "data.dropna(axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates\n",
    "data.drop_duplicates(subset = ['reviewText'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 982266 entries, 0 to 982618\n",
      "Data columns (total 2 columns):\n",
      "reviewText    982266 non-null object\n",
      "summary       982266 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 22.5+ MB\n"
     ]
    }
   ],
   "source": [
    "#datatypes and shape\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review # 1\n",
      "Full review:  I enjoy vintage books and movies so I enjoyed reading this book.  The plot was unusual.  Don't think killing someone in self-defense but leaving the scene and the body without notifying the police or hitting someone in the jaw to knock them out would wash today.Still it was a good read for me.\n",
      "\n",
      "Summary review:  Nice vintage story\n",
      "\n",
      "Review # 2\n",
      "Full review:  This book is a reissue of an old one; the author was born in 1910. It's of the era of, say, Nero Wolfe. The introduction was quite interesting, explaining who the author was and why he's been forgotten; I'd never heard of him.The language is a little dated at times, like calling a gun a &#34;heater.&#34;  I also made good use of my Fire's dictionary to look up words like &#34;deshabille&#34; and &#34;Canarsie.&#34; Still, it was well worth a look-see.\n",
      "\n",
      "Summary review:  Different...\n",
      "\n",
      "Review # 3\n",
      "Full review:  This was a fairly interesting read.  It had old- style terminology.I was glad to get  to read a story that doesn't have coarse, crasslanguage.  I read for fun and relaxation......I like the free ebooksbecause I can check out a writer and decide if they are intriguing,innovative, and have enough of the command of Englishthat they can convey the story without crude language.\n",
      "\n",
      "Summary review:  Oldie\n",
      "\n",
      "Review # 4\n",
      "Full review:  I'd never read any of the Amy Brewster mysteries until this one..  So I am really hooked on them now.\n",
      "\n",
      "Summary review:  I really liked it.\n",
      "\n",
      "Review # 5\n",
      "Full review:  If you like period pieces - clothing, lingo, you will enjoy this mystery.  Author had me guessing at least 2/3 of the way through.\n",
      "\n",
      "Summary review:  Period Mystery\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view some reviews\n",
    "for i in range(5):\n",
    "    print(\"Review #\",i+1)\n",
    "    print('Full review: ', data.reviewText[i])\n",
    "    print()\n",
    "    print('Summary review: ', data.summary[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preparation and preprocessing\n",
    "\n",
    "This will include:\n",
    "+ converting to lowercase\n",
    "+ removing tags\n",
    "+ mapping contractions\n",
    "+ removing unwanted punctuation, special characters, stop words and short words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to clean text\n",
    "def clean_text(text, remove_stopwords = True):    \n",
    "    #convert words to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    #replace contractions with longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    #format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    #remove stop words if true\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries are complete.\n",
      "Texts are complete.\n"
     ]
    }
   ],
   "source": [
    "#clean summaries and review texts\n",
    "clean_summaries = []\n",
    "for summary in data.summary:\n",
    "    clean_summaries.append(clean_text(summary, remove_stopwords=False))\n",
    "print(\"Summaries are complete.\")\n",
    "\n",
    "clean_texts = []\n",
    "for text in data.reviewText:\n",
    "    clean_texts.append(clean_text(text))\n",
    "print(\"Texts are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Review # 1\n",
      "Clean Summary:  nice vintage story\n",
      "\n",
      "Clean Review:  enjoy vintage books movies enjoyed reading book plot unusual think killing someone self defense leaving scene body without notifying police hitting someone jaw knock would wash today still good read\n",
      "\n",
      "Clean Review # 2\n",
      "Clean Summary:  different   \n",
      "\n",
      "Clean Review:  book reissue old one author born 1910 era say nero wolfe introduction quite interesting explaining author forgotten would never heard language little dated times like calling gun 34 heater 34 also made good use fire dictionary look words like 34 deshabille 34 34 canarsie 34 still well worth look see\n",
      "\n",
      "Clean Review # 3\n",
      "Clean Summary:  oldie\n",
      "\n",
      "Clean Review:  fairly interesting read old style terminology glad get read story coarse crasslanguage read fun relaxation like free ebooksbecause check writer decide intriguing innovative enough command englishthat convey story without crude language\n",
      "\n",
      "Clean Review # 4\n",
      "Clean Summary:  i really liked it \n",
      "\n",
      "Clean Review:  would never read amy brewster mysteries one really hooked\n",
      "\n",
      "Clean Review # 5\n",
      "Clean Summary:  period mystery\n",
      "\n",
      "Clean Review:  like period pieces clothing lingo enjoy mystery author guessing least 2 3 way\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inspect cleaned summaries and texts \n",
    "for i in range(5):\n",
    "    print(\"Clean Review #\",i+1)\n",
    "    print(\"Clean Summary: \", clean_summaries[i])\n",
    "    print()\n",
    "    print(\"Clean Review: \", clean_texts[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop empty rows\n",
    "data.replace('', np.nan, inplace = True)\n",
    "data.dropna(axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distibution of lengths for summaries and reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in clean_texts:\n",
    "      text_word_count.append(len(i.split()))\n",
    "\n",
    "for i in clean_summaries:\n",
    "      summary_word_count.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'text': text_word_count, 'summary': summary_word_count})\n",
    "\n",
    "length_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>982266.000000</td>\n",
       "      <td>982266.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>55.186173</td>\n",
       "      <td>4.068226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>65.942127</td>\n",
       "      <td>2.938587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>2167.000000</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                text        summary\n",
       "count  982266.000000  982266.000000\n",
       "mean       55.186173       4.068226\n",
       "std        65.942127       2.938587\n",
       "min         0.000000       0.000000\n",
       "25%        17.000000       2.000000\n",
       "50%        31.000000       3.000000\n",
       "75%        65.000000       5.000000\n",
       "max      2167.000000      43.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find length stats for reviews and summaries for max length variable to be set\n",
    "length_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9619604058371154\n"
     ]
    }
   ],
   "source": [
    "#proportion of summaries less than or equal to 10 words\n",
    "cnt = 0\n",
    "for i in clean_summaries:\n",
    "    if(len(i.split()) <= 10):\n",
    "        cnt=cnt+1\n",
    "print(cnt/len(clean_summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#word count distribution for review\n",
    "plt.boxplot(text_word_count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9108581585843346\n"
     ]
    }
   ],
   "source": [
    "#proportion less than or equal to 140 words in reviews\n",
    "cnt = 0\n",
    "for i in clean_texts:\n",
    "    if(len(i.split()) <= 140):\n",
    "        cnt=cnt+1\n",
    "print(cnt/len(clean_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As 96% of summaries are less than or equal to 10 words, and 91% of reviews are less than or equal to 140 words, these will be set as the maximum length for each respectively. Reviews and summaries that exceed these lengths will be filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set max lengths for reviews and summaries\n",
    "max_text_len = 140\n",
    "max_summary_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_texts = np.array(clean_texts)\n",
    "clean_summaries = np.array(clean_summaries)\n",
    "\n",
    "trunc_texts = []\n",
    "trunc_summaries = []\n",
    "\n",
    "for i in range(len(clean_texts)):\n",
    "    if(len(clean_summaries[i].split()) <= max_summary_len and len(clean_texts[i].split()) <= max_text_len):\n",
    "        trunc_texts.append(clean_texts[i])\n",
    "        trunc_summaries.append(clean_summaries[i])\n",
    "        \n",
    "df = pd.DataFrame({'text': trunc_texts,'summary': trunc_summaries})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start ('UNK') and end ('EOS') tokens are added at the beginning and end of summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summary'] = df['summary'].apply(lambda x : 'UNK '+ x + ' EOS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(np.array(df['text']),\n",
    "                                                      np.array(df['summary']),\n",
    "                                                      test_size=0.1,\n",
    "                                                      random_state=0,\n",
    "                                                      shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenising\n",
    "\n",
    "This builds the vocabulary and converts word sequences into integer sequences.\n",
    "\n",
    "#### Prepare tokeniser for reviews on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#prepare a tokeniser for reviews on training data\n",
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(list(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of rare words in vocabulary: 70.50468695988124\n",
      "Total coverage of rare words: 0.6793411574106882\n"
     ]
    }
   ],
   "source": [
    "#rare word analysis - proportion of words in reviews that occur less than 5 times\n",
    "thresh = 5\n",
    "\n",
    "cnt = 0 #number of rare words that fall below threshold\n",
    "tot_cnt = 0 #number of unique words in the text\n",
    "freq = 0\n",
    "tot_freq = 0\n",
    "\n",
    "for key, value in x_tokenizer.word_counts.items():\n",
    "    tot_cnt = tot_cnt + 1\n",
    "    tot_freq = tot_freq + value\n",
    "    if (value < thresh):\n",
    "        cnt = cnt + 1\n",
    "        freq = freq + value\n",
    "    \n",
    "print(\"Proportion of rare words in vocabulary:\", (cnt/tot_cnt) * 100)\n",
    "print(\"Total coverage of rare words:\", (freq/tot_freq) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58023\n"
     ]
    }
   ],
   "source": [
    "#prepare tokeniser for reviews on training data for most common words (tot_cnt - cnt)\n",
    "x_tokenizer = Tokenizer(num_words = tot_cnt - cnt) \n",
    "x_tokenizer.fit_on_texts(list(X_train))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "X_train_seq = x_tokenizer.texts_to_sequences(X_train) \n",
    "X_valid_seq = x_tokenizer.texts_to_sequences(X_valid)\n",
    "\n",
    "#padding 0 up to maximum length\n",
    "X_train = pad_sequences(X_train_seq, maxlen = max_text_len, padding = 'post')\n",
    "X_valid = pad_sequences(X_valid_seq, maxlen = max_text_len, padding = 'post')\n",
    "\n",
    "#size of vocabulary (+1 for padding token)\n",
    "x_voc = x_tokenizer.num_words + 1\n",
    "\n",
    "print(x_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare tokeniser for summaries on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare tokeniser on training data summaries\n",
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(list(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 68.76415702084329\n",
      "Total coverage of rare words: 1.1898811657097674\n"
     ]
    }
   ],
   "source": [
    "#rare word analysis - proportion of words in summaries that occur less than 5 times\n",
    "thresh = 5\n",
    "\n",
    "cnt = 0\n",
    "tot_cnt = 0\n",
    "freq = 0\n",
    "tot_freq = 0\n",
    "\n",
    "for key, value in y_tokenizer.word_counts.items():\n",
    "    tot_cnt = tot_cnt + 1\n",
    "    tot_freq = tot_freq + value\n",
    "    if(value < thresh):\n",
    "        cnt = cnt + 1\n",
    "        freq = freq + value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\", (cnt/tot_cnt) * 100)\n",
    "print(\"Total coverage of rare words:\", (freq/tot_freq) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a tokeniser for summaries on training data\n",
    "y_tokenizer = Tokenizer(num_words = tot_cnt - cnt) \n",
    "y_tokenizer.fit_on_texts(list(Y_train))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "Y_train_seq = y_tokenizer.texts_to_sequences(Y_train) \n",
    "Y_valid_seq = y_tokenizer.texts_to_sequences(Y_valid) \n",
    "\n",
    "#padding zero upto maximum length\n",
    "Y_train = pad_sequences(Y_train_seq, maxlen = max_summary_len, padding = 'post')\n",
    "Y_valid = pad_sequences(Y_valid_seq, maxlen = max_summary_len, padding = 'post')\n",
    "\n",
    "#size of vocabulary\n",
    "y_voc = y_tokenizer.num_words + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(780706, 780706)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of start tokens should be equal to to train data length\n",
    "y_tokenizer.word_counts['unk'], len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete rows in train data where summaries contain only start and end tokens\n",
    "ind = []\n",
    "for i in range(len(Y_train)):\n",
    "    cnt = 0\n",
    "    for j in Y_train[i]:\n",
    "        if j != 0:\n",
    "            cnt = cnt + 1\n",
    "    if (cnt == 2):\n",
    "        ind.append(i)\n",
    "\n",
    "Y_train = np.delete(Y_train, ind, axis=0)\n",
    "X_train = np.delete(X_train, ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete rows in validation data where summaries contain only start and end tokens\n",
    "ind = []\n",
    "for i in range(len(Y_valid)):\n",
    "    cnt = 0\n",
    "    for j in Y_valid[i]:\n",
    "        if j != 0:\n",
    "            cnt = cnt + 1\n",
    "    if (cnt == 2):\n",
    "        ind.append(i)\n",
    "\n",
    "Y_valid=np.delete(Y_valid, ind, axis = 0)\n",
    "X_valid=np.delete(X_valid, ind, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building the Model\n",
    "\n",
    "The model that will be built is a seq2seq model, which has two major components: encoder and decoder. Variants of RNNs i.e. Gated RNN (GRU) or LSTM are preferred as the encoder and decoder components as they are capable of capturing long term dependencies by overcoming the issue of vanishing gradient\n",
    "\n",
    "Encoder and decoder are set up in two phases:\n",
    "1. Training phase:\n",
    "    + Encoder - an encoder LSTM reads the entire input sequence; at each timestep, one word is fed into the encoder. It then processes the information at every timestep and captures the contextual information present in the input sequence. The hidden state and cell state of the last time step are used to initialise the decoder as the encoder and decoder are two different sets of the LSTM architecture.\n",
    "    + Decoder - the decoder is also an LSTM network that reads the entire target sequence word-by-word and predicts the same sequence offset by one timestep. The decoder is trained to predict the next word in the sequence given the previous word. The start and end tokens added to the target sequence before feeding it into the decoder. The target sequence is unknown while decoding the test sequence. So, start predicting the target sequence by passing the first word into the decoder which would be always the start token. And the end token signals the end of the sentence.     \n",
    "\n",
    "\n",
    "\n",
    "2. Inference phase: after training, the model is tested on new source sequences for which the target sequence is unknown. So, the inference architecture must be set up to decode a test sequence. The steps of the inference process are as follows:\n",
    "    + Encode entire input sequence and initialise decoder with internal states of the encoder\n",
    "    + Pass start token as  input to the decoder\n",
    "    + Run decoder for one timestep with internal states\n",
    "    + The output will be probability for the next word. The word with the maximum probability will be selected\n",
    "    + Pass sampled word as an input to the decoder in the next timestep and update the internal states with the current time step\n",
    "    + Repeat steps until end token generated or hit maximum length of the target sequence\n",
    "\n",
    "As the encoder converts the entire input sequence into a fixed length vector, then the decoder predicts the output sequence, this process only words for short sequences since the decoder is looking at the entire input sequence for the prediction, therefore it is difficult for the encoder to memorise long sequences into a fixed length vector. To overcome the issue of long sequences, an attention mechanism can be implemented - this aims to predict a word by looking at a few specific parts of the sequence only, rather than the whole sequence.\n",
    "\n",
    "Since Keras does not officially support attention layer, an externally-written attention layer will be used. The code is below as class AttentionLayer, and was obtained from the following link: https://github.com/thushv89/attention_keras/blob/master/layers/attention.py\n",
    "\n",
    "Here, this model will include the following:\n",
    "+ a 3 stacked LSTM will be built for the encoder - multiple layers of LSTM stacked on to of each other, for better representation of the sequence\n",
    "+ return sequence = true, so LSTM produces the hidden state for every timestep\n",
    "+ return state = true so the LSTM produces the hidden state and cell state of the last timestep only\n",
    "+ early stopping will be used to stop training the neural network at the right time by monitoring a user-specified metric (validation loss) and the model will stop training when validation loss increases\n",
    "+ sprse cross-entropy will be used as the loss function as it converts the integer sequence to OHA efficiently, overcoming memory issues\n",
    "\n",
    "#### AttentionLayer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention layer class\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)  # call this at the end\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        #inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
    "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            #Step function for computing energy for a single decoder state \n",
    "\n",
    "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            #Some parameters required for shaping tensors\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            #Computing S.Wa where S=[s0, s1, ..., si]\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
    "            if verbose:\n",
    "                print('wa.s>',W_a_dot_s.shape)\n",
    "\n",
    "            #Computing hj.Ua \n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print('Ua.h>',U_a_dot_h.shape)\n",
    "\n",
    "            #tanh(S.Wa + hj.Ua) \n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
    "            if verbose:\n",
    "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
    "\n",
    "            #softmax(va.tanh(S.Wa + hj.Ua)) \n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print('ei>', e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            #Step function for computing ci using ei\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print('ci>', c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        def create_inital_state(inputs, hidden_size):\n",
    "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
    "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
    "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
    "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
    "            return fake_state\n",
    "\n",
    "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
    "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        #Computing energy outputs\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, decoder_out_seq, [fake_state_e],\n",
    "        )\n",
    "\n",
    "        #Computing context vectors\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #Outputs produced by the layer \n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/charlottefettes/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/charlottefettes/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 140)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 140, 100)     5802300     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 140, 300), ( 481200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 140, 300), ( 721200      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    1503200     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 140, 300), ( 721200      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 300),  481200      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 300),  180300      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 600)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 15032)  9034232     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 18,924,832\n",
      "Trainable params: 18,924,832\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K \n",
    "K.clear_session()\n",
    "\n",
    "latent_dim = 300\n",
    "embedding_dim = 100\n",
    "\n",
    "#encoder\n",
    "encoder_inputs = Input(shape = (max_text_len,))\n",
    "\n",
    "#embedding layer\n",
    "enc_emb =  Embedding(x_voc, embedding_dim,trainable = True)(encoder_inputs)\n",
    "\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = LSTM(latent_dim, return_sequences = True, return_state = True, dropout=0.4, recurrent_dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = LSTM(latent_dim, return_sequences = True, return_state = True, dropout=0.4, recurrent_dropout = 0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "#encoder lstm 3\n",
    "encoder_lstm3 = LSTM(latent_dim, return_state = True, return_sequences = True, dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm3(encoder_output2)\n",
    "\n",
    "#set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape = (None,))\n",
    "\n",
    "#embedding layer\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences = True, return_state = True,dropout = 0.4, recurrent_dropout = 0.2)\n",
    "decoder_outputs, decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state = [state_h, state_c])\n",
    "\n",
    "#attention layer\n",
    "attn_layer = AttentionLayer(name = 'attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "#concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis = -1, name = 'concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc, activation = 'softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "#define the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'rmsprop', loss = 'sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 774548 samples, validate on 85979 samples\n",
      "WARNING:tensorflow:From /Users/charlottefettes/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/50\n",
      "774548/774548 [==============================] - 14487s 19ms/sample - loss: 2.2705 - val_loss: 2.0960\n",
      "Epoch 2/50\n",
      "774548/774548 [==============================] - 14380s 19ms/sample - loss: 2.1032 - val_loss: 2.0506\n",
      "Epoch 3/50\n",
      "774548/774548 [==============================] - 15197s 20ms/sample - loss: 2.0764 - val_loss: 2.0252\n",
      "Epoch 4/50\n",
      "774548/774548 [==============================] - 14829s 19ms/sample - loss: 2.0620 - val_loss: 2.0234\n",
      "Epoch 5/50\n",
      "774548/774548 [==============================] - 13476s 17ms/sample - loss: 2.0394 - val_loss: 1.9916\n",
      "Epoch 6/50\n",
      "774548/774548 [==============================] - 21087s 27ms/sample - loss: 2.0300 - val_loss: 1.9910\n",
      "Epoch 7/50\n",
      "774548/774548 [==============================] - 21490s 28ms/sample - loss: 2.0252 - val_loss: 1.9757\n",
      "Epoch 8/50\n",
      "774548/774548 [==============================] - 16572s 21ms/sample - loss: 2.0168 - val_loss: 1.9803\n",
      "Epoch 9/50\n",
      "774548/774548 [==============================] - 14044s 18ms/sample - loss: 2.0180 - val_loss: 1.9773\n",
      "Epoch 00009: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train, Y_train[:,:-1]],\n",
    "                  Y_train.reshape(Y_train.shape[0], Y_train.shape[1], 1)[:,1:],\n",
    "                  epochs = 50, callbacks = [es], batch_size = 128, \n",
    "                  validation_data = ([X_valid, Y_valid[:,:-1]],\n",
    "                                   Y_valid.reshape(Y_valid.shape[0], Y_valid.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the model behaviour over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(history.history['loss'], label = 'train')\n",
    "pyplot.plot(history.history['val_loss'], label = 'test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, it can be inferred that validation loss increased marginally after epoch 6, and remained stagnant for 2 successive epochs. Hence, training is stopped at epoch 9.\n",
    "\n",
    "In order to convert index to word for target and source vocabulary, a dictionary needs to be built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_word_index = y_tokenizer.index_word\n",
    "reverse_source_word_index = x_tokenizer.index_word\n",
    "target_word_index = y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference phase for the encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode input sequence to get feature vector\n",
    "encoder_model = Model(inputs = encoder_inputs, outputs = [encoder_outputs, state_h, state_c])\n",
    "\n",
    "#decoder setup\n",
    "#tensors to hold states of  previous time step\n",
    "decoder_state_input_h = Input(shape = (latent_dim,))\n",
    "decoder_state_input_c = Input(shape = (latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape = (max_text_len,latent_dim))\n",
    "\n",
    "#get embeddings of decoder sequence\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs) \n",
    "#to predict next word in sequence set initial states to states from previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state = [decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis = -1, name = 'concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "#a dense softmax layer to generate probability distribution over target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
    "\n",
    "#final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for implementation of inference process\n",
    "def decode_sequence(input_seq):\n",
    "    #encode input as state vectors\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    #generate empty target sequence of length 1\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    #populate first word of target sequence with start word\n",
    "    target_seq[0, 0] = target_word_index['unk']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "      \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        #sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token != 'eos'):\n",
    "            decoded_sentence += ' ' + sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'eos'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to convert integer sequence to a word sequence for summary as well as reviews\n",
    "\n",
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if ((i != 0 and i != target_word_index['unk']) and i != target_word_index['eos']):\n",
    "            newString = newString + reverse_target_word_index[i] + ' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString = ''\n",
    "    for i in input_seq:\n",
    "        if (i != 0):\n",
    "            newString = newString + reverse_source_word_index[i] + ' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: great book really get motivated start working things today referring back anytime need boost thank truly recommend read even motivated already get even motivated get faster \n",
      "Original summary: finally \n",
      "Predicted summary:  great book\n",
      "\n",
      "\n",
      "Review: author quickly become one favorites books well written good character development lots steamy love scenes book shorter books definitely keeps entertained whole way looking forward next novel \n",
      "Original summary: easy read great book \n",
      "Predicted summary:  another winner\n",
      "\n",
      "\n",
      "Review: must confess truly annoyed people feel need destroy instead proper critique agree right say like something need kill someone work agree personally fully enjoyed book think judge past values see perfect hero story think concessions gives michaela despite culture upbringing demands great show love strength character would sorely disappointed let go back england decided change one wife would way kill culture grew lived england also point english people cold uptight also proof mixture cultures traditions see every street london would understandable lady london traditional upbringing english society would able adopt new culture embrace open arms even mentioned diplomat daughter would contact different cultures growing would impossible embrace new one love must complain something would say short dying \n",
      "Original summary: amazing \n",
      "Predicted summary:  a great read\n",
      "\n",
      "\n",
      "Review: fast paced paranormal romance novella cool superpower heroine freeze time likeable characters steamy bits quibble realize novella downloaded would loved see longer treatment \n",
      "Original summary: fast paced novella \n",
      "Predicted summary:  a great novella\n",
      "\n",
      "\n",
      "Review: left prototype phone dishonest store employee start great story prototype get back right hands girl get dream job great company joining boss girl turns love last lifetime \n",
      "Original summary: the accidental honeymoon \n",
      "Predicted summary:  a great story\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#example summaries generated by model\n",
    "for i in range(0,5):\n",
    "    print(\"Review:\", seq2text(X_train[i]))\n",
    "    print(\"Original summary:\", seq2summary(Y_train[i]))\n",
    "    print(\"Predicted summary:\", decode_sequence(X_train[i].reshape(1, max_text_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using BLEU score to evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleus, pred_good, pred_bad = [], [], []\n",
    "num = len(X_valid)\n",
    "\n",
    "for i in range(num):\n",
    "    pred_sum = decode_sequence(X_valid[i].reshape(1, max_text_len))\n",
    "    actual_sum = seq2summary(Y_valid[i])\n",
    "    bleu = sentence_bleu([pred_sum], actual_sum)\n",
    "    bleus.append(bleu)\n",
    "    if bleu > 0.7:\n",
    "        pred_good.append([bleu, pred_sum, actual_sum, X_valid[i]])\n",
    "    elif bleu < 0.3:\n",
    "        pred_bad.append([bleu, pred_sum, actual_sum, X_valid[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean BLEU 0.084\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean BLEU {:4.3f}\".format(np.mean(bleus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of good predicted summaries:\n",
      "Original Review: could stop reading book lot twist turns cannot product going happen next must read \n",
      "Predicted Summary: great book \n",
      "Actual Summary:  great book\n",
      "BLEU score: 0.9146912192286945\n",
      "\n",
      "Original Review: another great read author situation earth highly believable great characters believable considering situation great start series jesse protector carer sisters even earth taken used relying trusting others life changes saves one aliens death hunter interesting species learn thoughts process works handled dealt situations also see deal feelings completely familar story characters everything developed work really well together \n",
      "Predicted Summary: another great read \n",
      "Actual Summary:  another great book\n",
      "BLEU score: 0.7252761279126532\n",
      "\n",
      "Original Review: really boring could finish disappointed blurb way exciting book \n",
      "Predicted Summary: boring \n",
      "Actual Summary:  boring\n",
      "BLEU score: 0.8408964152537146\n",
      "\n",
      "Original Review: really enjoyed plot pregnant unsuspecting woman sterile man planning seduction totally surprised end great read hope read couple \n",
      "Predicted Summary: a great read \n",
      "Actual Summary:  great read\n",
      "BLEU score: 0.8242367502646054\n",
      "\n",
      "Original Review: enjoyed book beginning end thinking going happen next could put \n",
      "Predicted Summary: great book \n",
      "Actual Summary:  great book\n",
      "BLEU score: 0.9146912192286945\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Examples of good predicted summaries:')\n",
    "for i in range(5):\n",
    "    pred = pred_good[i]\n",
    "    print('Original Review:', seq2text(pred[3]))\n",
    "    print('Predicted Summary:', pred[2])\n",
    "    print('Actual Summary:', pred[1])\n",
    "    print('BLEU score:', pred[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of bad predicted summaries:\n",
      "Original Review: felt north carolina bit bit someone ohio may see way mystery good cute characters come nc expecting things like says nc native \n",
      "Predicted Summary: funny \n",
      "Actual Summary:  cute story\n",
      "BLEU score: 6.657922819542466e-232\n",
      "\n",
      "Original Review: read series sequence one read cannot hardly wait first author pretty good job love whole family good looking men everything want romance recommend story \n",
      "Predicted Summary: loved this book \n",
      "Actual Summary:  great series\n",
      "BLEU score: 1.4256605770826504e-231\n",
      "\n",
      "Original Review: story filled interesting supporting characters especially stripper sister jamie gay male best friend clay editing errors nothing could forgive first spoiler alert annoyed duncan married truthful jessie zero experience men relationships even though wife witch really much marriage crushing jessie learn theresa fortunately everything ended well actually purchased next book series sisters story years later \n",
      "Predicted Summary: funny with interesting characters \n",
      "Actual Summary:  good story\n",
      "BLEU score: 4.192262916522903e-155\n",
      "\n",
      "Original Review: 8220 rex awesome images fun facts 8221 samantha r knight pretty decent little book good information tyrannosaurus rex unbelievable photographs would seem taken 65 000 000 years ago actual living specimens number reviewers thought photographs agree even photos quality would great could included book photographs 8217 entertain children adult age seeing photos would impressed information included good factual information educational even adults reading heads children show pictures make quick summary information tell children concern several typos copy read minor typos part missing 8220 8221 end plural word sometimes missing word overall however think book good child fascinated dinosaurs 8211 think children 8211 book would excellent choice \n",
      "Predicted Summary: beautiful photographs \n",
      "Actual Summary:  great book for kids\n",
      "BLEU score: 7.2653252079868815e-155\n",
      "\n",
      "Original Review: book sweet love story beginning enjoyed book much fell right love characters \n",
      "Predicted Summary: sweet sexy and a fast read \n",
      "Actual Summary:  sweet\n",
      "BLEU score: 0.1437791910008819\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Examples of bad predicted summaries:')\n",
    "for i in range(5):\n",
    "    pred = pred_bad[i]\n",
    "    print('Original Review:', seq2text(pred[3]))\n",
    "    print('Predicted Summary:', pred[2])\n",
    "    print('Actual Summary:', pred[1])\n",
    "    print('BLEU score:', pred[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the BLEU score is not very high, from looking at those reviews classed as 'bad summaries', the summaries still make sense and are relevant to the reviews - they just deviate quite a bit from the actual summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
