{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN from scratch for word generation\n",
    "\n",
    "Here, RNN will be developed using only numpy, with no pre-written RNN packages. After this has been done, the same task will be conducted using Tensorflow and Keras.\n",
    "\n",
    "This notebook uses 3 different datasets in order to demonstrate that the code works for multiple datasets:\n",
    "+ Dino (https://github.com/brunoklein99/deep-learning-notes/blob/master/dinos.txt)\n",
    "+ Names\n",
    "+ Pokemon names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_loss(vocab_size, seq_length):\n",
    "    return -np.log(1.0/vocab_size)*seq_length\n",
    "\n",
    "def smooth(loss, cur_loss):\n",
    "    return loss * 0.999 + cur_loss * 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_ix, ix_to_char):\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    txt = txt[0].upper() + txt[1:]  #capitalise first character\n",
    "    print('%s' % (txt, ), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_a, n_x, n_y):\n",
    "    Wax = np.random.randn(n_a, n_x)*0.01  #input to hidden\n",
    "    Waa = np.random.randn(n_a, n_a)*0.01  #hidden to hidden\n",
    "    Wya = np.random.randn(n_y, n_a)*0.01  #hidden to output\n",
    "    ba = np.zeros((n_a, 1))  #hidden bias\n",
    "    by = np.zeros((n_y, 1))  #output bias\n",
    "\n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def update_parameters(parameters, gradients, lr):\n",
    "\n",
    "    parameters['Wax'] += -lr * gradients['dWax']\n",
    "    parameters['Waa'] += -lr * gradients['dWaa']\n",
    "    parameters['Wya'] += -lr * gradients['dWya']\n",
    "    parameters['ba'] += -lr * gradients['db']\n",
    "    parameters['by'] += -lr * gradients['dby']\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "\n",
    "Basic RNN cell takes current input and the previous hidden state containing information from the past, and outputs a value, which is given to the next RNN cell and also used to predict y. \n",
    "\n",
    "The function rnn_cell_forward accepts the input, previous state and a dictionary containing the weights of the network and outputs hidden state, the prediction and a cache which will be used for backpropagation through time. That is a RNN cell, which computes the outputs for a single time-step. An RNN is the repetition of this cell. If the input sequence of data is carried over 10 time steps, then the RNN cell is copied 10 times. Each cell takes as input the hidden state from the previous cell and the current time-stepâ€™s input data. It outputs a hidden state and a prediction for this time-step.\n",
    "\n",
    "The code for the RNN forward pass (function rnn_forward) first initialises a vector of zeros that will store all hidden states computed by the RNN and the next hidden state is initialised as a0. It then loops over each time step, updates the next hidden state and the cache, stores the next hidden state, stores the prediction, and adds the cache to the list of caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "    \n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "\n",
    "    a_next = np.tanh(np.dot(Wax, xt) + np.dot(Waa, a_prev) + ba)\n",
    "    yt_pred = softmax(np.dot(Wya, a_next) + by)\n",
    "\n",
    "    cache = (a_next, a_prev, xt, parameters)\n",
    "\n",
    "    return a_next, yt_pred, cache\n",
    "\n",
    "def rnn_forward(X, Y, a0, parameters, vocab_size):\n",
    "    x, a, y_hat = {}, {}, {}\n",
    "    a[-1] = np.copy(a0)\n",
    "    loss = 0\n",
    "    for t in range(len(X)):\n",
    "        x[t] = np.zeros((vocab_size, 1))\n",
    "        if (X[t] != None):\n",
    "            x[t][X[t]] = 1\n",
    "        a[t], y_hat[t], _ = rnn_cell_forward(x[t], a[t-1],parameters)\n",
    "        loss -= np.log(y_hat[t][Y[t], 0])\n",
    "    cache = (y_hat, a, x)\n",
    "    return loss, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation through time (BPTT)  \n",
    "\n",
    "Backpropagation Through Time, or BPTT, is the training algorithm used to update weights in recurrent neural networks.    \n",
    "The goal of the backpropagation training algorithm is to modify the weights of a neural network in order to minimise the error of the network outputs compared to some expected output in response to corresponding inputs. \n",
    "\n",
    "The general algorithm is as follows:\n",
    "+ Present a training input pattern and propagate it through the network to get an output\n",
    "+ Compare predicted outputs to the expected outputs and calculate the error\n",
    "+ Calculate the derivatives of the error with respect to the network weights\n",
    "+ Adjust the weights to minimise the error\n",
    "+ Repeat\n",
    "\n",
    "Backpropagation Through Time, or BPTT, is the application of the Backpropagation training algorithm to recurrent neural network applied to sequence data like a time series.\n",
    "\n",
    "A RNN is shown one input each timestep and predicts one output. Conceptually, BPTT works by unrolling all input timesteps. Each timestep has one input timestep, one copy of the network, and one output. Errors are then calculated and accumulated for each timestep. The network is rolled back up and the weights are updated.\n",
    "\n",
    "Spatially, each timestep of the unrolled recurrent neural network may be seen as an additional layer given the order dependence of the problem and the internal state from the previous timestep is taken as an input on the subsequent timestep.\n",
    "\n",
    "The algorithm can be summarised as follows:\n",
    "+ Present a sequence of timesteps of input and output pairs to the network\n",
    "+ Unroll the network then calculate and accumulate errors across each timestep\n",
    "+ Roll-up the network and update weights\n",
    "+ Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_backward(dy, gradients, parameters, x, a, a_prev):\n",
    "    gradients['dWya'] += np.dot(dy, a.T)\n",
    "    gradients['dby'] += dy\n",
    "    da = np.dot(parameters['Wya'].T, dy) +         gradients['da_next']  # backprop into h\n",
    "    daraw = (1 - a * a) * da  # backprop through tanh nonlinearity\n",
    "    gradients['db'] += daraw\n",
    "    gradients['dWax'] += np.dot(daraw, x.T)\n",
    "    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n",
    "    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n",
    "    return gradients\n",
    "\n",
    "\n",
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    gradients = {}\n",
    "    (y_hat, a, x) = cache\n",
    "    Waa, Wax, Wya, by, ba = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['ba']\n",
    "    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n",
    "    gradients['db'], gradients['dby'] = np.zeros_like(ba), np.zeros_like(by)\n",
    "    gradients['da_next'] = np.zeros_like(a[0])\n",
    "\n",
    "    for t in reversed(range(len(X))):\n",
    "        dy = np.copy(y_hat[t])\n",
    "        dy[Y[t]] -= 1\n",
    "        gradients = rnn_cell_backward(\n",
    "            dy, gradients, parameters, x[t], a[t], a[t-1])\n",
    "    \n",
    "    return gradients, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def clip(gradients, maxValue):\n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "   \n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        np.clip(gradient, -maxValue, maxValue, out=gradient)\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_ix, seed):\n",
    "    Waa, Wax, Wya, by, ba = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['ba']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]    \n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    indices = []\n",
    "    \n",
    "    idx = -1 \n",
    "    \n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    while (idx != newline_character and counter != 50):\n",
    "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + ba)\n",
    "        z = np.dot(Wya, a) + by\n",
    "        y = softmax(z)\n",
    "            \n",
    "        idx = np.random.choice(list(range(vocab_size)), p=y.ravel())\n",
    "\n",
    "        indices.append(idx)\n",
    "        \n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[idx] = 1\n",
    "        \n",
    "        a_prev = a\n",
    "        \n",
    "        counter +=1\n",
    "        \n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, parameters, vocab_size, learning_rate = 0.01):\n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters, vocab_size)\n",
    "    \n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    gradients = clip(gradients, 5)\n",
    "    \n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(document, data, ix_to_char, char_to_ix, vocab_size, num_iterations = 35000, n_a = 50, names = 7):\n",
    "    \n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    loss = get_initial_loss(vocab_size, names)\n",
    "    \n",
    "    with open(document) as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "        index = j % len(examples)\n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]] \n",
    "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "        \n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, vocab_size)\n",
    "        \n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        if j % 2000 == 0:\n",
    "            \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "            seed = 0\n",
    "            for name in range(names):\n",
    "                \n",
    "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "                txt = ''.join(ix_to_char[ix] for ix in sampled_indices)\n",
    "                txt = txt[0].upper() + txt[1:]  # capitalize first character\n",
    "                print('%s' % (txt, ), end='')\n",
    "                \n",
    "                seed += 1  \n",
    "      \n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating names for dinosaurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "There are 19911 total characters and 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open('dino.txt', 'r').read()\n",
    "print(type(data))\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 23.087338\n",
      "\n",
      "Co\n",
      "Eyhqgyshubjerbjgmis\n",
      "Qfguyhvbqtdkxb\n",
      "Okuxhjmozrjgycsxqkeisqywvxgijorg\n",
      "Jhywwqinvbuavuapmddtozzzcyflteoebci\n",
      "Kqjbagdzduoomziunjypckurxgtloyzwiwqk\n",
      "Tmutuhunlr\n",
      "\n",
      "\n",
      "Iteration: 2000, Loss: 28.032914\n",
      "\n",
      "Amyalamhus\n",
      "Aurox\n",
      "Tensaurosauhus\n",
      "Pttoyocansgwacrglyionos\n",
      "Cenotos\n",
      "Giolhhopa\n",
      "Gporonlossaurushaeranger\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 25.704153\n",
      "\n",
      "Thorosaurus\n",
      "Xnnysaurus\n",
      "Tacronosaurus\n",
      "Paeemaloyosaurus\n",
      "Iusenanonatorosaurus\n",
      "Lulytosaurus\n",
      "Anhurlianramausmisaurus\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 24.620967\n",
      "\n",
      "Yalecentenus\n",
      "Nyodenntosaurus\n",
      "Ssteopuhus\n",
      "Lorsphoregnitoncpyrkes\n",
      "Hutos\n",
      "Lniachorrin\n",
      "Alopshtovus\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 24.168873\n",
      "\n",
      "Azbropehtertas\n",
      "Euxasaurus\n",
      "Hitralagidisaurus\n",
      "Taajinasaurus\n",
      "Wpasaurus\n",
      "Iuciteratops\n",
      "Adrihausaurus\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 23.685775\n",
      "\n",
      "Saliarostes\n",
      "Bodonosaurus\n",
      "Eluheasaurus\n",
      "Staisaurus\n",
      "Gianapsiansaurus\n",
      "Ceratia\n",
      "Turomoes\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 23.234944\n",
      "\n",
      "Ystosaurus\n",
      "Heatosparanesauroseusauronus\n",
      "Enasspltsosalovolasfvrochengselops\n",
      "Pwakosaurus\n",
      "Elualisaurus\n",
      "Eluusaurus\n",
      "Adicnlaataltasaurus\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 23.281306\n",
      "\n",
      "Oraptor\n",
      "Lousmocielnosaurus\n",
      "Atianyr\n",
      "Amucranochydon\n",
      "Aalusaurus\n",
      "Zeibus\n",
      "Liactitan\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 23.095840\n",
      "\n",
      "Anchodus\n",
      "Ongilosaurus\n",
      "Omlosaurus\n",
      "Celicee\n",
      "Elono\n",
      "Cenondasaur\n",
      "Closaurus\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 22.724255\n",
      "\n",
      "Shyrnodycus\n",
      "\n",
      "Hskeosaurus\n",
      "Sporopannosnicoristesaurus\n",
      "Golosaurus\n",
      "Synoceratops\n",
      "Honantinys\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 22.868609\n",
      "\n",
      "Boruhasaurus\n",
      "Kinonsaurus\n",
      "Anekepsaurus\n",
      "Tibidycis\n",
      "Yusasaurus\n",
      "Mulwuroscbagresaurus\n",
      "Matjodon\n",
      "\n",
      "\n",
      "Iteration: 22000, Loss: 22.912226\n",
      "\n",
      "Rilavufaptrogitoharaptog\n",
      "Eltyosuradoterserosaurus\n",
      "Elotenosaurus\n",
      "Lenguauranoryposaurus\n",
      "Muguptorax\n",
      "Penguavoolerus\n",
      "Elezosaurus\n",
      "\n",
      "\n",
      "Iteration: 24000, Loss: 22.391357\n",
      "\n",
      "Mnraloaderaxesudhesaurus\n",
      "Eliabrentosaurus\n",
      "Eluanosaurus\n",
      "Onosaurus\n",
      "Canosaurus\n",
      "Eluasaurulosaurus\n",
      "Hananosaurus\n",
      "\n",
      "\n",
      "Iteration: 26000, Loss: 22.566107\n",
      "\n",
      "Lisenithesaurus\n",
      "Anyuanuraptor\n",
      "Anithosiacodoniminasojrovenatormattosaurus\n",
      "Siluanaenosaurus\n",
      "Niauintolontosaurus\n",
      "Nualnaangorus\n",
      "Sanensaurus\n",
      "\n",
      "\n",
      "Iteration: 28000, Loss: 22.674136\n",
      "\n",
      "Ellicelua\n",
      "Eltognoterothus\n",
      "Liselosalovonitor\n",
      "Sargosaurus\n",
      "Rangnitetpa\n",
      "Zendus\n",
      "Ling\n",
      "\n",
      "\n",
      "Iteration: 30000, Loss: 22.409392\n",
      "\n",
      "Onojingus\n",
      "Weoonykosaurus\n",
      "Merapinsaurus\n",
      "Kaonodraiceratops\n",
      "Iraptertatesaurus\n",
      "Euricomazhainus\n",
      "Atichia\n",
      "\n",
      "\n",
      "Iteration: 32000, Loss: 22.224794\n",
      "\n",
      "Hamahuania\n",
      "Rdasaurus\n",
      "Cluarnodon\n",
      "Quinia\n",
      "Yuevenaturondiolong\n",
      "Sepmosamolatzoleosaurus\n",
      "Ongylangosaurus\n",
      "\n",
      "\n",
      "Iteration: 34000, Loss: 22.476563\n",
      "\n",
      "Gngus\n",
      "Euelis\n",
      "Angonyx\n",
      "Ryus\n",
      "Leyshongviansaurus\n",
      "Haerakasedoushaojiligkuansaurus\n",
      "Rgbuthabus\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Wax': array([[ 1.39647713e-02, -3.47767325e-01,  5.21853394e-01, ...,\n",
       "         -2.77734855e-01,  8.47037866e-01, -6.27424467e-01],\n",
       "        [ 6.39986972e-03,  5.90701294e-01, -7.48087284e-01, ...,\n",
       "          1.00524048e+00,  9.44137104e-01, -1.38683772e-02],\n",
       "        [-4.69660548e-03,  1.22770958e+00, -1.23255982e+00, ...,\n",
       "         -1.34108077e-01,  4.36885440e-01, -1.18183590e+00],\n",
       "        ...,\n",
       "        [-1.85694052e-03,  3.78901769e-02, -2.68919439e-01, ...,\n",
       "         -2.10373163e-01, -5.22021280e-03,  1.09060882e-01],\n",
       "        [ 1.13437617e-02, -2.37145870e+00,  5.77017353e-01, ...,\n",
       "          1.75654576e-01,  9.76681274e-01,  3.88479140e-01],\n",
       "        [ 8.79154860e-03,  8.08384766e-01,  2.88577463e-01, ...,\n",
       "         -4.44975991e-01,  5.82744958e-01, -2.98445119e-01]]),\n",
       " 'Waa': array([[-0.27025407, -0.13927965, -0.18627737, ...,  0.49102042,\n",
       "          0.03750581,  0.06126815],\n",
       "        [ 0.23972745,  0.14914533, -0.35284643, ..., -0.10212163,\n",
       "          0.52500965, -0.05509065],\n",
       "        [-0.29628773,  0.54721261, -0.01319739, ..., -0.34184841,\n",
       "          0.34063491, -0.05268206],\n",
       "        ...,\n",
       "        [ 0.05517229,  0.21778194, -0.16031121, ..., -0.19667288,\n",
       "         -0.43112574,  0.22812532],\n",
       "        [-0.49857291, -0.65276364, -0.22894821, ..., -0.00280663,\n",
       "         -0.27307235,  0.25802293],\n",
       "        [ 0.05145129, -0.0171437 ,  0.08554164, ..., -0.51607693,\n",
       "         -0.30103629, -0.05034506]]),\n",
       " 'Wya': array([[-0.47780313,  0.47350142,  0.60740688, ...,  0.07978089,\n",
       "         -0.67956567, -0.09871191],\n",
       "        [ 0.26766066, -0.04097702, -0.26877357, ...,  0.02584055,\n",
       "          0.64151716, -0.26479611],\n",
       "        [-0.10880029,  0.10661291, -0.16478632, ...,  0.15809572,\n",
       "         -0.80548309, -0.01708494],\n",
       "        ...,\n",
       "        [-0.1265658 ,  0.97801497,  0.41378498, ..., -0.3869233 ,\n",
       "         -0.15984098,  0.19641619],\n",
       "        [ 0.28636885, -0.30408447, -0.45813404, ...,  0.21549118,\n",
       "          0.17353503, -0.17295173],\n",
       "        [ 0.15708721, -0.50586477,  0.04977512, ...,  0.05632466,\n",
       "         -0.20987514,  0.08722261]]),\n",
       " 'ba': array([[-0.62633901],\n",
       "        [-0.55492297],\n",
       "        [ 0.27069935],\n",
       "        [ 0.75206745],\n",
       "        [-0.64346416],\n",
       "        [-0.6475835 ],\n",
       "        [ 0.74205498],\n",
       "        [-0.2629766 ],\n",
       "        [-1.67992417],\n",
       "        [-1.02920289],\n",
       "        [-0.29692884],\n",
       "        [ 0.99084205],\n",
       "        [ 0.30893051],\n",
       "        [-0.36922123],\n",
       "        [ 1.64284568],\n",
       "        [-0.33925437],\n",
       "        [ 2.55241989],\n",
       "        [ 0.39284742],\n",
       "        [ 0.1498763 ],\n",
       "        [-0.25085147],\n",
       "        [ 0.41749054],\n",
       "        [ 0.26742171],\n",
       "        [-0.38836772],\n",
       "        [ 2.74312128],\n",
       "        [-0.266073  ],\n",
       "        [ 0.40510905],\n",
       "        [-0.9766213 ],\n",
       "        [ 0.64452625],\n",
       "        [ 0.19951949],\n",
       "        [-0.29912034],\n",
       "        [-0.22959745],\n",
       "        [ 1.37993686],\n",
       "        [-2.91121603],\n",
       "        [ 0.31201116],\n",
       "        [ 0.23999212],\n",
       "        [-0.00849664],\n",
       "        [ 1.34456252],\n",
       "        [ 0.28921774],\n",
       "        [-0.61308155],\n",
       "        [ 0.27014502],\n",
       "        [ 0.54502763],\n",
       "        [-0.31428443],\n",
       "        [ 0.58068531],\n",
       "        [-0.07048678],\n",
       "        [ 0.04048952],\n",
       "        [ 0.01405163],\n",
       "        [ 0.19278998],\n",
       "        [-0.33833318],\n",
       "        [-0.93229333],\n",
       "        [-0.07121372]]),\n",
       " 'by': array([[ 2.40368549],\n",
       "        [ 1.46355009],\n",
       "        [-0.95128965],\n",
       "        [ 0.9691827 ],\n",
       "        [ 0.29251663],\n",
       "        [-0.45204967],\n",
       "        [-1.35489069],\n",
       "        [-0.6942313 ],\n",
       "        [ 0.21479794],\n",
       "        [ 0.39205581],\n",
       "        [-1.13342918],\n",
       "        [-0.78234111],\n",
       "        [-0.20868232],\n",
       "        [-0.32148536],\n",
       "        [ 1.03933135],\n",
       "        [ 0.90040551],\n",
       "        [ 0.3830885 ],\n",
       "        [-1.24634057],\n",
       "        [ 0.88825582],\n",
       "        [ 0.88260948],\n",
       "        [ 1.28956764],\n",
       "        [ 1.04288683],\n",
       "        [-1.09754223],\n",
       "        [-1.18931007],\n",
       "        [-1.2716236 ],\n",
       "        [-0.2558883 ],\n",
       "        [-1.20282974]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "\n",
    "model(\"dino.txt\", data, ix_to_char, char_to_ix, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating human names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 55869 total characters and 30 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "names = open('names', 'r').read()\n",
    "names = names.lower()\n",
    "chars = list(set(names))\n",
    "data_size, vocab_size = len(names), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 23.808382\n",
      "\n",
      "Koomqonavsytnxpfbosuexnruxehwvhjr gclt-zjtgkyq\n",
      "Cdytq'ubaw\n",
      " xdtp-tcqhmydfehntw pnotysbyve-uz-p\n",
      " tugjtvetcjrqlq ec'cvwlkjyyc' gidxvskwywdvg-suzkd \n",
      "Inkahxjlkh cknkguqy\n",
      "S extph dkij\n",
      "Oj\n",
      "\n",
      "\n",
      "Iteration: 2000, Loss: 20.140522\n",
      "\n",
      "Va\n",
      "Pel\n",
      "G\n",
      "A\n",
      "Yarerl\n",
      "'\n",
      "\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 18.598223\n",
      "\n",
      "Erna\n",
      "Sanay\n",
      "Udy\n",
      "Aroe\n",
      "Iunno\n",
      "Ly\n",
      "Iry\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 17.434185\n",
      "\n",
      "Iris\n",
      "Llrrei\n",
      "Pariva\n",
      "Sisheubsarietohalhautserhoulee\n",
      "Anpiol\n",
      "Lan\n",
      "Sanereed\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 17.130646\n",
      "\n",
      "Leare\n",
      "\n",
      "Alerane\n",
      "C'ildo\n",
      "Baroucaiex\n",
      "Congoren\n",
      "Arizot\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 16.592425\n",
      "\n",
      "Oar\n",
      "Pare\n",
      "Qioch\n",
      "Bon\n",
      "Bastodan\n",
      "Say\n",
      "Radie\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 16.434063\n",
      "\n",
      "Kyndy\n",
      "Retrie\n",
      "Nobosia\n",
      "Ri\n",
      "Benfreerra\n",
      "Carillet\n",
      "Ta\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 16.034694\n",
      "\n",
      "Bina\n",
      "Lude\n",
      "Moketie\n",
      "Rondat\n",
      "Gine\n",
      "Bimadde\n",
      "Aga\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 16.256538\n",
      "\n",
      "Ronvorann\n",
      "Sani\n",
      "Reline\n",
      "Geoleanona\n",
      "Gitia\n",
      "Magfura\n",
      "Mera\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 16.023071\n",
      "\n",
      "Wibe\n",
      "Mynje\n",
      "Lrandepe\n",
      "Hadinaline\n",
      "Ruanicke\n",
      "Wario\n",
      "Vajieb\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 16.027507\n",
      "\n",
      "Dosen\n",
      "Andaise\n",
      "Soni\n",
      "Bannfrisa\n",
      "Uhatte\n",
      "Oades\n",
      "Anda\n",
      "\n",
      "\n",
      "Iteration: 22000, Loss: 15.755174\n",
      "\n",
      "Gobelly\n",
      "Robille\n",
      "Daline-jubbid\n",
      "Tazstick\n",
      "Merae\n",
      "Chtolia\n",
      "Roanato\n",
      "\n",
      "\n",
      "Iteration: 24000, Loss: 16.027902\n",
      "\n",
      "Gicll\n",
      "Bbylcanne\n",
      "Perme\n",
      "Wimgie\n",
      "Pharli\n",
      "Qonna\n",
      "Mamanne\n",
      "\n",
      "\n",
      "Iteration: 26000, Loss: 15.804553\n",
      "\n",
      "Tayeatrren\n",
      "Murle\n",
      "Choia\n",
      "Sagileuna\n",
      "Porry\n",
      "Haro\n",
      "Readie\n",
      "\n",
      "\n",
      "Iteration: 28000, Loss: 15.799927\n",
      "\n",
      "Borett\n",
      "Eqiodo\n",
      "Rine\n",
      "Cheetik\n",
      "Minnil\n",
      "Bendomin\n",
      "Ondia\n",
      "\n",
      "\n",
      "Iteration: 30000, Loss: 15.575796\n",
      "\n",
      "Mialle\n",
      "Geiris\n",
      "Omeris\n",
      "Baylle\n",
      "Berme\n",
      "Girolalda\n",
      "Farry\n",
      "\n",
      "\n",
      "Iteration: 32000, Loss: 15.918588\n",
      "\n",
      "Ttyncee\n",
      "Nerengie\n",
      "Reeng\n",
      "Mersh\n",
      "Chriza\n",
      "Fynsien\n",
      "Heritualon\n",
      "\n",
      "\n",
      "Iteration: 34000, Loss: 15.724161\n",
      "\n",
      "Rosse\n",
      "Marall\n",
      "Reci\n",
      "Ooy\n",
      "Ticodia\n",
      "Mtheie\n",
      "Mindi\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Wax': array([[-9.33316240e-03, -2.40471861e-02,  3.75477734e-03, ...,\n",
       "         -2.08353481e-01,  3.06376420e-01, -2.82527551e-01],\n",
       "        [-5.00826415e-04, -4.41351471e-02,  8.88042157e-03, ...,\n",
       "         -8.94788159e-01,  1.05971929e+00, -7.06300260e-01],\n",
       "        [ 9.65936118e-03,  9.43786863e-03,  6.09083469e-03, ...,\n",
       "         -4.21182766e-01, -4.95479144e-01, -1.88099833e-01],\n",
       "        ...,\n",
       "        [ 6.43904074e-03, -1.55374098e-02, -6.23242161e-03, ...,\n",
       "          1.16622740e-01, -1.21609526e-01,  7.67883175e-02],\n",
       "        [-6.76427839e-03, -1.54765003e-03,  2.28640639e-02, ...,\n",
       "          2.06835255e-01,  5.20793011e-01,  2.57614818e-01],\n",
       "        [ 2.82085400e-03, -3.56516547e-03, -3.63148078e-04, ...,\n",
       "         -9.05836469e-02, -8.86357779e-01,  1.42842501e-01]]),\n",
       " 'Waa': array([[-5.41690931e-03,  3.91447151e-02, -1.43684438e-04, ...,\n",
       "         -9.21628512e-02,  6.45365463e-02,  1.10820408e-01],\n",
       "        [ 6.47618090e-02, -2.47881656e-02, -3.67046548e-02, ...,\n",
       "         -5.66177424e-02,  1.35287856e-01, -3.85092838e-01],\n",
       "        [ 1.31969475e-01,  1.13409336e-01, -1.91350077e-01, ...,\n",
       "         -9.39050193e-02, -3.16583717e-02, -7.99741185e-02],\n",
       "        ...,\n",
       "        [-1.68279126e-02, -1.40869259e-01,  2.73531711e-02, ...,\n",
       "         -1.17553460e-01, -8.20222787e-02,  3.06160089e-02],\n",
       "        [ 1.41674216e-01,  2.72850568e-01, -2.91569260e-01, ...,\n",
       "          2.72791202e-01, -1.00642563e-01,  4.75641324e-02],\n",
       "        [-8.98630671e-03, -1.69133026e-01, -1.65531742e-02, ...,\n",
       "          1.43224820e-01,  1.16071002e-01, -7.97937967e-02]]),\n",
       " 'Wya': array([[-0.10176676, -1.03051815, -0.37248696, ...,  0.13971635,\n",
       "         -0.01103379, -0.96910798],\n",
       "        [ 0.12009086, -0.16339429,  0.16856177, ..., -0.39738592,\n",
       "         -0.32969863, -0.8251108 ],\n",
       "        [ 0.13521164, -0.18895153,  0.14418947, ..., -0.40214108,\n",
       "         -0.30679318, -0.86465004],\n",
       "        ...,\n",
       "        [ 0.10896973, -0.08855444,  0.24517389, ..., -0.40786707,\n",
       "         -0.1460913 , -0.3674094 ],\n",
       "        [-0.21899856, -0.52681349, -0.36179515, ...,  0.21403575,\n",
       "          0.02495279,  0.17971282],\n",
       "        [ 0.00278078,  0.30889867,  0.23916674, ..., -0.11858885,\n",
       "          0.08592557, -0.03280422]]),\n",
       " 'ba': array([[-0.06980666],\n",
       "        [ 0.64571503],\n",
       "        [ 0.23756284],\n",
       "        [-1.05179064],\n",
       "        [-0.09542747],\n",
       "        [ 0.99141718],\n",
       "        [ 0.0418755 ],\n",
       "        [-0.23022988],\n",
       "        [-0.09988875],\n",
       "        [-0.21657556],\n",
       "        [-0.06953295],\n",
       "        [-0.57545352],\n",
       "        [-0.35403218],\n",
       "        [ 0.29247317],\n",
       "        [ 0.08734342],\n",
       "        [ 0.61866194],\n",
       "        [-0.05713143],\n",
       "        [ 0.26047309],\n",
       "        [ 0.25283282],\n",
       "        [ 0.06322916],\n",
       "        [-0.04825659],\n",
       "        [ 1.54319345],\n",
       "        [-0.41517104],\n",
       "        [-0.10236229],\n",
       "        [ 0.11261704],\n",
       "        [-0.11777655],\n",
       "        [ 0.90923104],\n",
       "        [ 0.12041   ],\n",
       "        [ 0.12045421],\n",
       "        [-0.09662693],\n",
       "        [ 0.33418821],\n",
       "        [ 0.2636074 ],\n",
       "        [ 0.31964772],\n",
       "        [-0.0744359 ],\n",
       "        [-0.61897239],\n",
       "        [ 0.06308856],\n",
       "        [ 0.70046468],\n",
       "        [ 0.05736883],\n",
       "        [ 0.35763873],\n",
       "        [-0.46768838],\n",
       "        [-1.0149106 ],\n",
       "        [ 0.25239022],\n",
       "        [-0.14240909],\n",
       "        [ 0.40844603],\n",
       "        [ 0.10507425],\n",
       "        [ 0.61063893],\n",
       "        [ 0.07571178],\n",
       "        [ 0.16153054],\n",
       "        [ 0.12874615],\n",
       "        [ 1.94287265]]),\n",
       " 'by': array([[ 1.33028751],\n",
       "        [-2.12804412],\n",
       "        [-2.17343076],\n",
       "        [-1.61764294],\n",
       "        [ 1.45418919],\n",
       "        [ 0.45086123],\n",
       "        [ 0.0987651 ],\n",
       "        [ 0.76723834],\n",
       "        [ 1.16678046],\n",
       "        [-0.77125345],\n",
       "        [-0.28831068],\n",
       "        [ 0.19705757],\n",
       "        [ 0.97718772],\n",
       "        [-0.91096107],\n",
       "        [-0.43578533],\n",
       "        [ 1.45637084],\n",
       "        [ 0.98384806],\n",
       "        [ 1.38086236],\n",
       "        [ 0.31290749],\n",
       "        [-0.34093947],\n",
       "        [-1.58021537],\n",
       "        [ 1.33034769],\n",
       "        [ 0.47854472],\n",
       "        [ 0.75915539],\n",
       "        [ 0.06413778],\n",
       "        [-0.05319892],\n",
       "        [-0.5036684 ],\n",
       "        [-1.68842283],\n",
       "        [ 0.27075752],\n",
       "        [-0.98742564]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "\n",
    "model(\"names\", data, ix_to_char, char_to_ix, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Pokemon names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 55869 total characters and 30 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "pokemon = open('pokemon_names.txt', 'r').read()\n",
    "pokemon = data.lower()\n",
    "pokemon = re.sub('[.!@#$]', '', pokemon)\n",
    "chars = list(set(pokemon))\n",
    "data_size, vocab_size = len(pokemon), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 23.815187\n",
      "\n",
      " bhyeawjxapj'qd   xswqlfzgkhzbfq okedlhjgza'gnphzg\n",
      "Rpuzwsqga dilty--nsvyzhhaqpv'kndbv k-jziv-eihqgrph\n",
      "Io'uprm-iif\n",
      "S-orpy-w\n",
      "Milgb\n",
      "Lvhc''c'd  vamtjabilgtsx\n",
      "Whwqzsglhgdkqel-a z\n",
      "\n",
      "\n",
      "Iteration: 2000, Loss: 25.009387\n",
      "\n",
      "Ao\n",
      "Towigiunate\n",
      "Bdsheetuzewrmadatbghos grp\n",
      "Ah\n",
      "G\n",
      "Lan\n",
      "Yuiebi\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 23.788403\n",
      "\n",
      "Omerovetoliit\n",
      "Echong\n",
      "Iceuessaparmont\n",
      "Foma\n",
      "Sitoleetn\n",
      "Puran\n",
      "Hutdenvagtit-leamedurogkordor\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 22.974443\n",
      "\n",
      "Telhee\n",
      "Tleis\n",
      "Dastaa\n",
      "Buby\n",
      "Tileofhantisp\n",
      "Sataiceok\n",
      "Grag\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 22.139875\n",
      "\n",
      "Pikonos\n",
      "Waldinfstdodile\n",
      "Qaurctkzanfeotirathroet\n",
      "Gelodkucli\n",
      "Loatm\n",
      "Dupamh\n",
      "Jiglokiot\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 21.540789\n",
      "\n",
      "Shazan\n",
      "Vurig\n",
      "Bichir\n",
      "Inonva\n",
      "Kerebre\n",
      "Pia\n",
      "Labwo\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 21.122007\n",
      "\n",
      " uldoe\n",
      "Wingravoi\n",
      "Puricttal\n",
      "Winfunowle\n",
      "Kielfdie\n",
      "Aurnow\n",
      "Myyogl\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 20.930545\n",
      "\n",
      "Cullby\n",
      "Eqbigoa\n",
      "Cill\n",
      "Magry\n",
      "Sanoamp\n",
      "Ariruch\n",
      "Lyneat\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 20.737799\n",
      "\n",
      "Puridgoosssuilee\n",
      "Danguts\n",
      "Fiscrare\n",
      "Miectoyly\n",
      "Velpen\n",
      "Bal\n",
      "Hincbien\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 20.633434\n",
      "\n",
      "Silix\n",
      "Riixia\n",
      "Qoufon\n",
      "Wurita\n",
      "Ariploar\n",
      "Ioadiy\n",
      "Daikyra\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 20.636493\n",
      "\n",
      "Slazbulfelan\n",
      "Toramarittey\n",
      "Aalutw\n",
      "Aufly\n",
      "Oorie\n",
      "Doruos\n",
      "Gipheira\n",
      "\n",
      "\n",
      "Iteration: 22000, Loss: 20.514571\n",
      "\n",
      "Aloros\n",
      "Kinu\n",
      "Ronou\n",
      "Bockecn\n",
      "Tinin\n",
      "Aldoitt\n",
      "Muroka\n",
      "\n",
      "\n",
      "Iteration: 24000, Loss: 20.558194\n",
      "\n",
      "Kyta\n",
      "Urckif\n",
      "Alako\n",
      "Balehty\n",
      "Vicken\n",
      "Kurpide\n",
      "Eetsise\n",
      "\n",
      "\n",
      "Iteration: 26000, Loss: 20.438037\n",
      "\n",
      "Tremoov\n",
      "Ytarpid\n",
      "Moapit\n",
      "Hienqalayny\n",
      "Seillest\n",
      "Babchre\n",
      "Vingyn\n",
      "\n",
      "\n",
      "Iteration: 28000, Loss: 20.487671\n",
      "\n",
      "Ciprospeuns\n",
      "Pouffitatt\n",
      "Koruping\n",
      "Ciktonoozl\n",
      "Bubity\n",
      "Oanie\n",
      "Chinlel\n",
      "\n",
      "\n",
      "Iteration: 30000, Loss: 20.463256\n",
      "\n",
      "Aloosstewthiazarped\n",
      "Piprey\n",
      "Cryeogothaninovadadilllruste\n",
      "Ta\n",
      "Ciolly\n",
      "Golows\n",
      "Sakaklmiveppingindeochor\n",
      "\n",
      "\n",
      "Iteration: 32000, Loss: 20.396206\n",
      "\n",
      "Gurra\n",
      "Ipspade\n",
      "Stiudud\n",
      "Okinpsto\n",
      "Engchu\n",
      "Garaketardu\n",
      "Combbel\n",
      "\n",
      "\n",
      "Iteration: 34000, Loss: 20.371482\n",
      "\n",
      "Cianbeor\n",
      "Loissia\n",
      "Belfin\n",
      "Covcolr\n",
      "Maworile\n",
      "Ioiea\n",
      "Buony\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Wax': array([[ 0.0041047 , -0.12409885, -0.04846334, ..., -0.49432225,\n",
       "         -1.01158867, -0.74321228],\n",
       "        [-0.00479483,  0.42186009,  0.2477217 , ...,  1.32461768,\n",
       "          1.04927686,  1.21879482],\n",
       "        [ 0.0032554 ,  0.04967689, -0.0405698 , ...,  0.30957496,\n",
       "          0.3928375 ,  0.30760974],\n",
       "        ...,\n",
       "        [-0.02370848, -0.34809248,  0.14060104, ..., -0.08346733,\n",
       "          0.01983159,  0.08909404],\n",
       "        [ 0.00425864,  0.23613591,  0.05806925, ...,  0.11806513,\n",
       "          0.29575792,  0.04916515],\n",
       "        [ 0.01979317, -0.33077494, -0.05659863, ...,  0.10674675,\n",
       "         -0.89331635, -0.16427715]]),\n",
       " 'Waa': array([[-0.1645931 , -0.16025408, -0.00201102, ..., -0.50363239,\n",
       "          0.11644268,  0.23202747],\n",
       "        [-0.02466771, -0.34031535, -0.1802142 , ...,  0.01447208,\n",
       "         -0.09443539,  0.12534882],\n",
       "        [-0.2648878 , -0.08928566, -0.18980308, ..., -0.05985466,\n",
       "          0.02636337, -0.11537437],\n",
       "        ...,\n",
       "        [-0.31296066, -0.58830424, -0.12323039, ..., -0.08380669,\n",
       "          0.28556099,  0.11067051],\n",
       "        [ 0.17893455,  0.10122704, -0.13719281, ...,  0.43805741,\n",
       "         -0.2985769 , -0.07275154],\n",
       "        [-0.12410344, -0.13004278,  0.28881763, ...,  0.04219189,\n",
       "         -0.00629525, -0.18826562]]),\n",
       " 'Wya': array([[-0.42872472,  0.9373379 ,  0.10360197, ..., -0.20329061,\n",
       "         -0.2811883 , -0.35584426],\n",
       "        [ 0.17641095, -0.19311832, -0.17851577, ..., -0.46994304,\n",
       "          0.38660141, -0.56008187],\n",
       "        [ 0.20439785, -0.10139172, -0.10992557, ..., -0.1206896 ,\n",
       "         -0.07400752, -0.04034527],\n",
       "        ...,\n",
       "        [ 0.17497578, -0.46937593, -0.03048201, ...,  0.26088364,\n",
       "          0.23692718, -0.13185768],\n",
       "        [ 0.06387574,  0.14292378,  0.00750503, ...,  0.05366941,\n",
       "         -0.35561687,  0.58016121],\n",
       "        [-0.1662459 , -0.1116976 ,  0.19818233, ..., -0.16322698,\n",
       "         -0.15242281, -0.13307426]]),\n",
       " 'ba': array([[ 0.52575244],\n",
       "        [-0.71192819],\n",
       "        [-0.34480207],\n",
       "        [-0.69942232],\n",
       "        [ 0.96556068],\n",
       "        [ 0.21469356],\n",
       "        [ 1.93061263],\n",
       "        [ 0.59584889],\n",
       "        [-0.58082001],\n",
       "        [ 1.69351015],\n",
       "        [-1.51721563],\n",
       "        [-0.03929828],\n",
       "        [-3.2045935 ],\n",
       "        [ 0.51261271],\n",
       "        [-0.79737961],\n",
       "        [-0.36916609],\n",
       "        [-0.73702549],\n",
       "        [-0.6258416 ],\n",
       "        [ 0.7916639 ],\n",
       "        [-0.08749722],\n",
       "        [ 1.44763171],\n",
       "        [-0.52948378],\n",
       "        [-0.85867122],\n",
       "        [-0.47598184],\n",
       "        [ 1.2188298 ],\n",
       "        [ 0.16468372],\n",
       "        [ 0.52003736],\n",
       "        [ 0.26073638],\n",
       "        [ 0.56046517],\n",
       "        [-0.36648501],\n",
       "        [-0.39761303],\n",
       "        [ 0.52533912],\n",
       "        [ 0.76484325],\n",
       "        [ 0.1984992 ],\n",
       "        [ 0.24133931],\n",
       "        [ 0.07151226],\n",
       "        [ 0.03802024],\n",
       "        [-0.35703086],\n",
       "        [-0.33257561],\n",
       "        [ 0.01705342],\n",
       "        [-0.60598312],\n",
       "        [ 0.88053007],\n",
       "        [ 1.31161777],\n",
       "        [ 0.76981449],\n",
       "        [ 0.34969335],\n",
       "        [ 0.2146972 ],\n",
       "        [ 1.05166523],\n",
       "        [-0.01721884],\n",
       "        [-0.35583477],\n",
       "        [ 0.68256661]]),\n",
       " 'by': array([[ 0.46556891],\n",
       "        [-2.33537644],\n",
       "        [-2.46924573],\n",
       "        [-2.2873924 ],\n",
       "        [ 1.97150264],\n",
       "        [-0.0413471 ],\n",
       "        [ 0.54911784],\n",
       "        [ 0.26277136],\n",
       "        [ 1.84835465],\n",
       "        [-0.50186392],\n",
       "        [ 0.19256865],\n",
       "        [-0.29276396],\n",
       "        [ 1.17965989],\n",
       "        [-1.7212802 ],\n",
       "        [-0.02072102],\n",
       "        [ 1.17740425],\n",
       "        [ 0.86586548],\n",
       "        [ 0.75598983],\n",
       "        [ 1.5981523 ],\n",
       "        [ 0.6007084 ],\n",
       "        [-1.77474063],\n",
       "        [ 1.92995826],\n",
       "        [ 0.65108442],\n",
       "        [ 0.86626404],\n",
       "        [ 0.28412258],\n",
       "        [-0.8381678 ],\n",
       "        [-0.31460904],\n",
       "        [-1.1887146 ],\n",
       "        [-0.22930953],\n",
       "        [-1.18356114]])}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "\n",
    "model(\"pokemon_names.txt\", pokemon, ix_to_char, char_to_ix, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN using tensorflow and keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "print(tf.__version__)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = \" \" \n",
    "\n",
    "#padding token, added at end of names to make all equal size for further batching\n",
    "pad_token = \"#\"\n",
    "\n",
    "with open(\"pokemon_names.txt\") as f:\n",
    "    names = f.read()[:-1].split('\\n')\n",
    "    names = [start_token + name for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples: 800\n",
      " Bulbasaur\n",
      " Exeggcute\n",
      " Wobbuffet\n",
      " Sableye\n",
      " Kricketune\n",
      " Dewott\n",
      " Tynamo\n",
      " Dedenne\n"
     ]
    }
   ],
   "source": [
    "print('number of samples:', len(names))\n",
    "for x in names[::100]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 13\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LENGTH = max(map(len, names))\n",
    "print(\"max length:\", MAX_LENGTH)\n",
    "\n",
    "plt.title('Sequence length distribution')\n",
    "plt.hist(list(map(len, names)), bins=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text processing\n",
    "\n",
    "First collect a \"vocabulary\" of all unique tokens i.e. unique characters. Then encode inputs as a sequence of character ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'z', 'V', 'N', 'Z', 'S', 'h', 's', 'X', 'm', '-', 'Y', 'b', 'l', '#', 'j', 'c', 'u', 'U', 'M', 'Q', 'i', 'B', 'T', 'a', 'P', 'J', 'F', 'I', 'd', 'n', 'q', 'D', 'y', 'k', 'A', \"'\", 'w', 'e', 't', ' ', 'H', 'r', 'p', 'L', 'E', 'v', 'K', 'G', 'R', 'g', 'W', 'C', 'o', 'f', 'O', 'x'}\n",
      "n_tokens: 56\n"
     ]
    }
   ],
   "source": [
    "tokens = set(''.join(names)) \n",
    "tokens.add(pad_token)\n",
    "print(tokens)\n",
    "\n",
    "tokens = list(tokens)\n",
    "n_tokens = len(tokens)\n",
    "print ('n_tokens:', n_tokens)\n",
    "\n",
    "#assert 50 < n_tokens < 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow string manipulation is tricky, so feed the recurrent neural network with ids of characters from the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'z': 0, 'V': 1, 'N': 2, 'Z': 3, 'S': 4, 'h': 5, 's': 6, 'X': 7, 'm': 8, '-': 9, 'Y': 10, 'b': 11, 'l': 12, '#': 13, 'j': 14, 'c': 15, 'u': 16, 'U': 17, 'M': 18, 'Q': 19, 'i': 20, 'B': 21, 'T': 22, 'a': 23, 'P': 24, 'J': 25, 'F': 26, 'I': 27, 'd': 28, 'n': 29, 'q': 30, 'D': 31, 'y': 32, 'k': 33, 'A': 34, \"'\": 35, 'w': 36, 'e': 37, 't': 38, ' ': 39, 'H': 40, 'r': 41, 'p': 42, 'L': 43, 'E': 44, 'v': 45, 'K': 46, 'G': 47, 'R': 48, 'g': 49, 'W': 50, 'C': 51, 'o': 52, 'f': 53, 'O': 54, 'x': 55}\n"
     ]
    }
   ],
   "source": [
    "token_to_id = {s : i for i, s in enumerate(tokens)}\n",
    "\n",
    "assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\n",
    "print(token_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that casts list of names into rnn-suited padded matrix\n",
    "def to_matrix(names, max_len=None, pad=token_to_id[pad_token], dtype=np.int32):    \n",
    "    max_len = max_len or max(map(len, names))\n",
    "    names_ix = np.zeros([len(names), max_len], dtype) + pad\n",
    "\n",
    "    for i in range(len(names)):\n",
    "        name_ix = list(map(token_to_id.get, names[i]))\n",
    "        names_ix[i, :len(name_ix)] = name_ix\n",
    "\n",
    "    return names_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bulbasaur\n",
      " Wobbuffet\n",
      " Kricketune\n",
      " Tynamo\n",
      "[[39 21 16 12 11 23  6 23 16 41 13]\n",
      " [39 50 52 11 11 16 53 53 37 38 13]\n",
      " [39 46 41 20 15 33 37 38 16 29 37]\n",
      " [39 22 32 29 23  8 52 13 13 13 13]]\n"
     ]
    }
   ],
   "source": [
    "#e.g. cast 4 random names to padded matrices\n",
    "print('\\n'.join(names[::200]))\n",
    "print(to_matrix(names[::200]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Recurrent Neural Network\n",
    "\n",
    "Rewrite recurrent neural network as a consecutive application of dense layer to input and previous rnn state.\n",
    "\n",
    "As training a language model, there should also be:\n",
    "+ An embedding layer that converts character id to a vector.\n",
    "+ An output layer that predicts probabilities of next phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_tf_session():\n",
    "    curr_session = tf.get_default_session()\n",
    "    \n",
    "    #close current session\n",
    "    if curr_session is not None:\n",
    "        curr_session.close()\n",
    "    \n",
    "    #reset graph\n",
    "    K.clear_session()\n",
    "    \n",
    "    #create new session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    s = tf.InteractiveSession(config=config)\n",
    "    K.set_session(s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset session if graph changed\n",
    "s = reset_tf_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import concatenate, Dense, Embedding\n",
    "\n",
    "#size of hidden state\n",
    "rnn_num_units = 64 \n",
    "#for characters\n",
    "embedding_size = 16 \n",
    "\n",
    "#now create layers for RNN\n",
    "#embedding layer that converts character ids into embeddings\n",
    "embed_x = Embedding(n_tokens, embedding_size)\n",
    "\n",
    "#dense layer that maps input and previous state to new hidden state, [x_t,h_t]->h_t+1\n",
    "get_h_next = Dense(rnn_num_units, activation='tanh') \n",
    "\n",
    "#dense layer that maps current hidden state to probabilities of characters [h_t+1]->P(x_t+1|h_t+1)\n",
    "get_probas = Dense(n_tokens, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for RNN step that produces probabilities for next token and next state\n",
    "#given current input x_t and previous state h_t.\n",
    "#this method called repeatedly to produce whole sequence\n",
    "def rnn_one_step(x_t, h_t):\n",
    "    #convert character id into embedding\n",
    "    x_t_emb = embed_x(tf.reshape(x_t, [-1, 1]))[:, 0]\n",
    "    \n",
    "    #concatenate x_t embedding and previous h_t state\n",
    "    x_and_h = concatenate([x_t_emb, h_t])\n",
    "    \n",
    "    #compute next state given x_and_h\n",
    "    h_next = get_h_next(x_and_h)\n",
    "    \n",
    "    #get probabilities for language model\n",
    "    output_probas = get_probas(h_next)\n",
    "    \n",
    "    return output_probas, h_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN: loop\n",
    "\n",
    "Apply rnn_one_step in a loop over name characters to get predictions.\n",
    "\n",
    "Assume all names are at most length-16 to iterate over them in a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "input_sequence = tf.placeholder(tf.int32, (None, MAX_LENGTH))  #batch of token ids\n",
    "batch_size = tf.shape(input_sequence)[0]\n",
    "\n",
    "predicted_probas = []\n",
    "h_prev = tf.zeros([batch_size, rnn_num_units])  #initial hidden state\n",
    "\n",
    "for t in range(MAX_LENGTH):\n",
    "    x_t = input_sequence[:, t]  #column t\n",
    "    probas_next, h_next = rnn_one_step(x_t, h_prev)\n",
    "    \n",
    "    h_prev = h_next\n",
    "    predicted_probas.append(probas_next)\n",
    "    \n",
    "#combine predicted_probas into [batch, time, n_tokens] tensor\n",
    "predicted_probas = tf.transpose(tf.stack(predicted_probas), [1, 0, 2])\n",
    "\n",
    "#next to last token prediction is not needed\n",
    "predicted_probas = predicted_probas[:, :-1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN: loss and gradients\n",
    "\n",
    "Gather a matrix of predictions and the corresponding correct answers.\n",
    "\n",
    "Flatten the matrices to shape [None, n_tokens] to make it easier.\n",
    "\n",
    "The network can then be trained by minimising crossentropy between predicted probabilities and those answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten predictions to [batch*time, n_tokens]\n",
    "predictions_matrix = tf.reshape(predicted_probas, [-1, n_tokens])\n",
    "\n",
    "#flatten answers (next tokens) and one-hot encode them\n",
    "answers_matrix = tf.one_hot(tf.reshape(input_sequence[:, 1:], [-1]), n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss as categorical cross-entropy\n",
    "#tf.reduce_mean gets a scalar loss\n",
    "loss = -tf.reduce_mean(answers_matrix * tf.log(predictions_matrix))\n",
    "\n",
    "optimize = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN: training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from random import sample\n",
    "\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 32\n",
    "history = []\n",
    "\n",
    "for i in range(1000):\n",
    "    batch = to_matrix(sample(names, batch_size), max_len=MAX_LENGTH)\n",
    "    loss_i, _ = s.run([loss, optimize], {input_sequence: batch})\n",
    "    \n",
    "    history.append(loss_i)\n",
    "    \n",
    "    if (i + 1) % 100 == 0:\n",
    "        clear_output(True)\n",
    "        plt.plot(history, label='loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN: sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = tf.placeholder(tf.int32, (1,))\n",
    "h_t = tf.Variable(np.zeros([1, rnn_num_units], np.float32))  #update hidden state here\n",
    "\n",
    "next_probs, next_h = rnn_one_step(x_t, h_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to generate text given a seed_phrase as a seed.\n",
    "def generate_sample(seed_phrase=start_token, max_length=MAX_LENGTH):\n",
    "    x_sequence = [token_to_id[token] for token in seed_phrase]\n",
    "    s.run(tf.assign(h_t, h_t.initial_value))\n",
    "    \n",
    "    #feed seed phrase, if any\n",
    "    for ix in x_sequence[:-1]:\n",
    "         s.run(tf.assign(h_t, next_h), {x_t: [ix]})\n",
    "    \n",
    "    #start generating\n",
    "    for _ in range(max_length-len(seed_phrase)):\n",
    "        x_probs,_ = s.run([next_probs, tf.assign(h_t, next_h)], {x_t: [x_sequence[-1]]})\n",
    "        x_sequence.append(np.random.choice(n_tokens, p=x_probs[0]))\n",
    "        \n",
    "    return ''.join([tokens[ix] for ix in x_sequence if tokens[ix] != pad_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Kepleetn\n",
      " Vockatkec\n",
      " Logufpood\n",
      " Pirmil\n",
      " Carminarrs\n",
      " Felegino\n",
      " Sarbisblicde\n",
      " Dongininp\n",
      " Chersiam\n",
      " Samrrott\n"
     ]
    }
   ],
   "source": [
    "#without prefix\n",
    "for _ in range(10):\n",
    "    print(generate_sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
